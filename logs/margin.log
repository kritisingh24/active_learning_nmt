2022-09-14 22:36:00,214 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                           cfg.name : transformer_100_enhi_bpe
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                     cfg.data.train : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/train_tok
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                       cfg.data.dev : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/val_tok
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                      cfg.data.test : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/test_tok
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain_ac
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -            cfg.data.src.max_length : 60
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/vocab.en
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/en.bpe.codes
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 16000
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : moses
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : hi
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -            cfg.data.trg.max_length : 60
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/vocab.hi
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/hi.bpe.codes
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 16000
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                 cfg.testing.n_best : 1
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -             cfg.testing.batch_size : 1024
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -             cfg.testing.batch_type : token
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -      cfg.testing.max_output_length : 130
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -           cfg.testing.eval_metrics : ['bleu']
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -            cfg.testing.return_prob : none
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -       cfg.testing.return_attention : False
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.tokenize : 13a
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.lowercase : False
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -     cfg.active_learning.query_size : 10000
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -    cfg.active_learning.interactive : False
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -      cfg.active_learning.pool_size : 6
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -     cfg.active_learning.batch_size : 256
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -    cfg.active_learning.num_workers : 4
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -    cfg.active_learning.num_queries : 32
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -     cfg.active_learning.al_percent : 30
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers - cfg.active_learning.query_strategy : margin
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -          cfg.active_learning.epoch : 2
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers - cfg.active_learning.validation_freq : 1000
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -            cfg.training.load_model : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/enhi_transformer_t1_r3/180000.ckpt
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -       cfg.training.reset_best_ckpt : False
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -       cfg.training.reset_scheduler : False
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -       cfg.training.reset_optimizer : False
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -      cfg.training.reset_iter_state : False
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers - cfg.training.learning_rate_decay_length : 2500
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -    cfg.training.learning_rate_peak : 0.005
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -  cfg.training.learning_rate_warmup : 1000
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -  cfg.training.learning_rate_factor : 0.5
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -              cfg.training.patience : 5
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 1e-08
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.1
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -            cfg.training.batch_size : 4096
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 1
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -                cfg.training.epochs : 1
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 1000
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -             cfg.training.model_dir : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3]
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 3
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : False
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 1024
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.3
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers -       cfg.model.encoder.layer_norm : pre
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 4
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 1024
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.3
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers -       cfg.model.decoder.layer_norm : pre
2022-09-14 22:36:01,297 - INFO - joeynmt.data - Building tokenizer...
2022-09-14 22:36:01,421 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-14 22:36:01,422 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-14 22:36:01,422 - INFO - joeynmt.data - Loading train set...
2022-09-14 22:39:02,000 - INFO - joeynmt.data - Building vocabulary...
2022-09-14 22:39:04,117 - INFO - joeynmt.data - Loading dev set...
2022-09-14 22:39:09,650 - INFO - joeynmt.data - Loading test set...
2022-09-14 22:39:15,053 - INFO - joeynmt.data - Data loaded.
2022-09-14 22:39:15,053 - INFO - joeynmt.helpers - Train dataset: PlaintextDatasetAC(split=train, len=1552563, src_lang=en, trg_lang=hi, has_trg=True, random_subset=-1)
2022-09-14 22:39:15,053 - INFO - joeynmt.helpers - Valid dataset: PlaintextDatasetAC(split=dev, len=40856, src_lang=en, trg_lang=hi, has_trg=True, random_subset=-1)
2022-09-14 22:39:15,053 - INFO - joeynmt.helpers -  Test dataset: PlaintextDatasetAC(split=test, len=40858, src_lang=en, trg_lang=hi, has_trg=True, random_subset=-1)
2022-09-14 22:39:15,054 - INFO - joeynmt.helpers - First training example:
	[SRC] give your application an accessibility work@@ out
	[TRG] अपने अनुप्रयोग को पहुंच@@ नीयता व्यायाम का लाभ दें
2022-09-14 22:39:15,054 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) , (6) . (7) of (8) and (9) to
2022-09-14 22:39:15,054 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) के (5) है (6) । (7) , (8) और (9) में
2022-09-14 22:39:15,054 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 16021
2022-09-14 22:39:15,054 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 16174
2022-09-14 22:39:15,137 - INFO - joeynmt.training - BASELINE MODEL START
2022-09-14 22:39:15,137 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-14 22:39:15,137 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-14 22:39:15,461 - INFO - joeynmt.model - Enc-dec model built.
2022-09-14 22:39:18,781 - INFO - joeynmt.model - Total params: 19302144
2022-09-14 22:39:18,782 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm="pre"),
	decoder=TransformerDecoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm="pre"),
	src_embed=Embeddings(embedding_dim=256, vocab_size=16021),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=16174),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.1))
2022-09-14 22:39:20,894 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=[0.9, 0.999])
2022-09-14 22:39:20,894 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=5)
2022-09-14 22:39:20,894 - INFO - joeynmt.training - Loading model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/enhi_transformer_t1_r3/180000.ckpt
2022-09-14 22:39:21,136 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/enhi_transformer_t1_r3/180000.ckpt.
2022-09-14 22:39:21,191 - INFO - joeynmt.training - BASELINE MODEL END
2022-09-14 22:39:21,192 - INFO - joeynmt.training - ACTIVE LEARNING MODEL START - RANDOM
2022-09-14 22:39:21,192 - INFO - joeynmt.training - Random Indices picked: [121958 146867 131932 365838 259178 119879 110268 207892  54886 137337] length: 27946
2022-09-14 22:39:21,193 - INFO - joeynmt.training - Executing Random Strategy
2022-09-14 22:39:21,193 - INFO - joeynmt.training - Final Query Indices picked: [121958, 146867, 131932, 365838, 259178, 119879, 110268, 207892, 54886, 137337] length: 10000
2022-09-14 22:39:21,193 - INFO - joeynmt.training - Query the samples, one at a time (interactive/file)
2022-09-14 22:39:22,808 - INFO - joeynmt.training - Remaining Pool Data size: 455884
2022-09-14 22:39:22,809 - INFO - joeynmt.training - Active Learning Data ready to train size: 1096795
2022-09-14 22:39:22,809 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 4096
	effective batch size (w. parallel & accumulation): 4096
2022-09-14 22:39:22,809 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-14 23:01:15,285 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-14 23:01:15,286 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  15.23, loss:   3.05, ppl:  21.15, acc:   0.43, generation: 1305.4354[sec], evaluation: 6.6988[sec]
2022-09-14 23:01:15,290 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-14 23:01:16,730 - INFO - joeynmt.training - Example #0
2022-09-14 23:01:16,742 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-14 23:01:16,743 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-14 23:01:16,743 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष पर , विषय ढकने वाले हैं
2022-09-14 23:01:16,743 - INFO - joeynmt.training - Example #1
2022-09-14 23:01:16,754 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-14 23:01:16,754 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-14 23:01:16,754 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-14 23:01:16,754 - INFO - joeynmt.training - Example #2
2022-09-14 23:01:16,764 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-14 23:01:16,765 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-14 23:01:16,765 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमई
2022-09-14 23:01:16,765 - INFO - joeynmt.training - Example #3
2022-09-14 23:01:16,775 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-14 23:01:16,776 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-14 23:01:16,776 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-14 23:01:16,865 - INFO - joeynmt.training - EPOCH 1
2022-09-14 23:01:40,339 - INFO - joeynmt.training - Epoch   1, Step:   180100, Batch Loss:     3.101286, Batch Acc: 0.004122, Tokens per Sec:     4930, Lr: 0.000035
2022-09-14 23:02:03,603 - INFO - joeynmt.training - Epoch   1, Step:   180200, Batch Loss:     3.026329, Batch Acc: 0.003955, Tokens per Sec:     4988, Lr: 0.000035
2022-09-14 23:02:26,861 - INFO - joeynmt.training - Epoch   1, Step:   180300, Batch Loss:     3.033374, Batch Acc: 0.004267, Tokens per Sec:     4887, Lr: 0.000035
2022-09-14 23:02:49,914 - INFO - joeynmt.training - Epoch   1, Step:   180400, Batch Loss:     3.037927, Batch Acc: 0.003317, Tokens per Sec:     4957, Lr: 0.000035
2022-09-14 23:03:12,910 - INFO - joeynmt.training - Epoch   1, Step:   180500, Batch Loss:     2.987492, Batch Acc: 0.004104, Tokens per Sec:     4927, Lr: 0.000035
2022-09-14 23:03:36,035 - INFO - joeynmt.training - Epoch   1, Step:   180600, Batch Loss:     2.947808, Batch Acc: 0.003779, Tokens per Sec:     4863, Lr: 0.000035
2022-09-14 23:03:59,103 - INFO - joeynmt.training - Epoch   1, Step:   180700, Batch Loss:     2.930070, Batch Acc: 0.003730, Tokens per Sec:     4928, Lr: 0.000035
2022-09-14 23:04:22,173 - INFO - joeynmt.training - Epoch   1, Step:   180800, Batch Loss:     3.117540, Batch Acc: 0.003457, Tokens per Sec:     4928, Lr: 0.000035
2022-09-14 23:04:45,179 - INFO - joeynmt.training - Epoch   1, Step:   180900, Batch Loss:     2.799219, Batch Acc: 0.004219, Tokens per Sec:     4997, Lr: 0.000035
2022-09-14 23:05:08,108 - INFO - joeynmt.training - Epoch   1, Step:   181000, Batch Loss:     2.911176, Batch Acc: 0.003932, Tokens per Sec:     4891, Lr: 0.000035
2022-09-14 23:05:08,108 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-14 23:28:13,204 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-14 23:28:13,206 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  17.06, loss:   2.91, ppl:  18.34, acc:   0.46, generation: 1377.7108[sec], evaluation: 6.8149[sec]
2022-09-14 23:28:13,209 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-14 23:28:15,315 - INFO - joeynmt.training - Example #0
2022-09-14 23:28:15,327 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-14 23:28:15,327 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-14 23:28:15,327 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष पर , विषय ढकने वाले हैं
2022-09-14 23:28:15,327 - INFO - joeynmt.training - Example #1
2022-09-14 23:28:15,338 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-14 23:28:15,338 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-14 23:28:15,338 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-14 23:28:15,338 - INFO - joeynmt.training - Example #2
2022-09-14 23:28:15,349 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-14 23:28:15,349 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-14 23:28:15,349 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमआईएमआईएमई
2022-09-14 23:28:15,349 - INFO - joeynmt.training - Example #3
2022-09-14 23:28:15,360 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-14 23:28:15,360 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-14 23:28:15,360 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-14 23:28:38,428 - INFO - joeynmt.training - Epoch   1, Step:   181100, Batch Loss:     2.736219, Batch Acc: 0.004995, Tokens per Sec:     4565, Lr: 0.000035
2022-09-14 23:29:01,200 - INFO - joeynmt.training - Epoch   1, Step:   181200, Batch Loss:     3.079721, Batch Acc: 0.003563, Tokens per Sec:     4979, Lr: 0.000035
2022-09-14 23:29:24,189 - INFO - joeynmt.training - Epoch   1, Step:   181300, Batch Loss:     2.844570, Batch Acc: 0.004917, Tokens per Sec:     4919, Lr: 0.000035
2022-09-14 23:29:46,904 - INFO - joeynmt.training - Epoch   1, Step:   181400, Batch Loss:     3.181087, Batch Acc: 0.003937, Tokens per Sec:     4965, Lr: 0.000035
2022-09-14 23:30:09,784 - INFO - joeynmt.training - Epoch   1, Step:   181500, Batch Loss:     2.965595, Batch Acc: 0.003719, Tokens per Sec:     4948, Lr: 0.000035
2022-09-14 23:30:32,610 - INFO - joeynmt.training - Epoch   1, Step:   181600, Batch Loss:     2.725780, Batch Acc: 0.004976, Tokens per Sec:     4940, Lr: 0.000035
2022-09-14 23:30:55,529 - INFO - joeynmt.training - Epoch   1, Step:   181700, Batch Loss:     3.094812, Batch Acc: 0.003640, Tokens per Sec:     5119, Lr: 0.000035
2022-09-14 23:31:18,315 - INFO - joeynmt.training - Epoch   1, Step:   181800, Batch Loss:     3.067086, Batch Acc: 0.004151, Tokens per Sec:     4980, Lr: 0.000035
2022-09-14 23:31:41,231 - INFO - joeynmt.training - Epoch   1, Step:   181900, Batch Loss:     3.159156, Batch Acc: 0.003760, Tokens per Sec:     5014, Lr: 0.000035
2022-09-14 23:32:04,218 - INFO - joeynmt.training - Epoch   1, Step:   182000, Batch Loss:     3.016039, Batch Acc: 0.004497, Tokens per Sec:     4963, Lr: 0.000035
2022-09-14 23:32:04,219 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-14 23:54:44,525 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-14 23:54:44,526 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  17.55, loss:   2.87, ppl:  17.71, acc:   0.47, generation: 1353.1838[sec], evaluation: 6.7764[sec]
2022-09-14 23:54:44,530 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-14 23:54:46,068 - INFO - joeynmt.training - Example #0
2022-09-14 23:54:46,081 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-14 23:54:46,081 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-14 23:54:46,081 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष पर , विषय ढोर हैं
2022-09-14 23:54:46,081 - INFO - joeynmt.training - Example #1
2022-09-14 23:54:46,091 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-14 23:54:46,091 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-14 23:54:46,091 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-14 23:54:46,091 - INFO - joeynmt.training - Example #2
2022-09-14 23:54:46,102 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-14 23:54:46,102 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-14 23:54:46,102 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमआईएमई
2022-09-14 23:54:46,102 - INFO - joeynmt.training - Example #3
2022-09-14 23:54:46,113 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-14 23:54:46,113 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-14 23:54:46,113 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-14 23:55:09,232 - INFO - joeynmt.training - Epoch   1, Step:   182100, Batch Loss:     2.929410, Batch Acc: 0.004822, Tokens per Sec:     4609, Lr: 0.000035
2022-09-14 23:55:31,891 - INFO - joeynmt.training - Epoch   1, Step:   182200, Batch Loss:     2.953255, Batch Acc: 0.004409, Tokens per Sec:     4975, Lr: 0.000035
2022-09-14 23:55:54,695 - INFO - joeynmt.training - Epoch   1, Step:   182300, Batch Loss:     2.798948, Batch Acc: 0.005048, Tokens per Sec:     5056, Lr: 0.000035
2022-09-14 23:56:17,566 - INFO - joeynmt.training - Epoch   1, Step:   182400, Batch Loss:     3.115314, Batch Acc: 0.003644, Tokens per Sec:     5099, Lr: 0.000035
2022-09-14 23:56:40,390 - INFO - joeynmt.training - Epoch   1, Step:   182500, Batch Loss:     2.996531, Batch Acc: 0.003694, Tokens per Sec:     5006, Lr: 0.000035
2022-09-14 23:57:03,148 - INFO - joeynmt.training - Epoch   1, Step:   182600, Batch Loss:     2.829016, Batch Acc: 0.004642, Tokens per Sec:     5055, Lr: 0.000035
2022-09-14 23:57:25,985 - INFO - joeynmt.training - Epoch   1, Step:   182700, Batch Loss:     2.791520, Batch Acc: 0.003542, Tokens per Sec:     4982, Lr: 0.000035
2022-09-14 23:57:48,893 - INFO - joeynmt.training - Epoch   1, Step:   182800, Batch Loss:     2.867266, Batch Acc: 0.004855, Tokens per Sec:     5026, Lr: 0.000035
2022-09-14 23:58:11,693 - INFO - joeynmt.training - Epoch   1, Step:   182900, Batch Loss:     2.867259, Batch Acc: 0.004643, Tokens per Sec:     4884, Lr: 0.000035
2022-09-14 23:58:34,480 - INFO - joeynmt.training - Epoch   1, Step:   183000, Batch Loss:     3.072885, Batch Acc: 0.004174, Tokens per Sec:     4973, Lr: 0.000035
2022-09-14 23:58:34,480 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 00:21:03,239 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 00:21:03,241 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  17.68, loss:   2.85, ppl:  17.34, acc:   0.47, generation: 1341.6146[sec], evaluation: 6.5206[sec]
2022-09-15 00:21:03,244 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 00:21:05,713 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/180000.ckpt
2022-09-15 00:21:05,756 - INFO - joeynmt.training - Example #0
2022-09-15 00:21:05,768 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 00:21:05,768 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 00:21:05,768 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 00:21:05,768 - INFO - joeynmt.training - Example #1
2022-09-15 00:21:05,779 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 00:21:05,779 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 00:21:05,779 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 00:21:05,779 - INFO - joeynmt.training - Example #2
2022-09-15 00:21:05,789 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 00:21:05,789 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 00:21:05,789 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमआईएमआईएम
2022-09-15 00:21:05,790 - INFO - joeynmt.training - Example #3
2022-09-15 00:21:05,800 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 00:21:05,800 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 00:21:05,800 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 00:21:28,772 - INFO - joeynmt.training - Epoch   1, Step:   183100, Batch Loss:     2.912664, Batch Acc: 0.004571, Tokens per Sec:     4525, Lr: 0.000035
2022-09-15 00:21:51,548 - INFO - joeynmt.training - Epoch   1, Step:   183200, Batch Loss:     2.996406, Batch Acc: 0.004555, Tokens per Sec:     5003, Lr: 0.000035
2022-09-15 00:22:14,418 - INFO - joeynmt.training - Epoch   1, Step:   183300, Batch Loss:     2.920285, Batch Acc: 0.004033, Tokens per Sec:     5031, Lr: 0.000035
2022-09-15 00:22:37,207 - INFO - joeynmt.training - Epoch   1, Step:   183400, Batch Loss:     2.746175, Batch Acc: 0.004704, Tokens per Sec:     4991, Lr: 0.000035
2022-09-15 00:23:00,297 - INFO - joeynmt.training - Epoch   1, Step:   183500, Batch Loss:     2.921814, Batch Acc: 0.004244, Tokens per Sec:     5020, Lr: 0.000035
2022-09-15 00:23:23,221 - INFO - joeynmt.training - Epoch   1, Step:   183600, Batch Loss:     3.095084, Batch Acc: 0.004062, Tokens per Sec:     5059, Lr: 0.000035
2022-09-15 00:23:46,138 - INFO - joeynmt.training - Epoch   1, Step:   183700, Batch Loss:     2.812469, Batch Acc: 0.003628, Tokens per Sec:     4968, Lr: 0.000035
2022-09-15 00:24:09,352 - INFO - joeynmt.training - Epoch   1, Step:   183800, Batch Loss:     3.006616, Batch Acc: 0.004035, Tokens per Sec:     4985, Lr: 0.000035
2022-09-15 00:24:32,205 - INFO - joeynmt.training - Epoch   1, Step:   183900, Batch Loss:     3.029201, Batch Acc: 0.003539, Tokens per Sec:     4995, Lr: 0.000035
2022-09-15 00:24:55,133 - INFO - joeynmt.training - Epoch   1, Step:   184000, Batch Loss:     2.950976, Batch Acc: 0.003685, Tokens per Sec:     4971, Lr: 0.000035
2022-09-15 00:24:55,133 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 00:46:39,467 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 00:46:39,468 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  17.99, loss:   2.83, ppl:  16.91, acc:   0.47, generation: 1297.0695[sec], evaluation: 6.9185[sec]
2022-09-15 00:46:39,472 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 00:46:41,176 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/181000.ckpt
2022-09-15 00:46:41,218 - INFO - joeynmt.training - Example #0
2022-09-15 00:46:41,230 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 00:46:41,230 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 00:46:41,230 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 00:46:41,230 - INFO - joeynmt.training - Example #1
2022-09-15 00:46:41,241 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 00:46:41,241 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 00:46:41,241 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 00:46:41,241 - INFO - joeynmt.training - Example #2
2022-09-15 00:46:41,252 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 00:46:41,252 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 00:46:41,252 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमआईएमएम
2022-09-15 00:46:41,252 - INFO - joeynmt.training - Example #3
2022-09-15 00:46:41,263 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 00:46:41,263 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 00:46:41,263 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 00:47:04,188 - INFO - joeynmt.training - Epoch   1, Step:   184100, Batch Loss:     2.809845, Batch Acc: 0.004305, Tokens per Sec:     4634, Lr: 0.000035
2022-09-15 00:47:26,891 - INFO - joeynmt.training - Epoch   1, Step:   184200, Batch Loss:     3.092948, Batch Acc: 0.004141, Tokens per Sec:     5021, Lr: 0.000035
2022-09-15 00:47:49,826 - INFO - joeynmt.training - Epoch   1, Step:   184300, Batch Loss:     2.957055, Batch Acc: 0.003766, Tokens per Sec:     5014, Lr: 0.000035
2022-09-15 00:48:12,745 - INFO - joeynmt.training - Epoch   1, Step:   184400, Batch Loss:     2.941078, Batch Acc: 0.004914, Tokens per Sec:     4990, Lr: 0.000035
2022-09-15 00:48:35,631 - INFO - joeynmt.training - Epoch   1, Step:   184500, Batch Loss:     3.088259, Batch Acc: 0.003648, Tokens per Sec:     5055, Lr: 0.000035
2022-09-15 00:48:58,320 - INFO - joeynmt.training - Epoch   1, Step:   184600, Batch Loss:     3.050706, Batch Acc: 0.003686, Tokens per Sec:     5094, Lr: 0.000035
2022-09-15 00:49:21,097 - INFO - joeynmt.training - Epoch   1, Step:   184700, Batch Loss:     2.731165, Batch Acc: 0.004009, Tokens per Sec:     5060, Lr: 0.000035
2022-09-15 00:49:43,963 - INFO - joeynmt.training - Epoch   1, Step:   184800, Batch Loss:     2.734295, Batch Acc: 0.005035, Tokens per Sec:     4985, Lr: 0.000035
2022-09-15 00:50:06,781 - INFO - joeynmt.training - Epoch   1, Step:   184900, Batch Loss:     3.102917, Batch Acc: 0.003904, Tokens per Sec:     5052, Lr: 0.000035
2022-09-15 00:50:29,779 - INFO - joeynmt.training - Epoch   1, Step:   185000, Batch Loss:     2.653249, Batch Acc: 0.004522, Tokens per Sec:     5115, Lr: 0.000035
2022-09-15 00:50:29,780 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 01:11:45,149 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 01:11:45,150 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.11, loss:   2.81, ppl:  16.55, acc:   0.47, generation: 1268.4920[sec], evaluation: 6.5301[sec]
2022-09-15 01:11:45,154 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 01:11:46,597 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/182000.ckpt
2022-09-15 01:11:46,641 - INFO - joeynmt.training - Example #0
2022-09-15 01:11:46,653 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 01:11:46,653 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 01:11:46,653 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय ढकने वाले हैं
2022-09-15 01:11:46,653 - INFO - joeynmt.training - Example #1
2022-09-15 01:11:46,663 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 01:11:46,663 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 01:11:46,663 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 01:11:46,663 - INFO - joeynmt.training - Example #2
2022-09-15 01:11:46,674 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 01:11:46,674 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 01:11:46,674 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमएम
2022-09-15 01:11:46,674 - INFO - joeynmt.training - Example #3
2022-09-15 01:11:46,684 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 01:11:46,685 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 01:11:46,685 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 01:12:09,553 - INFO - joeynmt.training - Epoch   1, Step:   185100, Batch Loss:     2.944378, Batch Acc: 0.004157, Tokens per Sec:     4712, Lr: 0.000035
2022-09-15 01:12:32,475 - INFO - joeynmt.training - Epoch   1, Step:   185200, Batch Loss:     3.070089, Batch Acc: 0.003917, Tokens per Sec:     5035, Lr: 0.000035
2022-09-15 01:12:55,251 - INFO - joeynmt.training - Epoch   1, Step:   185300, Batch Loss:     2.791632, Batch Acc: 0.004757, Tokens per Sec:     4874, Lr: 0.000035
2022-09-15 01:13:18,405 - INFO - joeynmt.training - Epoch   1, Step:   185400, Batch Loss:     2.758584, Batch Acc: 0.005004, Tokens per Sec:     4954, Lr: 0.000035
2022-09-15 01:13:41,281 - INFO - joeynmt.training - Epoch   1, Step:   185500, Batch Loss:     2.954321, Batch Acc: 0.004828, Tokens per Sec:     5061, Lr: 0.000035
2022-09-15 01:14:03,995 - INFO - joeynmt.training - Epoch   1, Step:   185600, Batch Loss:     2.780972, Batch Acc: 0.004182, Tokens per Sec:     4927, Lr: 0.000035
2022-09-15 01:14:26,797 - INFO - joeynmt.training - Epoch   1, Step:   185700, Batch Loss:     2.884415, Batch Acc: 0.004631, Tokens per Sec:     5086, Lr: 0.000035
2022-09-15 01:14:49,643 - INFO - joeynmt.training - Epoch   1, Step:   185800, Batch Loss:     2.848700, Batch Acc: 0.004613, Tokens per Sec:     5057, Lr: 0.000035
2022-09-15 01:15:12,518 - INFO - joeynmt.training - Epoch   1, Step:   185900, Batch Loss:     3.095895, Batch Acc: 0.003231, Tokens per Sec:     5101, Lr: 0.000035
2022-09-15 01:15:35,425 - INFO - joeynmt.training - Epoch   1, Step:   186000, Batch Loss:     2.697147, Batch Acc: 0.004185, Tokens per Sec:     4944, Lr: 0.000035
2022-09-15 01:15:35,425 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 01:38:09,364 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 01:38:09,367 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.35, loss:   2.79, ppl:  16.30, acc:   0.48, generation: 1347.0738[sec], evaluation: 6.5207[sec]
2022-09-15 01:38:09,370 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 01:38:10,906 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/183000.ckpt
2022-09-15 01:38:10,948 - INFO - joeynmt.training - Example #0
2022-09-15 01:38:10,960 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 01:38:10,960 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 01:38:10,960 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय ढकने वाले हैं
2022-09-15 01:38:10,960 - INFO - joeynmt.training - Example #1
2022-09-15 01:38:10,971 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 01:38:10,971 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 01:38:10,971 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 01:38:10,971 - INFO - joeynmt.training - Example #2
2022-09-15 01:38:10,981 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 01:38:10,981 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 01:38:10,981 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और आईएमएम
2022-09-15 01:38:10,981 - INFO - joeynmt.training - Example #3
2022-09-15 01:38:10,992 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 01:38:10,992 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 01:38:10,992 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 01:38:33,833 - INFO - joeynmt.training - Epoch   1, Step:   186100, Batch Loss:     3.335998, Batch Acc: 0.003870, Tokens per Sec:     4679, Lr: 0.000035
2022-09-15 01:38:56,738 - INFO - joeynmt.training - Epoch   1, Step:   186200, Batch Loss:     3.070542, Batch Acc: 0.004149, Tokens per Sec:     5009, Lr: 0.000035
2022-09-15 01:39:19,954 - INFO - joeynmt.training - Epoch   1, Step:   186300, Batch Loss:     2.931484, Batch Acc: 0.004394, Tokens per Sec:     4902, Lr: 0.000035
2022-09-15 01:39:42,646 - INFO - joeynmt.training - Epoch   1, Step:   186400, Batch Loss:     2.849386, Batch Acc: 0.004338, Tokens per Sec:     4957, Lr: 0.000035
2022-09-15 01:40:05,438 - INFO - joeynmt.training - Epoch   1, Step:   186500, Batch Loss:     2.934476, Batch Acc: 0.003807, Tokens per Sec:     5001, Lr: 0.000035
2022-09-15 01:40:28,249 - INFO - joeynmt.training - Epoch   1, Step:   186600, Batch Loss:     2.948687, Batch Acc: 0.004027, Tokens per Sec:     5085, Lr: 0.000035
2022-09-15 01:40:51,045 - INFO - joeynmt.training - Epoch   1, Step:   186700, Batch Loss:     2.962434, Batch Acc: 0.004238, Tokens per Sec:     5000, Lr: 0.000035
2022-09-15 01:41:13,801 - INFO - joeynmt.training - Epoch   1, Step:   186800, Batch Loss:     2.596545, Batch Acc: 0.004743, Tokens per Sec:     5059, Lr: 0.000035
2022-09-15 01:41:36,541 - INFO - joeynmt.training - Epoch   1, Step:   186900, Batch Loss:     3.133003, Batch Acc: 0.004329, Tokens per Sec:     4967, Lr: 0.000035
2022-09-15 01:41:59,369 - INFO - joeynmt.training - Epoch   1, Step:   187000, Batch Loss:     2.914850, Batch Acc: 0.004186, Tokens per Sec:     4971, Lr: 0.000035
2022-09-15 01:41:59,369 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 02:02:54,307 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 02:02:54,308 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.48, loss:   2.78, ppl:  16.09, acc:   0.48, generation: 1248.0585[sec], evaluation: 6.5340[sec]
2022-09-15 02:02:54,312 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 02:02:56,072 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/184000.ckpt
2022-09-15 02:02:56,116 - INFO - joeynmt.training - Example #0
2022-09-15 02:02:56,128 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 02:02:56,128 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 02:02:56,128 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय कवर किए गए हैं
2022-09-15 02:02:56,128 - INFO - joeynmt.training - Example #1
2022-09-15 02:02:56,138 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 02:02:56,138 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 02:02:56,138 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 02:02:56,138 - INFO - joeynmt.training - Example #2
2022-09-15 02:02:56,149 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 02:02:56,149 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 02:02:56,149 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमएम
2022-09-15 02:02:56,149 - INFO - joeynmt.training - Example #3
2022-09-15 02:02:56,159 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 02:02:56,160 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 02:02:56,160 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 02:03:18,999 - INFO - joeynmt.training - Epoch   1, Step:   187100, Batch Loss:     2.819584, Batch Acc: 0.003859, Tokens per Sec:     4608, Lr: 0.000035
2022-09-15 02:03:41,720 - INFO - joeynmt.training - Epoch   1, Step:   187200, Batch Loss:     2.743662, Batch Acc: 0.004611, Tokens per Sec:     4954, Lr: 0.000035
2022-09-15 02:04:04,648 - INFO - joeynmt.training - Epoch   1, Step:   187300, Batch Loss:     3.098873, Batch Acc: 0.004226, Tokens per Sec:     5016, Lr: 0.000035
2022-09-15 02:04:27,462 - INFO - joeynmt.training - Epoch   1, Step:   187400, Batch Loss:     2.782271, Batch Acc: 0.005139, Tokens per Sec:     5032, Lr: 0.000035
2022-09-15 02:04:50,270 - INFO - joeynmt.training - Epoch   1, Step:   187500, Batch Loss:     2.931974, Batch Acc: 0.004430, Tokens per Sec:     5058, Lr: 0.000035
2022-09-15 02:05:13,375 - INFO - joeynmt.training - Epoch   1, Step:   187600, Batch Loss:     2.912590, Batch Acc: 0.004051, Tokens per Sec:     4936, Lr: 0.000035
2022-09-15 02:05:36,206 - INFO - joeynmt.training - Epoch   1, Step:   187700, Batch Loss:     2.951637, Batch Acc: 0.003977, Tokens per Sec:     5121, Lr: 0.000035
2022-09-15 02:05:59,150 - INFO - joeynmt.training - Epoch   1, Step:   187800, Batch Loss:     2.806710, Batch Acc: 0.004317, Tokens per Sec:     5018, Lr: 0.000035
2022-09-15 02:06:21,915 - INFO - joeynmt.training - Epoch   1, Step:   187900, Batch Loss:     2.967054, Batch Acc: 0.003784, Tokens per Sec:     4980, Lr: 0.000035
2022-09-15 02:06:44,804 - INFO - joeynmt.training - Epoch   1, Step:   188000, Batch Loss:     2.945558, Batch Acc: 0.004530, Tokens per Sec:     5054, Lr: 0.000035
2022-09-15 02:06:44,804 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 02:27:32,324 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 02:27:32,325 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.64, loss:   2.76, ppl:  15.82, acc:   0.48, generation: 1240.5402[sec], evaluation: 6.6313[sec]
2022-09-15 02:27:32,329 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 02:27:33,945 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/185000.ckpt
2022-09-15 02:27:33,987 - INFO - joeynmt.training - Example #0
2022-09-15 02:27:34,000 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 02:27:34,000 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 02:27:34,000 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 02:27:34,000 - INFO - joeynmt.training - Example #1
2022-09-15 02:27:34,010 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 02:27:34,010 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 02:27:34,010 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 02:27:34,010 - INFO - joeynmt.training - Example #2
2022-09-15 02:27:34,021 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 02:27:34,021 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 02:27:34,021 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमएम
2022-09-15 02:27:34,021 - INFO - joeynmt.training - Example #3
2022-09-15 02:27:34,031 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 02:27:34,031 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 02:27:34,031 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 02:27:56,961 - INFO - joeynmt.training - Epoch   1, Step:   188100, Batch Loss:     2.990107, Batch Acc: 0.003672, Tokens per Sec:     4622, Lr: 0.000035
2022-09-15 02:28:19,825 - INFO - joeynmt.training - Epoch   1, Step:   188200, Batch Loss:     2.774274, Batch Acc: 0.004252, Tokens per Sec:     4978, Lr: 0.000035
2022-09-15 02:28:42,688 - INFO - joeynmt.training - Epoch   1, Step:   188300, Batch Loss:     2.872617, Batch Acc: 0.004374, Tokens per Sec:     4960, Lr: 0.000035
2022-09-15 02:29:05,483 - INFO - joeynmt.training - Epoch   1, Step:   188400, Batch Loss:     3.202240, Batch Acc: 0.003764, Tokens per Sec:     4976, Lr: 0.000035
2022-09-15 02:29:28,246 - INFO - joeynmt.training - Epoch   1, Step:   188500, Batch Loss:     2.888229, Batch Acc: 0.004033, Tokens per Sec:     5000, Lr: 0.000035
2022-09-15 02:29:50,963 - INFO - joeynmt.training - Epoch   1, Step:   188600, Batch Loss:     2.880863, Batch Acc: 0.004467, Tokens per Sec:     4977, Lr: 0.000035
2022-09-15 02:30:13,764 - INFO - joeynmt.training - Epoch   1, Step:   188700, Batch Loss:     2.878229, Batch Acc: 0.004008, Tokens per Sec:     5033, Lr: 0.000035
2022-09-15 02:30:36,586 - INFO - joeynmt.training - Epoch   1, Step:   188800, Batch Loss:     2.833012, Batch Acc: 0.004273, Tokens per Sec:     5066, Lr: 0.000035
2022-09-15 02:30:59,421 - INFO - joeynmt.training - Epoch   1, Step:   188900, Batch Loss:     2.760223, Batch Acc: 0.004410, Tokens per Sec:     4906, Lr: 0.000035
2022-09-15 02:31:22,324 - INFO - joeynmt.training - Epoch   1, Step:   189000, Batch Loss:     3.184700, Batch Acc: 0.003951, Tokens per Sec:     5139, Lr: 0.000035
2022-09-15 02:31:22,325 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 02:52:14,460 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 02:52:14,463 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  19.08, loss:   2.75, ppl:  15.69, acc:   0.48, generation: 1244.7619[sec], evaluation: 6.6595[sec]
2022-09-15 02:52:14,466 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 02:52:15,924 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/186000.ckpt
2022-09-15 02:52:15,966 - INFO - joeynmt.training - Example #0
2022-09-15 02:52:15,978 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 02:52:15,978 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 02:52:15,978 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय कवर किए गए हैं
2022-09-15 02:52:15,978 - INFO - joeynmt.training - Example #1
2022-09-15 02:52:15,989 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 02:52:15,989 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 02:52:15,989 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 02:52:15,989 - INFO - joeynmt.training - Example #2
2022-09-15 02:52:16,000 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 02:52:16,000 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 02:52:16,000 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमएम
2022-09-15 02:52:16,000 - INFO - joeynmt.training - Example #3
2022-09-15 02:52:16,011 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 02:52:16,011 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 02:52:16,011 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 02:52:38,726 - INFO - joeynmt.training - Epoch   1, Step:   189100, Batch Loss:     3.011812, Batch Acc: 0.004000, Tokens per Sec:     4627, Lr: 0.000035
2022-09-15 02:53:01,404 - INFO - joeynmt.training - Epoch   1, Step:   189200, Batch Loss:     2.912446, Batch Acc: 0.004009, Tokens per Sec:     5016, Lr: 0.000035
2022-09-15 02:53:24,129 - INFO - joeynmt.training - Epoch   1, Step:   189300, Batch Loss:     2.871601, Batch Acc: 0.004203, Tokens per Sec:     5067, Lr: 0.000035
2022-09-15 02:53:47,084 - INFO - joeynmt.training - Epoch   1, Step:   189400, Batch Loss:     2.787082, Batch Acc: 0.004229, Tokens per Sec:     5088, Lr: 0.000035
2022-09-15 02:54:09,851 - INFO - joeynmt.training - Epoch   1, Step:   189500, Batch Loss:     2.887023, Batch Acc: 0.003980, Tokens per Sec:     4988, Lr: 0.000035
2022-09-15 02:54:32,505 - INFO - joeynmt.training - Epoch   1, Step:   189600, Batch Loss:     3.077711, Batch Acc: 0.003749, Tokens per Sec:     5004, Lr: 0.000035
2022-09-15 02:54:55,287 - INFO - joeynmt.training - Epoch   1, Step:   189700, Batch Loss:     2.934809, Batch Acc: 0.003902, Tokens per Sec:     5073, Lr: 0.000035
2022-09-15 02:55:18,139 - INFO - joeynmt.training - Epoch   1, Step:   189800, Batch Loss:     2.837546, Batch Acc: 0.004913, Tokens per Sec:     5184, Lr: 0.000035
2022-09-15 02:55:40,831 - INFO - joeynmt.training - Epoch   1, Step:   189900, Batch Loss:     2.575200, Batch Acc: 0.005420, Tokens per Sec:     5066, Lr: 0.000035
2022-09-15 02:56:03,615 - INFO - joeynmt.training - Epoch   1, Step:   190000, Batch Loss:     2.822368, Batch Acc: 0.003812, Tokens per Sec:     5043, Lr: 0.000035
2022-09-15 02:56:03,615 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 03:16:19,922 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 03:16:19,925 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.99, loss:   2.74, ppl:  15.49, acc:   0.48, generation: 1208.9084[sec], evaluation: 7.0550[sec]
2022-09-15 03:16:19,929 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 03:16:21,520 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/187000.ckpt
2022-09-15 03:16:21,562 - INFO - joeynmt.training - Example #0
2022-09-15 03:16:21,574 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 03:16:21,574 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 03:16:21,574 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 03:16:21,574 - INFO - joeynmt.training - Example #1
2022-09-15 03:16:21,585 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 03:16:21,585 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 03:16:21,585 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 03:16:21,585 - INFO - joeynmt.training - Example #2
2022-09-15 03:16:21,595 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 03:16:21,595 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 03:16:21,595 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमएम
2022-09-15 03:16:21,595 - INFO - joeynmt.training - Example #3
2022-09-15 03:16:21,606 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 03:16:21,606 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 03:16:21,606 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 03:16:44,696 - INFO - joeynmt.training - Epoch   1, Step:   190100, Batch Loss:     2.983704, Batch Acc: 0.004698, Tokens per Sec:     4641, Lr: 0.000035
2022-09-15 03:17:07,501 - INFO - joeynmt.training - Epoch   1, Step:   190200, Batch Loss:     2.848974, Batch Acc: 0.004145, Tokens per Sec:     4993, Lr: 0.000035
2022-09-15 03:17:30,179 - INFO - joeynmt.training - Epoch   1, Step:   190300, Batch Loss:     2.656975, Batch Acc: 0.003939, Tokens per Sec:     5071, Lr: 0.000035
2022-09-15 03:17:52,954 - INFO - joeynmt.training - Epoch   1, Step:   190400, Batch Loss:     2.990782, Batch Acc: 0.004439, Tokens per Sec:     5173, Lr: 0.000035
2022-09-15 03:18:15,760 - INFO - joeynmt.training - Epoch   1, Step:   190500, Batch Loss:     2.929703, Batch Acc: 0.004544, Tokens per Sec:     5153, Lr: 0.000035
2022-09-15 03:18:38,544 - INFO - joeynmt.training - Epoch   1, Step:   190600, Batch Loss:     2.891140, Batch Acc: 0.004358, Tokens per Sec:     5076, Lr: 0.000035
2022-09-15 03:19:01,305 - INFO - joeynmt.training - Epoch   1, Step:   190700, Batch Loss:     2.950466, Batch Acc: 0.004171, Tokens per Sec:     5141, Lr: 0.000035
2022-09-15 03:19:24,213 - INFO - joeynmt.training - Epoch   1, Step:   190800, Batch Loss:     2.710520, Batch Acc: 0.004186, Tokens per Sec:     5047, Lr: 0.000035
2022-09-15 03:19:47,121 - INFO - joeynmt.training - Epoch   1, Step:   190900, Batch Loss:     2.860869, Batch Acc: 0.004619, Tokens per Sec:     5047, Lr: 0.000035
2022-09-15 03:20:10,035 - INFO - joeynmt.training - Epoch   1, Step:   191000, Batch Loss:     2.756564, Batch Acc: 0.004088, Tokens per Sec:     5135, Lr: 0.000035
2022-09-15 03:20:10,036 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 03:40:20,220 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 03:40:20,222 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  19.27, loss:   2.73, ppl:  15.28, acc:   0.49, generation: 1203.1473[sec], evaluation: 6.6898[sec]
2022-09-15 03:40:20,226 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 03:40:21,691 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/188000.ckpt
2022-09-15 03:40:21,733 - INFO - joeynmt.training - Example #0
2022-09-15 03:40:21,746 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 03:40:21,746 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 03:40:21,746 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 03:40:21,746 - INFO - joeynmt.training - Example #1
2022-09-15 03:40:21,756 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 03:40:21,756 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 03:40:21,756 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 03:40:21,756 - INFO - joeynmt.training - Example #2
2022-09-15 03:40:21,767 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 03:40:21,767 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 03:40:21,767 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमएम
2022-09-15 03:40:21,767 - INFO - joeynmt.training - Example #3
2022-09-15 03:40:21,777 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 03:40:21,777 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 03:40:21,777 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 03:40:44,755 - INFO - joeynmt.training - Epoch   1, Step:   191100, Batch Loss:     2.908355, Batch Acc: 0.004273, Tokens per Sec:     4742, Lr: 0.000035
2022-09-15 03:41:07,636 - INFO - joeynmt.training - Epoch   1, Step:   191200, Batch Loss:     3.138769, Batch Acc: 0.003533, Tokens per Sec:     5010, Lr: 0.000035
2022-09-15 03:41:30,361 - INFO - joeynmt.training - Epoch   1, Step:   191300, Batch Loss:     2.865178, Batch Acc: 0.003718, Tokens per Sec:     5030, Lr: 0.000035
2022-09-15 03:41:53,182 - INFO - joeynmt.training - Epoch   1, Step:   191400, Batch Loss:     3.122874, Batch Acc: 0.003651, Tokens per Sec:     5016, Lr: 0.000035
2022-09-15 03:42:15,992 - INFO - joeynmt.training - Epoch   1, Step:   191500, Batch Loss:     3.038470, Batch Acc: 0.003760, Tokens per Sec:     5002, Lr: 0.000035
2022-09-15 03:42:38,868 - INFO - joeynmt.training - Epoch   1, Step:   191600, Batch Loss:     2.847741, Batch Acc: 0.003948, Tokens per Sec:     5093, Lr: 0.000035
2022-09-15 03:43:01,635 - INFO - joeynmt.training - Epoch   1, Step:   191700, Batch Loss:     2.909057, Batch Acc: 0.003913, Tokens per Sec:     4996, Lr: 0.000035
2022-09-15 03:43:24,464 - INFO - joeynmt.training - Epoch   1, Step:   191800, Batch Loss:     2.510681, Batch Acc: 0.004138, Tokens per Sec:     4997, Lr: 0.000035
2022-09-15 03:43:47,191 - INFO - joeynmt.training - Epoch   1, Step:   191900, Batch Loss:     2.747587, Batch Acc: 0.003338, Tokens per Sec:     5114, Lr: 0.000035
2022-09-15 03:44:10,004 - INFO - joeynmt.training - Epoch   1, Step:   192000, Batch Loss:     2.992269, Batch Acc: 0.004208, Tokens per Sec:     5010, Lr: 0.000035
2022-09-15 03:44:10,004 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 04:04:17,630 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 04:04:17,632 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  19.54, loss:   2.72, ppl:  15.18, acc:   0.49, generation: 1200.1312[sec], evaluation: 7.1482[sec]
2022-09-15 04:04:17,636 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 04:04:19,182 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/189000.ckpt
2022-09-15 04:04:19,225 - INFO - joeynmt.training - Example #0
2022-09-15 04:04:19,237 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 04:04:19,237 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 04:04:19,237 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों पर शामिल हैं
2022-09-15 04:04:19,237 - INFO - joeynmt.training - Example #1
2022-09-15 04:04:19,248 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 04:04:19,248 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 04:04:19,248 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 04:04:19,248 - INFO - joeynmt.training - Example #2
2022-09-15 04:04:19,258 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 04:04:19,258 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 04:04:19,258 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और इम
2022-09-15 04:04:19,258 - INFO - joeynmt.training - Example #3
2022-09-15 04:04:19,269 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 04:04:19,269 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 04:04:19,269 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 04:04:42,150 - INFO - joeynmt.training - Epoch   1, Step:   192100, Batch Loss:     2.942381, Batch Acc: 0.003945, Tokens per Sec:     4622, Lr: 0.000035
2022-09-15 04:05:04,895 - INFO - joeynmt.training - Epoch   1, Step:   192200, Batch Loss:     2.852902, Batch Acc: 0.004179, Tokens per Sec:     4966, Lr: 0.000035
2022-09-15 04:05:27,739 - INFO - joeynmt.training - Epoch   1, Step:   192300, Batch Loss:     2.894291, Batch Acc: 0.003976, Tokens per Sec:     4976, Lr: 0.000035
2022-09-15 04:05:50,457 - INFO - joeynmt.training - Epoch   1, Step:   192400, Batch Loss:     2.772505, Batch Acc: 0.004854, Tokens per Sec:     5088, Lr: 0.000035
2022-09-15 04:06:13,190 - INFO - joeynmt.training - Epoch   1, Step:   192500, Batch Loss:     3.046392, Batch Acc: 0.003758, Tokens per Sec:     5022, Lr: 0.000035
2022-09-15 04:06:36,031 - INFO - joeynmt.training - Epoch   1, Step:   192600, Batch Loss:     2.741679, Batch Acc: 0.005009, Tokens per Sec:     5026, Lr: 0.000035
2022-09-15 04:06:58,895 - INFO - joeynmt.training - Epoch   1, Step:   192700, Batch Loss:     2.944114, Batch Acc: 0.004112, Tokens per Sec:     5095, Lr: 0.000035
2022-09-15 04:07:21,730 - INFO - joeynmt.training - Epoch   1, Step:   192800, Batch Loss:     2.812586, Batch Acc: 0.004050, Tokens per Sec:     5061, Lr: 0.000035
2022-09-15 04:07:44,499 - INFO - joeynmt.training - Epoch   1, Step:   192900, Batch Loss:     2.947968, Batch Acc: 0.003584, Tokens per Sec:     4963, Lr: 0.000035
2022-09-15 04:08:07,383 - INFO - joeynmt.training - Epoch   1, Step:   193000, Batch Loss:     2.748670, Batch Acc: 0.005020, Tokens per Sec:     4979, Lr: 0.000035
2022-09-15 04:08:07,384 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 04:29:40,783 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 04:29:40,784 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  19.62, loss:   2.70, ppl:  14.95, acc:   0.49, generation: 1286.3178[sec], evaluation: 6.7320[sec]
2022-09-15 04:29:40,788 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 04:29:42,278 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/190000.ckpt
2022-09-15 04:29:42,320 - INFO - joeynmt.training - Example #0
2022-09-15 04:29:42,332 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 04:29:42,332 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 04:29:42,332 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 04:29:42,332 - INFO - joeynmt.training - Example #1
2022-09-15 04:29:42,343 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 04:29:42,343 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 04:29:42,343 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 04:29:42,343 - INFO - joeynmt.training - Example #2
2022-09-15 04:29:42,353 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 04:29:42,353 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 04:29:42,353 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और इम
2022-09-15 04:29:42,353 - INFO - joeynmt.training - Example #3
2022-09-15 04:29:42,364 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 04:29:42,364 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 04:29:42,364 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 04:30:05,215 - INFO - joeynmt.training - Epoch   1, Step:   193100, Batch Loss:     2.787038, Batch Acc: 0.004373, Tokens per Sec:     4728, Lr: 0.000035
2022-09-15 04:30:27,909 - INFO - joeynmt.training - Epoch   1, Step:   193200, Batch Loss:     2.846979, Batch Acc: 0.004156, Tokens per Sec:     5036, Lr: 0.000035
2022-09-15 04:30:50,687 - INFO - joeynmt.training - Epoch   1, Step:   193300, Batch Loss:     2.593731, Batch Acc: 0.004684, Tokens per Sec:     5071, Lr: 0.000035
2022-09-15 04:31:13,439 - INFO - joeynmt.training - Epoch   1, Step:   193400, Batch Loss:     2.380951, Batch Acc: 0.005025, Tokens per Sec:     5021, Lr: 0.000035
2022-09-15 04:31:36,223 - INFO - joeynmt.training - Epoch   1, Step:   193500, Batch Loss:     2.735201, Batch Acc: 0.004989, Tokens per Sec:     5112, Lr: 0.000035
2022-09-15 04:31:58,998 - INFO - joeynmt.training - Epoch   1, Step:   193600, Batch Loss:     2.807085, Batch Acc: 0.004389, Tokens per Sec:     5002, Lr: 0.000035
2022-09-15 04:32:21,979 - INFO - joeynmt.training - Epoch   1, Step:   193700, Batch Loss:     2.864379, Batch Acc: 0.004132, Tokens per Sec:     5066, Lr: 0.000035
2022-09-15 04:32:44,798 - INFO - joeynmt.training - Epoch   1, Step:   193800, Batch Loss:     2.568259, Batch Acc: 0.005812, Tokens per Sec:     5059, Lr: 0.000035
2022-09-15 04:33:07,588 - INFO - joeynmt.training - Epoch   1, Step:   193900, Batch Loss:     2.625645, Batch Acc: 0.004906, Tokens per Sec:     5133, Lr: 0.000035
2022-09-15 04:33:30,629 - INFO - joeynmt.training - Epoch   1, Step:   194000, Batch Loss:     2.876255, Batch Acc: 0.003638, Tokens per Sec:     5070, Lr: 0.000035
2022-09-15 04:33:30,630 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 04:52:55,445 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 04:52:55,446 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  19.67, loss:   2.70, ppl:  14.86, acc:   0.49, generation: 1157.7799[sec], evaluation: 6.6882[sec]
2022-09-15 04:52:55,450 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 04:52:56,942 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/191000.ckpt
2022-09-15 04:52:56,984 - INFO - joeynmt.training - Example #0
2022-09-15 04:52:56,996 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 04:52:56,996 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 04:52:56,996 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 04:52:56,996 - INFO - joeynmt.training - Example #1
2022-09-15 04:52:57,007 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 04:52:57,007 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 04:52:57,007 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 04:52:57,007 - INFO - joeynmt.training - Example #2
2022-09-15 04:52:57,018 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 04:52:57,018 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 04:52:57,018 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और इम
2022-09-15 04:52:57,018 - INFO - joeynmt.training - Example #3
2022-09-15 04:52:57,028 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 04:52:57,028 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 04:52:57,028 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 04:53:20,032 - INFO - joeynmt.training - Epoch   1, Step:   194100, Batch Loss:     2.811909, Batch Acc: 0.004022, Tokens per Sec:     4632, Lr: 0.000035
2022-09-15 04:53:42,875 - INFO - joeynmt.training - Epoch   1, Step:   194200, Batch Loss:     2.672616, Batch Acc: 0.003865, Tokens per Sec:     5109, Lr: 0.000035
2022-09-15 04:54:05,612 - INFO - joeynmt.training - Epoch   1, Step:   194300, Batch Loss:     2.857432, Batch Acc: 0.004289, Tokens per Sec:     4933, Lr: 0.000035
2022-09-15 04:54:11,583 - INFO - joeynmt.training - Epoch   1: total training loss 41565.03
2022-09-15 04:54:11,583 - INFO - joeynmt.training - EPOCH 2
2022-09-15 04:54:28,160 - INFO - joeynmt.training - Epoch   2, Step:   194400, Batch Loss:     2.943583, Batch Acc: 0.005167, Tokens per Sec:     5091, Lr: 0.000035
2022-09-15 04:54:50,931 - INFO - joeynmt.training - Epoch   2, Step:   194500, Batch Loss:     2.997986, Batch Acc: 0.004261, Tokens per Sec:     5040, Lr: 0.000035
2022-09-15 04:55:13,623 - INFO - joeynmt.training - Epoch   2, Step:   194600, Batch Loss:     2.727179, Batch Acc: 0.004142, Tokens per Sec:     4969, Lr: 0.000035
2022-09-15 04:55:36,341 - INFO - joeynmt.training - Epoch   2, Step:   194700, Batch Loss:     2.883156, Batch Acc: 0.003965, Tokens per Sec:     5029, Lr: 0.000035
2022-09-15 04:55:59,014 - INFO - joeynmt.training - Epoch   2, Step:   194800, Batch Loss:     2.744349, Batch Acc: 0.005357, Tokens per Sec:     5047, Lr: 0.000035
2022-09-15 04:56:21,874 - INFO - joeynmt.training - Epoch   2, Step:   194900, Batch Loss:     2.900767, Batch Acc: 0.004475, Tokens per Sec:     5073, Lr: 0.000035
2022-09-15 04:56:44,646 - INFO - joeynmt.training - Epoch   2, Step:   195000, Batch Loss:     2.958248, Batch Acc: 0.004715, Tokens per Sec:     5058, Lr: 0.000035
2022-09-15 04:56:44,646 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 05:15:40,570 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 05:15:40,572 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  19.82, loss:   2.69, ppl:  14.76, acc:   0.49, generation: 1128.4517[sec], evaluation: 6.6912[sec]
2022-09-15 05:15:40,576 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 05:15:42,129 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/192000.ckpt
2022-09-15 05:15:42,172 - INFO - joeynmt.training - Example #0
2022-09-15 05:15:42,184 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 05:15:42,184 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 05:15:42,184 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 05:15:42,184 - INFO - joeynmt.training - Example #1
2022-09-15 05:15:42,195 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 05:15:42,195 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 05:15:42,195 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 05:15:42,195 - INFO - joeynmt.training - Example #2
2022-09-15 05:15:42,206 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 05:15:42,206 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 05:15:42,206 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और इम
2022-09-15 05:15:42,206 - INFO - joeynmt.training - Example #3
2022-09-15 05:15:42,217 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 05:15:42,217 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 05:15:42,217 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 05:16:04,997 - INFO - joeynmt.training - Epoch   2, Step:   195100, Batch Loss:     3.004717, Batch Acc: 0.003922, Tokens per Sec:     4667, Lr: 0.000035
2022-09-15 05:16:27,743 - INFO - joeynmt.training - Epoch   2, Step:   195200, Batch Loss:     3.027936, Batch Acc: 0.004002, Tokens per Sec:     5075, Lr: 0.000035
2022-09-15 05:16:50,547 - INFO - joeynmt.training - Epoch   2, Step:   195300, Batch Loss:     3.009899, Batch Acc: 0.003794, Tokens per Sec:     5097, Lr: 0.000035
2022-09-15 05:17:13,300 - INFO - joeynmt.training - Epoch   2, Step:   195400, Batch Loss:     2.545414, Batch Acc: 0.004241, Tokens per Sec:     5099, Lr: 0.000035
2022-09-15 05:17:36,067 - INFO - joeynmt.training - Epoch   2, Step:   195500, Batch Loss:     2.973841, Batch Acc: 0.003978, Tokens per Sec:     5068, Lr: 0.000035
2022-09-15 05:17:58,764 - INFO - joeynmt.training - Epoch   2, Step:   195600, Batch Loss:     2.605567, Batch Acc: 0.004624, Tokens per Sec:     5021, Lr: 0.000035
2022-09-15 05:18:21,495 - INFO - joeynmt.training - Epoch   2, Step:   195700, Batch Loss:     2.730541, Batch Acc: 0.003913, Tokens per Sec:     5037, Lr: 0.000035
2022-09-15 05:18:44,215 - INFO - joeynmt.training - Epoch   2, Step:   195800, Batch Loss:     2.725955, Batch Acc: 0.005385, Tokens per Sec:     5117, Lr: 0.000035
2022-09-15 05:19:06,799 - INFO - joeynmt.training - Epoch   2, Step:   195900, Batch Loss:     2.685081, Batch Acc: 0.004647, Tokens per Sec:     5060, Lr: 0.000035
2022-09-15 05:19:29,378 - INFO - joeynmt.training - Epoch   2, Step:   196000, Batch Loss:     2.750356, Batch Acc: 0.004981, Tokens per Sec:     5051, Lr: 0.000035
2022-09-15 05:19:29,378 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 05:38:53,736 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 05:38:53,737 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.19, loss:   2.68, ppl:  14.60, acc:   0.49, generation: 1157.2616[sec], evaluation: 6.7489[sec]
2022-09-15 05:38:53,741 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 05:38:55,200 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/193000.ckpt
2022-09-15 05:38:55,243 - INFO - joeynmt.training - Example #0
2022-09-15 05:38:55,256 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 05:38:55,256 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 05:38:55,256 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 05:38:55,256 - INFO - joeynmt.training - Example #1
2022-09-15 05:38:55,266 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 05:38:55,266 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 05:38:55,266 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 05:38:55,266 - INFO - joeynmt.training - Example #2
2022-09-15 05:38:55,277 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 05:38:55,277 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 05:38:55,277 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 05:38:55,277 - INFO - joeynmt.training - Example #3
2022-09-15 05:38:55,288 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 05:38:55,288 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 05:38:55,288 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 05:39:18,025 - INFO - joeynmt.training - Epoch   2, Step:   196100, Batch Loss:     2.799880, Batch Acc: 0.004346, Tokens per Sec:     4662, Lr: 0.000035
2022-09-15 05:39:40,797 - INFO - joeynmt.training - Epoch   2, Step:   196200, Batch Loss:     2.699608, Batch Acc: 0.004352, Tokens per Sec:     5076, Lr: 0.000035
2022-09-15 05:40:03,569 - INFO - joeynmt.training - Epoch   2, Step:   196300, Batch Loss:     2.813494, Batch Acc: 0.004126, Tokens per Sec:     5088, Lr: 0.000035
2022-09-15 05:40:26,336 - INFO - joeynmt.training - Epoch   2, Step:   196400, Batch Loss:     2.900611, Batch Acc: 0.004651, Tokens per Sec:     4977, Lr: 0.000035
2022-09-15 05:40:48,994 - INFO - joeynmt.training - Epoch   2, Step:   196500, Batch Loss:     2.752345, Batch Acc: 0.004278, Tokens per Sec:     5024, Lr: 0.000035
2022-09-15 05:41:11,702 - INFO - joeynmt.training - Epoch   2, Step:   196600, Batch Loss:     2.902818, Batch Acc: 0.003572, Tokens per Sec:     4968, Lr: 0.000035
2022-09-15 05:41:34,423 - INFO - joeynmt.training - Epoch   2, Step:   196700, Batch Loss:     3.001684, Batch Acc: 0.004599, Tokens per Sec:     5072, Lr: 0.000035
2022-09-15 05:41:57,230 - INFO - joeynmt.training - Epoch   2, Step:   196800, Batch Loss:     2.674809, Batch Acc: 0.005005, Tokens per Sec:     4994, Lr: 0.000035
2022-09-15 05:42:20,101 - INFO - joeynmt.training - Epoch   2, Step:   196900, Batch Loss:     3.016856, Batch Acc: 0.003498, Tokens per Sec:     5038, Lr: 0.000035
2022-09-15 05:42:42,911 - INFO - joeynmt.training - Epoch   2, Step:   197000, Batch Loss:     2.722703, Batch Acc: 0.005784, Tokens per Sec:     4995, Lr: 0.000035
2022-09-15 05:42:42,911 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 06:02:04,289 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 06:02:04,290 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.29, loss:   2.68, ppl:  14.54, acc:   0.49, generation: 1154.3959[sec], evaluation: 6.6325[sec]
2022-09-15 06:02:04,294 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 06:02:06,479 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/194000.ckpt
2022-09-15 06:02:06,521 - INFO - joeynmt.training - Example #0
2022-09-15 06:02:06,533 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 06:02:06,533 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 06:02:06,533 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 06:02:06,533 - INFO - joeynmt.training - Example #1
2022-09-15 06:02:06,543 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 06:02:06,543 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 06:02:06,543 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 06:02:06,543 - INFO - joeynmt.training - Example #2
2022-09-15 06:02:06,553 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 06:02:06,553 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 06:02:06,553 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और इम
2022-09-15 06:02:06,553 - INFO - joeynmt.training - Example #3
2022-09-15 06:02:06,564 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 06:02:06,564 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 06:02:06,564 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 06:02:29,364 - INFO - joeynmt.training - Epoch   2, Step:   197100, Batch Loss:     2.908132, Batch Acc: 0.004173, Tokens per Sec:     4541, Lr: 0.000035
2022-09-15 06:02:52,128 - INFO - joeynmt.training - Epoch   2, Step:   197200, Batch Loss:     2.779969, Batch Acc: 0.004708, Tokens per Sec:     5076, Lr: 0.000035
2022-09-15 06:03:14,924 - INFO - joeynmt.training - Epoch   2, Step:   197300, Batch Loss:     2.922796, Batch Acc: 0.004285, Tokens per Sec:     4986, Lr: 0.000035
2022-09-15 06:03:37,600 - INFO - joeynmt.training - Epoch   2, Step:   197400, Batch Loss:     2.404596, Batch Acc: 0.004658, Tokens per Sec:     4999, Lr: 0.000035
2022-09-15 06:04:00,378 - INFO - joeynmt.training - Epoch   2, Step:   197500, Batch Loss:     3.018724, Batch Acc: 0.004191, Tokens per Sec:     5007, Lr: 0.000035
2022-09-15 06:04:23,197 - INFO - joeynmt.training - Epoch   2, Step:   197600, Batch Loss:     2.756185, Batch Acc: 0.005258, Tokens per Sec:     5009, Lr: 0.000035
2022-09-15 06:04:45,918 - INFO - joeynmt.training - Epoch   2, Step:   197700, Batch Loss:     2.924367, Batch Acc: 0.003784, Tokens per Sec:     5013, Lr: 0.000035
2022-09-15 06:05:08,729 - INFO - joeynmt.training - Epoch   2, Step:   197800, Batch Loss:     2.699042, Batch Acc: 0.004149, Tokens per Sec:     5029, Lr: 0.000035
2022-09-15 06:05:31,531 - INFO - joeynmt.training - Epoch   2, Step:   197900, Batch Loss:     2.721470, Batch Acc: 0.004641, Tokens per Sec:     4999, Lr: 0.000035
2022-09-15 06:05:54,466 - INFO - joeynmt.training - Epoch   2, Step:   198000, Batch Loss:     2.794541, Batch Acc: 0.004948, Tokens per Sec:     4961, Lr: 0.000035
2022-09-15 06:05:54,466 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 06:24:21,851 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 06:24:21,852 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.35, loss:   2.67, ppl:  14.50, acc:   0.49, generation: 1099.9661[sec], evaluation: 6.6613[sec]
2022-09-15 06:24:21,856 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 06:24:23,333 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/195000.ckpt
2022-09-15 06:24:23,375 - INFO - joeynmt.training - Example #0
2022-09-15 06:24:23,387 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 06:24:23,387 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 06:24:23,387 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 06:24:23,387 - INFO - joeynmt.training - Example #1
2022-09-15 06:24:23,398 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 06:24:23,398 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 06:24:23,398 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 06:24:23,398 - INFO - joeynmt.training - Example #2
2022-09-15 06:24:23,409 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 06:24:23,409 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 06:24:23,409 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और इम
2022-09-15 06:24:23,409 - INFO - joeynmt.training - Example #3
2022-09-15 06:24:23,420 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 06:24:23,420 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 06:24:23,420 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 06:24:46,281 - INFO - joeynmt.training - Epoch   2, Step:   198100, Batch Loss:     2.757334, Batch Acc: 0.004530, Tokens per Sec:     4646, Lr: 0.000035
2022-09-15 06:25:09,045 - INFO - joeynmt.training - Epoch   2, Step:   198200, Batch Loss:     2.966832, Batch Acc: 0.003708, Tokens per Sec:     5023, Lr: 0.000035
2022-09-15 06:25:31,816 - INFO - joeynmt.training - Epoch   2, Step:   198300, Batch Loss:     2.744278, Batch Acc: 0.004175, Tokens per Sec:     4954, Lr: 0.000035
2022-09-15 06:25:54,681 - INFO - joeynmt.training - Epoch   2, Step:   198400, Batch Loss:     2.759696, Batch Acc: 0.004636, Tokens per Sec:     4991, Lr: 0.000035
2022-09-15 06:26:17,608 - INFO - joeynmt.training - Epoch   2, Step:   198500, Batch Loss:     2.800926, Batch Acc: 0.004652, Tokens per Sec:     4969, Lr: 0.000035
2022-09-15 06:26:40,362 - INFO - joeynmt.training - Epoch   2, Step:   198600, Batch Loss:     3.070003, Batch Acc: 0.004146, Tokens per Sec:     5014, Lr: 0.000035
2022-09-15 06:27:03,077 - INFO - joeynmt.training - Epoch   2, Step:   198700, Batch Loss:     2.830786, Batch Acc: 0.004305, Tokens per Sec:     5082, Lr: 0.000035
2022-09-15 06:27:25,791 - INFO - joeynmt.training - Epoch   2, Step:   198800, Batch Loss:     3.193580, Batch Acc: 0.003625, Tokens per Sec:     5113, Lr: 0.000035
2022-09-15 06:27:48,521 - INFO - joeynmt.training - Epoch   2, Step:   198900, Batch Loss:     2.769883, Batch Acc: 0.004280, Tokens per Sec:     4996, Lr: 0.000035
2022-09-15 06:28:11,239 - INFO - joeynmt.training - Epoch   2, Step:   199000, Batch Loss:     2.731771, Batch Acc: 0.004671, Tokens per Sec:     4929, Lr: 0.000035
2022-09-15 06:28:11,239 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 06:45:51,017 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 06:45:51,019 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.44, loss:   2.67, ppl:  14.45, acc:   0.49, generation: 1052.8004[sec], evaluation: 6.6340[sec]
2022-09-15 06:45:51,023 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 06:45:52,508 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/196000.ckpt
2022-09-15 06:45:52,550 - INFO - joeynmt.training - Example #0
2022-09-15 06:45:52,562 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 06:45:52,562 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 06:45:52,562 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को ढका हुआ है
2022-09-15 06:45:52,562 - INFO - joeynmt.training - Example #1
2022-09-15 06:45:52,573 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 06:45:52,573 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 06:45:52,573 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 06:45:52,573 - INFO - joeynmt.training - Example #2
2022-09-15 06:45:52,583 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 06:45:52,583 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 06:45:52,583 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और इम
2022-09-15 06:45:52,583 - INFO - joeynmt.training - Example #3
2022-09-15 06:45:52,594 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 06:45:52,594 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 06:45:52,594 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 06:46:15,464 - INFO - joeynmt.training - Epoch   2, Step:   199100, Batch Loss:     2.575809, Batch Acc: 0.004068, Tokens per Sec:     4737, Lr: 0.000035
2022-09-15 06:46:38,251 - INFO - joeynmt.training - Epoch   2, Step:   199200, Batch Loss:     2.643629, Batch Acc: 0.005683, Tokens per Sec:     5058, Lr: 0.000035
2022-09-15 06:47:00,965 - INFO - joeynmt.training - Epoch   2, Step:   199300, Batch Loss:     2.892365, Batch Acc: 0.003791, Tokens per Sec:     4993, Lr: 0.000035
2022-09-15 06:47:23,595 - INFO - joeynmt.training - Epoch   2, Step:   199400, Batch Loss:     2.720987, Batch Acc: 0.004631, Tokens per Sec:     5067, Lr: 0.000035
2022-09-15 06:47:46,393 - INFO - joeynmt.training - Epoch   2, Step:   199500, Batch Loss:     2.678196, Batch Acc: 0.004974, Tokens per Sec:     5001, Lr: 0.000035
2022-09-15 06:48:09,175 - INFO - joeynmt.training - Epoch   2, Step:   199600, Batch Loss:     2.466534, Batch Acc: 0.005090, Tokens per Sec:     5036, Lr: 0.000035
2022-09-15 06:48:31,953 - INFO - joeynmt.training - Epoch   2, Step:   199700, Batch Loss:     2.673551, Batch Acc: 0.004877, Tokens per Sec:     5095, Lr: 0.000035
2022-09-15 06:48:54,733 - INFO - joeynmt.training - Epoch   2, Step:   199800, Batch Loss:     2.768292, Batch Acc: 0.004228, Tokens per Sec:     5108, Lr: 0.000035
2022-09-15 06:49:17,391 - INFO - joeynmt.training - Epoch   2, Step:   199900, Batch Loss:     2.915219, Batch Acc: 0.004042, Tokens per Sec:     5078, Lr: 0.000035
2022-09-15 06:49:40,045 - INFO - joeynmt.training - Epoch   2, Step:   200000, Batch Loss:     2.676454, Batch Acc: 0.005238, Tokens per Sec:     4972, Lr: 0.000035
2022-09-15 06:49:40,045 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 07:08:09,713 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 07:08:09,715 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.50, loss:   2.66, ppl:  14.28, acc:   0.49, generation: 1102.6982[sec], evaluation: 6.6219[sec]
2022-09-15 07:08:09,719 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 07:08:11,891 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/197000.ckpt
2022-09-15 07:08:11,934 - INFO - joeynmt.training - Example #0
2022-09-15 07:08:11,946 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 07:08:11,946 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 07:08:11,946 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को ढका हुआ है
2022-09-15 07:08:11,946 - INFO - joeynmt.training - Example #1
2022-09-15 07:08:11,957 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 07:08:11,957 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 07:08:11,957 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-15 07:08:11,957 - INFO - joeynmt.training - Example #2
2022-09-15 07:08:11,967 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 07:08:11,967 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 07:08:11,967 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और इम
2022-09-15 07:08:11,967 - INFO - joeynmt.training - Example #3
2022-09-15 07:08:11,978 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 07:08:11,978 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 07:08:11,978 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 07:08:34,835 - INFO - joeynmt.training - Epoch   2, Step:   200100, Batch Loss:     2.901125, Batch Acc: 0.004396, Tokens per Sec:     4529, Lr: 0.000035
2022-09-15 07:08:57,504 - INFO - joeynmt.training - Epoch   2, Step:   200200, Batch Loss:     2.671663, Batch Acc: 0.004523, Tokens per Sec:     5052, Lr: 0.000035
2022-09-15 07:09:20,259 - INFO - joeynmt.training - Epoch   2, Step:   200300, Batch Loss:     2.803390, Batch Acc: 0.004396, Tokens per Sec:     5099, Lr: 0.000035
2022-09-15 07:09:43,091 - INFO - joeynmt.training - Epoch   2, Step:   200400, Batch Loss:     2.807390, Batch Acc: 0.003775, Tokens per Sec:     5024, Lr: 0.000035
2022-09-15 07:10:05,910 - INFO - joeynmt.training - Epoch   2, Step:   200500, Batch Loss:     2.677365, Batch Acc: 0.004408, Tokens per Sec:     5041, Lr: 0.000035
2022-09-15 07:10:28,726 - INFO - joeynmt.training - Epoch   2, Step:   200600, Batch Loss:     2.595201, Batch Acc: 0.005171, Tokens per Sec:     5043, Lr: 0.000035
2022-09-15 07:10:51,407 - INFO - joeynmt.training - Epoch   2, Step:   200700, Batch Loss:     2.667221, Batch Acc: 0.004604, Tokens per Sec:     5094, Lr: 0.000035
2022-09-15 07:11:14,257 - INFO - joeynmt.training - Epoch   2, Step:   200800, Batch Loss:     2.841647, Batch Acc: 0.004706, Tokens per Sec:     5012, Lr: 0.000035
2022-09-15 07:11:37,105 - INFO - joeynmt.training - Epoch   2, Step:   200900, Batch Loss:     2.453547, Batch Acc: 0.004302, Tokens per Sec:     5108, Lr: 0.000035
2022-09-15 07:11:59,857 - INFO - joeynmt.training - Epoch   2, Step:   201000, Batch Loss:     3.060385, Batch Acc: 0.003742, Tokens per Sec:     5050, Lr: 0.000035
2022-09-15 07:11:59,857 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 07:30:46,159 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 07:30:46,160 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.56, loss:   2.65, ppl:  14.17, acc:   0.50, generation: 1119.1880[sec], evaluation: 6.7653[sec]
2022-09-15 07:30:46,164 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 07:30:47,632 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/198000.ckpt
2022-09-15 07:30:47,674 - INFO - joeynmt.training - Example #0
2022-09-15 07:30:47,687 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 07:30:47,687 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 07:30:47,687 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को ढका हुआ है
2022-09-15 07:30:47,687 - INFO - joeynmt.training - Example #1
2022-09-15 07:30:47,697 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 07:30:47,697 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 07:30:47,697 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 07:30:47,697 - INFO - joeynmt.training - Example #2
2022-09-15 07:30:47,708 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 07:30:47,708 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 07:30:47,708 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और इम
2022-09-15 07:30:47,708 - INFO - joeynmt.training - Example #3
2022-09-15 07:30:47,718 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 07:30:47,718 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 07:30:47,718 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 07:31:10,591 - INFO - joeynmt.training - Epoch   2, Step:   201100, Batch Loss:     2.816682, Batch Acc: 0.003715, Tokens per Sec:     4672, Lr: 0.000035
2022-09-15 07:31:33,399 - INFO - joeynmt.training - Epoch   2, Step:   201200, Batch Loss:     2.891230, Batch Acc: 0.003580, Tokens per Sec:     5071, Lr: 0.000035
2022-09-15 07:31:56,155 - INFO - joeynmt.training - Epoch   2, Step:   201300, Batch Loss:     2.876618, Batch Acc: 0.004161, Tokens per Sec:     5049, Lr: 0.000035
2022-09-15 07:32:18,926 - INFO - joeynmt.training - Epoch   2, Step:   201400, Batch Loss:     2.714729, Batch Acc: 0.003975, Tokens per Sec:     4994, Lr: 0.000035
2022-09-15 07:32:41,603 - INFO - joeynmt.training - Epoch   2, Step:   201500, Batch Loss:     2.729680, Batch Acc: 0.004531, Tokens per Sec:     5032, Lr: 0.000035
2022-09-15 07:33:04,232 - INFO - joeynmt.training - Epoch   2, Step:   201600, Batch Loss:     2.947062, Batch Acc: 0.003950, Tokens per Sec:     5034, Lr: 0.000035
2022-09-15 07:33:26,862 - INFO - joeynmt.training - Epoch   2, Step:   201700, Batch Loss:     2.900627, Batch Acc: 0.004256, Tokens per Sec:     5026, Lr: 0.000035
2022-09-15 07:33:49,615 - INFO - joeynmt.training - Epoch   2, Step:   201800, Batch Loss:     2.492532, Batch Acc: 0.004339, Tokens per Sec:     5035, Lr: 0.000035
2022-09-15 07:34:12,319 - INFO - joeynmt.training - Epoch   2, Step:   201900, Batch Loss:     2.942937, Batch Acc: 0.004208, Tokens per Sec:     5035, Lr: 0.000035
2022-09-15 07:34:35,090 - INFO - joeynmt.training - Epoch   2, Step:   202000, Batch Loss:     2.843464, Batch Acc: 0.004303, Tokens per Sec:     5082, Lr: 0.000035
2022-09-15 07:34:35,090 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 07:53:43,809 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 07:53:43,810 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.65, loss:   2.64, ppl:  14.04, acc:   0.50, generation: 1141.7384[sec], evaluation: 6.6335[sec]
2022-09-15 07:53:43,814 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 07:53:45,241 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/199000.ckpt
2022-09-15 07:53:45,284 - INFO - joeynmt.training - Example #0
2022-09-15 07:53:45,296 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 07:53:45,296 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 07:53:45,296 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को ढका हुआ है
2022-09-15 07:53:45,296 - INFO - joeynmt.training - Example #1
2022-09-15 07:53:45,307 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 07:53:45,307 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 07:53:45,307 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 07:53:45,307 - INFO - joeynmt.training - Example #2
2022-09-15 07:53:45,318 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 07:53:45,318 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 07:53:45,318 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और इम
2022-09-15 07:53:45,318 - INFO - joeynmt.training - Example #3
2022-09-15 07:53:45,329 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 07:53:45,329 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 07:53:45,329 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 07:54:08,206 - INFO - joeynmt.training - Epoch   2, Step:   202100, Batch Loss:     2.837337, Batch Acc: 0.003937, Tokens per Sec:     4821, Lr: 0.000035
2022-09-15 07:54:31,010 - INFO - joeynmt.training - Epoch   2, Step:   202200, Batch Loss:     2.703827, Batch Acc: 0.004480, Tokens per Sec:     5022, Lr: 0.000035
2022-09-15 07:54:53,736 - INFO - joeynmt.training - Epoch   2, Step:   202300, Batch Loss:     2.960382, Batch Acc: 0.004055, Tokens per Sec:     4991, Lr: 0.000035
2022-09-15 07:55:16,351 - INFO - joeynmt.training - Epoch   2, Step:   202400, Batch Loss:     2.675699, Batch Acc: 0.004731, Tokens per Sec:     5122, Lr: 0.000035
2022-09-15 07:55:39,026 - INFO - joeynmt.training - Epoch   2, Step:   202500, Batch Loss:     2.896457, Batch Acc: 0.005305, Tokens per Sec:     4947, Lr: 0.000035
2022-09-15 07:56:01,692 - INFO - joeynmt.training - Epoch   2, Step:   202600, Batch Loss:     2.607372, Batch Acc: 0.004675, Tokens per Sec:     4974, Lr: 0.000035
2022-09-15 07:56:24,485 - INFO - joeynmt.training - Epoch   2, Step:   202700, Batch Loss:     3.147129, Batch Acc: 0.004027, Tokens per Sec:     4990, Lr: 0.000035
2022-09-15 07:56:47,277 - INFO - joeynmt.training - Epoch   2, Step:   202800, Batch Loss:     2.789565, Batch Acc: 0.004729, Tokens per Sec:     5019, Lr: 0.000035
2022-09-15 07:57:10,027 - INFO - joeynmt.training - Epoch   2, Step:   202900, Batch Loss:     2.963069, Batch Acc: 0.003372, Tokens per Sec:     5044, Lr: 0.000035
2022-09-15 07:57:32,859 - INFO - joeynmt.training - Epoch   2, Step:   203000, Batch Loss:     2.904061, Batch Acc: 0.004179, Tokens per Sec:     5031, Lr: 0.000035
2022-09-15 07:57:32,859 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 08:16:00,642 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 08:16:00,644 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.70, loss:   2.64, ppl:  14.00, acc:   0.50, generation: 1100.3010[sec], evaluation: 7.1369[sec]
2022-09-15 08:16:00,648 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 08:16:02,319 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/200000.ckpt
2022-09-15 08:16:02,363 - INFO - joeynmt.training - Example #0
2022-09-15 08:16:02,375 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 08:16:02,375 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 08:16:02,375 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को ढका हुआ है
2022-09-15 08:16:02,375 - INFO - joeynmt.training - Example #1
2022-09-15 08:16:02,385 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 08:16:02,385 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 08:16:02,385 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 08:16:02,385 - INFO - joeynmt.training - Example #2
2022-09-15 08:16:02,396 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 08:16:02,396 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 08:16:02,396 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और इम
2022-09-15 08:16:02,396 - INFO - joeynmt.training - Example #3
2022-09-15 08:16:02,406 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 08:16:02,406 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 08:16:02,406 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 08:16:25,286 - INFO - joeynmt.training - Epoch   2, Step:   203100, Batch Loss:     2.854558, Batch Acc: 0.003747, Tokens per Sec:     4647, Lr: 0.000035
2022-09-15 08:16:48,015 - INFO - joeynmt.training - Epoch   2, Step:   203200, Batch Loss:     2.877663, Batch Acc: 0.004155, Tokens per Sec:     5093, Lr: 0.000035
2022-09-15 08:17:10,742 - INFO - joeynmt.training - Epoch   2, Step:   203300, Batch Loss:     2.634490, Batch Acc: 0.004740, Tokens per Sec:     5106, Lr: 0.000035
2022-09-15 08:17:33,406 - INFO - joeynmt.training - Epoch   2, Step:   203400, Batch Loss:     2.759227, Batch Acc: 0.005043, Tokens per Sec:     5058, Lr: 0.000035
2022-09-15 08:17:56,163 - INFO - joeynmt.training - Epoch   2, Step:   203500, Batch Loss:     2.775618, Batch Acc: 0.004202, Tokens per Sec:     5010, Lr: 0.000035
2022-09-15 08:18:18,882 - INFO - joeynmt.training - Epoch   2, Step:   203600, Batch Loss:     2.750017, Batch Acc: 0.005015, Tokens per Sec:     4986, Lr: 0.000035
2022-09-15 08:18:41,554 - INFO - joeynmt.training - Epoch   2, Step:   203700, Batch Loss:     2.778307, Batch Acc: 0.004405, Tokens per Sec:     5047, Lr: 0.000035
2022-09-15 08:19:04,302 - INFO - joeynmt.training - Epoch   2, Step:   203800, Batch Loss:     2.887502, Batch Acc: 0.004054, Tokens per Sec:     5118, Lr: 0.000035
2022-09-15 08:19:26,953 - INFO - joeynmt.training - Epoch   2, Step:   203900, Batch Loss:     2.624898, Batch Acc: 0.004227, Tokens per Sec:     5035, Lr: 0.000035
2022-09-15 08:19:49,663 - INFO - joeynmt.training - Epoch   2, Step:   204000, Batch Loss:     2.816271, Batch Acc: 0.003912, Tokens per Sec:     5042, Lr: 0.000035
2022-09-15 08:19:49,663 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 09:55:48,356 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -                           cfg.name : transformer_100_enhi_bpe
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -                     cfg.data.train : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/train_tok
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -                       cfg.data.dev : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/val_tok
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -                      cfg.data.test : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/test_tok
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain_ac
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -            cfg.data.src.max_length : 60
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/vocab.en
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/en.bpe.codes
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 16000
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : moses
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : hi
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -            cfg.data.trg.max_length : 60
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/vocab.hi
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/hi.bpe.codes
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 16000
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -                 cfg.testing.n_best : 1
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0
2022-09-15 09:55:48,357 - INFO - joeynmt.helpers -             cfg.testing.batch_size : 1024
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -             cfg.testing.batch_type : token
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -      cfg.testing.max_output_length : 130
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -           cfg.testing.eval_metrics : ['bleu']
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -            cfg.testing.return_prob : none
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -       cfg.testing.return_attention : False
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.tokenize : 13a
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.lowercase : False
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -     cfg.active_learning.query_size : 10000
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -    cfg.active_learning.interactive : False
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -      cfg.active_learning.pool_size : 6
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -     cfg.active_learning.batch_size : 256
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -    cfg.active_learning.num_workers : 4
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -    cfg.active_learning.num_queries : 5
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -     cfg.active_learning.al_percent : 30
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers - cfg.active_learning.query_strategy : margin
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -          cfg.active_learning.epoch : 1
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers - cfg.active_learning.validation_freq : 1000
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -            cfg.training.load_model : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/best.ckpt
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -       cfg.training.reset_best_ckpt : False
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -       cfg.training.reset_scheduler : False
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -       cfg.training.reset_optimizer : False
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -      cfg.training.reset_iter_state : False
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers - cfg.training.learning_rate_decay_length : 2500
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -    cfg.training.learning_rate_peak : 0.005
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -  cfg.training.learning_rate_warmup : 1000
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -  cfg.training.learning_rate_factor : 0.5
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -              cfg.training.patience : 5
2022-09-15 09:55:48,358 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 1e-08
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.1
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -            cfg.training.batch_size : 4096
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 1
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -                cfg.training.epochs : 1
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 1000
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -             cfg.training.model_dir : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3]
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 3
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : False
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2022-09-15 09:55:48,359 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 1024
2022-09-15 09:55:48,360 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.3
2022-09-15 09:55:48,360 - INFO - joeynmt.helpers -       cfg.model.encoder.layer_norm : pre
2022-09-15 09:55:48,360 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2022-09-15 09:55:48,360 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6
2022-09-15 09:55:48,360 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 4
2022-09-15 09:55:48,360 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2022-09-15 09:55:48,360 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2022-09-15 09:55:48,360 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2
2022-09-15 09:55:48,360 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2022-09-15 09:55:48,360 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 1024
2022-09-15 09:55:48,360 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.3
2022-09-15 09:55:48,360 - INFO - joeynmt.helpers -       cfg.model.decoder.layer_norm : pre
2022-09-15 09:55:49,742 - INFO - joeynmt.data - Building tokenizer...
2022-09-15 09:55:50,218 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 09:55:50,218 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 09:55:50,218 - INFO - joeynmt.data - Loading train set...
2022-09-15 09:58:56,524 - INFO - joeynmt.data - Building vocabulary...
2022-09-15 09:58:58,650 - INFO - joeynmt.data - Loading dev set...
2022-09-15 09:59:04,386 - INFO - joeynmt.data - Loading test set...
2022-09-15 09:59:09,986 - INFO - joeynmt.data - Data loaded.
2022-09-15 09:59:09,986 - INFO - joeynmt.helpers - Train dataset: PlaintextDatasetAC(split=train, len=1552563, src_lang=en, trg_lang=hi, has_trg=True, random_subset=-1)
2022-09-15 09:59:09,987 - INFO - joeynmt.helpers - Valid dataset: PlaintextDatasetAC(split=dev, len=40856, src_lang=en, trg_lang=hi, has_trg=True, random_subset=-1)
2022-09-15 09:59:09,987 - INFO - joeynmt.helpers -  Test dataset: PlaintextDatasetAC(split=test, len=40858, src_lang=en, trg_lang=hi, has_trg=True, random_subset=-1)
2022-09-15 09:59:09,987 - INFO - joeynmt.helpers - First training example:
	[SRC] give your application an accessibility work@@ out
	[TRG] अपने अनुप्रयोग को पहुंच@@ नीयता व्यायाम का लाभ दें
2022-09-15 09:59:09,987 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) , (6) . (7) of (8) and (9) to
2022-09-15 09:59:09,987 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) के (5) है (6) । (7) , (8) और (9) में
2022-09-15 09:59:09,987 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 16021
2022-09-15 09:59:09,987 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 16174
2022-09-15 09:59:10,086 - INFO - joeynmt.training - BASELINE MODEL START
2022-09-15 09:59:10,086 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 09:59:10,086 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 09:59:10,837 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 09:59:14,173 - INFO - joeynmt.model - Total params: 19302144
2022-09-15 09:59:14,174 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm="pre"),
	decoder=TransformerDecoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm="pre"),
	src_embed=Embeddings(embedding_dim=256, vocab_size=16021),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=16174),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.1))
2022-09-15 09:59:28,719 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=[0.9, 0.999])
2022-09-15 09:59:28,719 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=5)
2022-09-15 09:59:28,719 - INFO - joeynmt.training - Loading model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/best.ckpt
2022-09-15 09:59:32,880 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 09:59:32,935 - INFO - joeynmt.training - BASELINE MODEL END
2022-09-15 09:59:32,936 - INFO - joeynmt.training - Random Indices picked: [121958 146867 131932 365838 259178 119879 110268 207892  54886 137337] length: 27946
2022-09-15 09:59:32,936 - INFO - joeynmt.training - Executing Random Strategy
2022-09-15 09:59:32,936 - INFO - joeynmt.training - Final Query Indices picked: [121958, 146867, 131932, 365838, 259178, 119879, 110268, 207892, 54886, 137337] length: 10000
2022-09-15 09:59:32,936 - INFO - joeynmt.training - Query the samples, one at a time (interactive/file)
2022-09-15 09:59:34,541 - INFO - joeynmt.training - ACTIVE LEARNING MODEL END - RANDOM
2022-09-15 09:59:34,541 - INFO - joeynmt.training - ACTIVE LEARNING MODEL START - MARGIN 0
2022-09-15 09:59:34,541 - INFO - joeynmt.training - Random Indices picked: [ 21058 388370 228845 308840 240035  38872  54971 344227  23306 421426] length: 27353
2022-09-15 09:59:34,542 - INFO - joeynmt.training - Processing Predictions on Batch 0/107
2022-09-15 09:59:36,808 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 09:59:36,808 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 09:59:37,024 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 09:59:37,165 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 09:59:37,304 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 09:59:37,304 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 09:59:37,334 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 09:59:45,127 - INFO - joeynmt.prediction - Generation took 7.7847[sec]. (No references given)
2022-09-15 09:59:45,135 - INFO - joeynmt.training - Processing Predictions on Batch 1/107
2022-09-15 09:59:47,319 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 09:59:47,319 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 09:59:47,534 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 09:59:47,666 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 09:59:47,749 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 09:59:47,749 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 09:59:47,779 - INFO - joeynmt.prediction - Predicting 248 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 09:59:55,123 - INFO - joeynmt.prediction - Generation took 7.3354[sec]. (No references given)
2022-09-15 09:59:55,131 - INFO - joeynmt.training - Processing Predictions on Batch 2/107
2022-09-15 09:59:57,342 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 09:59:57,342 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 09:59:57,556 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 09:59:57,689 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 09:59:57,774 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 09:59:57,774 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 09:59:57,803 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:00:03,677 - INFO - joeynmt.prediction - Generation took 5.8591[sec]. (No references given)
2022-09-15 10:00:03,685 - INFO - joeynmt.training - Processing Predictions on Batch 3/107
2022-09-15 10:00:05,910 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:00:05,910 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:00:06,123 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:00:06,257 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:00:06,342 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:00:06,342 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:00:06,372 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:00:13,123 - INFO - joeynmt.prediction - Generation took 6.7429[sec]. (No references given)
2022-09-15 10:00:13,132 - INFO - joeynmt.training - Processing Predictions on Batch 4/107
2022-09-15 10:00:15,366 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:00:15,366 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:00:15,581 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:00:15,714 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:00:15,800 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:00:15,800 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:00:15,828 - INFO - joeynmt.prediction - Predicting 251 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:00:23,781 - INFO - joeynmt.prediction - Generation took 7.9444[sec]. (No references given)
2022-09-15 10:00:23,790 - INFO - joeynmt.training - Processing Predictions on Batch 5/107
2022-09-15 10:00:26,055 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:00:26,055 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:00:26,268 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:00:26,456 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:00:26,541 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:00:26,541 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:00:26,568 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:00:33,322 - INFO - joeynmt.prediction - Generation took 6.7466[sec]. (No references given)
2022-09-15 10:00:33,331 - INFO - joeynmt.training - Processing Predictions on Batch 6/107
2022-09-15 10:00:35,583 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:00:35,584 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:00:35,802 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:00:35,933 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:00:36,019 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:00:36,019 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:00:36,052 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:00:42,133 - INFO - joeynmt.prediction - Generation took 6.0726[sec]. (No references given)
2022-09-15 10:00:42,142 - INFO - joeynmt.training - Processing Predictions on Batch 7/107
2022-09-15 10:00:44,400 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:00:44,400 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:00:44,612 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:00:44,745 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:00:44,830 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:00:44,831 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:00:44,858 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:00:51,892 - INFO - joeynmt.prediction - Generation took 7.0263[sec]. (No references given)
2022-09-15 10:00:51,901 - INFO - joeynmt.training - Processing Predictions on Batch 8/107
2022-09-15 10:00:54,172 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:00:54,172 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:00:54,386 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:00:54,518 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:00:54,603 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:00:54,603 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:00:54,633 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:01:02,023 - INFO - joeynmt.prediction - Generation took 7.3814[sec]. (No references given)
2022-09-15 10:01:02,031 - INFO - joeynmt.training - Processing Predictions on Batch 9/107
2022-09-15 10:01:04,298 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:01:04,298 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:01:04,511 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:01:04,642 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:01:04,728 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:01:04,728 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:01:04,760 - INFO - joeynmt.prediction - Predicting 250 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:01:11,104 - INFO - joeynmt.prediction - Generation took 6.3356[sec]. (No references given)
2022-09-15 10:01:11,113 - INFO - joeynmt.training - Processing Predictions on Batch 10/107
2022-09-15 10:01:13,362 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:01:13,362 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:01:13,576 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:01:13,709 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:01:13,850 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:01:13,850 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:01:13,880 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:01:20,520 - INFO - joeynmt.prediction - Generation took 6.6321[sec]. (No references given)
2022-09-15 10:01:20,529 - INFO - joeynmt.training - Processing Predictions on Batch 11/107
2022-09-15 10:01:22,751 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:01:22,751 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:01:22,967 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:01:23,098 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:01:23,184 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:01:23,184 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:01:23,216 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:01:31,273 - INFO - joeynmt.prediction - Generation took 8.0491[sec]. (No references given)
2022-09-15 10:01:31,282 - INFO - joeynmt.training - Processing Predictions on Batch 12/107
2022-09-15 10:01:33,489 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:01:33,490 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:01:33,705 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:01:33,838 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:01:33,923 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:01:33,923 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:01:33,955 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:01:41,835 - INFO - joeynmt.prediction - Generation took 7.8719[sec]. (No references given)
2022-09-15 10:01:41,844 - INFO - joeynmt.training - Processing Predictions on Batch 13/107
2022-09-15 10:01:44,072 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:01:44,072 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:01:44,288 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:01:44,478 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:01:44,562 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:01:44,562 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:01:44,591 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:01:50,915 - INFO - joeynmt.prediction - Generation took 6.3152[sec]. (No references given)
2022-09-15 10:01:50,923 - INFO - joeynmt.training - Processing Predictions on Batch 14/107
2022-09-15 10:01:53,165 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:01:53,165 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:01:53,378 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:01:53,515 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:01:53,600 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:01:53,600 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:01:53,632 - INFO - joeynmt.prediction - Predicting 248 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:02:00,144 - INFO - joeynmt.prediction - Generation took 6.5036[sec]. (No references given)
2022-09-15 10:02:00,152 - INFO - joeynmt.training - Processing Predictions on Batch 15/107
2022-09-15 10:02:02,393 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:02:02,394 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:02:02,609 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:02:02,742 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:02:02,827 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:02:02,827 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:02:02,856 - INFO - joeynmt.prediction - Predicting 251 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:02:10,884 - INFO - joeynmt.prediction - Generation took 8.0189[sec]. (No references given)
2022-09-15 10:02:10,892 - INFO - joeynmt.training - Processing Predictions on Batch 16/107
2022-09-15 10:02:13,161 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:02:13,161 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:02:13,376 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:02:13,510 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:02:13,593 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:02:13,593 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:02:13,623 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:02:20,246 - INFO - joeynmt.prediction - Generation took 6.6150[sec]. (No references given)
2022-09-15 10:02:20,255 - INFO - joeynmt.training - Processing Predictions on Batch 17/107
2022-09-15 10:02:22,538 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:02:22,538 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:02:22,752 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:02:22,885 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:02:22,969 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:02:22,969 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:02:22,999 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:02:30,573 - INFO - joeynmt.prediction - Generation took 7.5665[sec]. (No references given)
2022-09-15 10:02:30,582 - INFO - joeynmt.training - Processing Predictions on Batch 18/107
2022-09-15 10:02:32,877 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:02:32,877 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:02:33,147 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:02:33,279 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:02:33,364 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:02:33,364 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:02:33,396 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:02:41,091 - INFO - joeynmt.prediction - Generation took 7.6864[sec]. (No references given)
2022-09-15 10:02:41,100 - INFO - joeynmt.training - Processing Predictions on Batch 19/107
2022-09-15 10:02:43,411 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:02:43,411 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:02:43,628 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:02:43,759 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:02:43,843 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:02:43,843 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:02:43,874 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:02:50,859 - INFO - joeynmt.prediction - Generation took 6.9773[sec]. (No references given)
2022-09-15 10:02:50,868 - INFO - joeynmt.training - Processing Predictions on Batch 20/107
2022-09-15 10:02:53,168 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:02:53,168 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:02:53,382 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:02:53,513 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:02:53,598 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:02:53,598 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:02:53,627 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:03:01,178 - INFO - joeynmt.prediction - Generation took 7.5425[sec]. (No references given)
2022-09-15 10:03:01,187 - INFO - joeynmt.training - Processing Predictions on Batch 21/107
2022-09-15 10:03:03,480 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:03:03,480 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:03:03,693 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:03:03,826 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:03:03,911 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:03:03,911 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:03:03,940 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:03:10,844 - INFO - joeynmt.prediction - Generation took 6.8953[sec]. (No references given)
2022-09-15 10:03:10,853 - INFO - joeynmt.training - Processing Predictions on Batch 22/107
2022-09-15 10:03:13,191 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:03:13,191 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:03:13,409 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:03:13,540 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:03:13,625 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:03:13,625 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:03:13,653 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:03:20,213 - INFO - joeynmt.prediction - Generation took 6.5515[sec]. (No references given)
2022-09-15 10:03:20,222 - INFO - joeynmt.training - Processing Predictions on Batch 23/107
2022-09-15 10:03:22,513 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:03:22,513 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:03:22,727 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:03:22,863 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:03:22,949 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:03:22,949 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:03:22,976 - INFO - joeynmt.prediction - Predicting 248 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:03:29,044 - INFO - joeynmt.prediction - Generation took 6.0599[sec]. (No references given)
2022-09-15 10:03:29,053 - INFO - joeynmt.training - Processing Predictions on Batch 24/107
2022-09-15 10:03:31,350 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:03:31,350 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:03:31,564 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:03:31,697 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:03:31,780 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:03:31,780 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:03:31,810 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:03:39,799 - INFO - joeynmt.prediction - Generation took 7.9811[sec]. (No references given)
2022-09-15 10:03:39,807 - INFO - joeynmt.training - Processing Predictions on Batch 25/107
2022-09-15 10:03:42,121 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:03:42,121 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:03:42,336 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:03:42,466 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:03:42,552 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:03:42,552 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:03:42,580 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:03:48,575 - INFO - joeynmt.prediction - Generation took 5.9872[sec]. (No references given)
2022-09-15 10:03:48,584 - INFO - joeynmt.training - Processing Predictions on Batch 26/107
2022-09-15 10:03:50,910 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:03:50,910 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:03:51,123 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:03:51,255 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:03:51,339 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:03:51,339 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:03:51,368 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:03:58,195 - INFO - joeynmt.prediction - Generation took 6.8181[sec]. (No references given)
2022-09-15 10:03:58,203 - INFO - joeynmt.training - Processing Predictions on Batch 27/107
2022-09-15 10:04:00,575 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:04:00,575 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:04:00,789 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:04:00,922 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:04:01,005 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:04:01,005 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:04:01,035 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:04:07,863 - INFO - joeynmt.prediction - Generation took 6.8196[sec]. (No references given)
2022-09-15 10:04:07,871 - INFO - joeynmt.training - Processing Predictions on Batch 28/107
2022-09-15 10:04:10,197 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:04:10,197 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:04:10,410 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:04:10,541 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:04:10,627 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:04:10,627 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:04:10,655 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:04:17,147 - INFO - joeynmt.prediction - Generation took 6.4846[sec]. (No references given)
2022-09-15 10:04:17,156 - INFO - joeynmt.training - Processing Predictions on Batch 29/107
2022-09-15 10:04:19,458 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:04:19,458 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:04:19,671 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:04:19,807 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:04:19,891 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:04:19,891 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:04:19,921 - INFO - joeynmt.prediction - Predicting 251 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:04:27,588 - INFO - joeynmt.prediction - Generation took 7.6588[sec]. (No references given)
2022-09-15 10:04:27,597 - INFO - joeynmt.training - Processing Predictions on Batch 30/107
2022-09-15 10:04:29,883 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:04:29,883 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:04:30,099 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:04:30,234 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:04:30,317 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:04:30,317 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:04:30,349 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:04:38,121 - INFO - joeynmt.prediction - Generation took 7.7638[sec]. (No references given)
2022-09-15 10:04:38,130 - INFO - joeynmt.training - Processing Predictions on Batch 31/107
2022-09-15 10:04:40,430 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:04:40,430 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:04:40,643 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:04:40,774 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:04:40,858 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:04:40,858 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:04:40,887 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:04:48,087 - INFO - joeynmt.prediction - Generation took 7.1913[sec]. (No references given)
2022-09-15 10:04:48,095 - INFO - joeynmt.training - Processing Predictions on Batch 32/107
2022-09-15 10:04:50,399 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:04:50,399 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:04:50,612 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:04:50,744 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:04:50,828 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:04:50,829 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:04:50,859 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:04:57,713 - INFO - joeynmt.prediction - Generation took 6.8461[sec]. (No references given)
2022-09-15 10:04:57,722 - INFO - joeynmt.training - Processing Predictions on Batch 33/107
2022-09-15 10:05:00,029 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:05:00,029 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:05:00,244 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:05:00,376 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:05:00,459 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:05:00,459 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:05:00,490 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:05:07,191 - INFO - joeynmt.prediction - Generation took 6.6926[sec]. (No references given)
2022-09-15 10:05:07,200 - INFO - joeynmt.training - Processing Predictions on Batch 34/107
2022-09-15 10:05:09,501 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:05:09,501 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:05:09,714 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:05:09,844 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:05:09,929 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:05:09,929 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:05:09,960 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:05:17,717 - INFO - joeynmt.prediction - Generation took 7.7488[sec]. (No references given)
2022-09-15 10:05:17,725 - INFO - joeynmt.training - Processing Predictions on Batch 35/107
2022-09-15 10:05:20,036 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:05:20,036 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:05:20,249 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:05:20,382 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:05:20,467 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:05:20,467 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:05:20,496 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:05:26,698 - INFO - joeynmt.prediction - Generation took 6.1931[sec]. (No references given)
2022-09-15 10:05:26,706 - INFO - joeynmt.training - Processing Predictions on Batch 36/107
2022-09-15 10:05:29,006 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:05:29,006 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:05:29,219 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:05:29,353 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:05:29,437 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:05:29,437 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:05:29,466 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:05:38,542 - INFO - joeynmt.prediction - Generation took 9.0676[sec]. (No references given)
2022-09-15 10:05:38,551 - INFO - joeynmt.training - Processing Predictions on Batch 37/107
2022-09-15 10:05:40,852 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:05:40,852 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:05:41,066 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:05:41,197 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:05:41,281 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:05:41,281 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:05:41,313 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:05:48,294 - INFO - joeynmt.prediction - Generation took 6.9725[sec]. (No references given)
2022-09-15 10:05:48,302 - INFO - joeynmt.training - Processing Predictions on Batch 38/107
2022-09-15 10:05:50,593 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:05:50,593 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:05:50,807 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:05:50,939 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:05:51,023 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:05:51,023 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:05:51,052 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:05:57,300 - INFO - joeynmt.prediction - Generation took 6.2394[sec]. (No references given)
2022-09-15 10:05:57,308 - INFO - joeynmt.training - Processing Predictions on Batch 39/107
2022-09-15 10:05:59,600 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:05:59,601 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:05:59,817 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:05:59,950 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:06:00,033 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:06:00,034 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:06:00,066 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:06:07,855 - INFO - joeynmt.prediction - Generation took 7.7797[sec]. (No references given)
2022-09-15 10:06:07,863 - INFO - joeynmt.training - Processing Predictions on Batch 40/107
2022-09-15 10:06:10,167 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:06:10,168 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:06:10,380 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:06:10,511 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:06:10,596 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:06:10,596 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:06:10,627 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:06:17,636 - INFO - joeynmt.prediction - Generation took 7.0013[sec]. (No references given)
2022-09-15 10:06:17,645 - INFO - joeynmt.training - Processing Predictions on Batch 41/107
2022-09-15 10:06:19,934 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:06:19,934 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:06:20,149 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:06:20,282 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:06:20,365 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:06:20,365 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:06:20,396 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:06:27,323 - INFO - joeynmt.prediction - Generation took 6.9178[sec]. (No references given)
2022-09-15 10:06:27,331 - INFO - joeynmt.training - Processing Predictions on Batch 42/107
2022-09-15 10:06:29,632 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:06:29,632 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:06:29,845 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:06:29,979 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:06:30,063 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:06:30,063 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:06:30,091 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:06:36,212 - INFO - joeynmt.prediction - Generation took 6.1123[sec]. (No references given)
2022-09-15 10:06:36,220 - INFO - joeynmt.training - Processing Predictions on Batch 43/107
2022-09-15 10:06:38,513 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:06:38,513 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:06:38,727 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:06:38,858 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:06:38,942 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:06:38,942 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:06:38,975 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:06:45,477 - INFO - joeynmt.prediction - Generation took 6.4933[sec]. (No references given)
2022-09-15 10:06:45,485 - INFO - joeynmt.training - Processing Predictions on Batch 44/107
2022-09-15 10:06:47,796 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:06:47,796 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:06:48,009 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:06:48,139 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:06:48,223 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:06:48,223 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:06:48,253 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:06:54,875 - INFO - joeynmt.prediction - Generation took 6.6140[sec]. (No references given)
2022-09-15 10:06:54,884 - INFO - joeynmt.training - Processing Predictions on Batch 45/107
2022-09-15 10:06:57,188 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:06:57,188 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:06:57,401 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:06:57,533 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:06:57,679 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:06:57,679 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:06:57,707 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:07:04,333 - INFO - joeynmt.prediction - Generation took 6.6173[sec]. (No references given)
2022-09-15 10:07:04,341 - INFO - joeynmt.training - Processing Predictions on Batch 46/107
2022-09-15 10:07:06,638 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:07:06,638 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:07:06,850 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:07:06,983 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:07:07,067 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:07:07,067 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:07:07,096 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:07:14,209 - INFO - joeynmt.prediction - Generation took 7.1048[sec]. (No references given)
2022-09-15 10:07:14,217 - INFO - joeynmt.training - Processing Predictions on Batch 47/107
2022-09-15 10:07:16,537 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:07:16,537 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:07:16,751 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:07:16,882 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:07:16,965 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:07:16,965 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:07:16,993 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:07:22,738 - INFO - joeynmt.prediction - Generation took 5.7377[sec]. (No references given)
2022-09-15 10:07:22,747 - INFO - joeynmt.training - Processing Predictions on Batch 48/107
2022-09-15 10:07:25,066 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:07:25,066 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:07:25,281 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:07:25,412 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:07:25,559 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:07:25,559 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:07:25,589 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:07:32,578 - INFO - joeynmt.prediction - Generation took 6.9816[sec]. (No references given)
2022-09-15 10:07:32,587 - INFO - joeynmt.training - Processing Predictions on Batch 49/107
2022-09-15 10:07:34,897 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:07:34,897 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:07:35,110 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:07:35,242 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:07:35,327 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:07:35,327 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:07:35,358 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:07:41,053 - INFO - joeynmt.prediction - Generation took 5.6870[sec]. (No references given)
2022-09-15 10:07:41,061 - INFO - joeynmt.training - Processing Predictions on Batch 50/107
2022-09-15 10:07:43,375 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:07:43,376 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:07:43,589 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:07:43,723 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:07:43,805 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:07:43,805 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:07:43,837 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:07:52,030 - INFO - joeynmt.prediction - Generation took 8.1847[sec]. (No references given)
2022-09-15 10:07:52,039 - INFO - joeynmt.training - Processing Predictions on Batch 51/107
2022-09-15 10:07:54,354 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:07:54,354 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:07:54,569 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:07:54,767 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:07:54,850 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:07:54,850 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:07:54,876 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:08:00,891 - INFO - joeynmt.prediction - Generation took 6.0080[sec]. (No references given)
2022-09-15 10:08:00,900 - INFO - joeynmt.training - Processing Predictions on Batch 52/107
2022-09-15 10:08:03,210 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:08:03,210 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:08:03,424 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:08:03,555 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:08:03,640 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:08:03,640 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:08:03,671 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:08:10,358 - INFO - joeynmt.prediction - Generation took 6.6791[sec]. (No references given)
2022-09-15 10:08:10,367 - INFO - joeynmt.training - Processing Predictions on Batch 53/107
2022-09-15 10:08:12,676 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:08:12,676 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:08:12,890 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:08:13,020 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:08:13,104 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:08:13,104 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:08:13,135 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:08:20,845 - INFO - joeynmt.prediction - Generation took 7.7012[sec]. (No references given)
2022-09-15 10:08:20,853 - INFO - joeynmt.training - Processing Predictions on Batch 54/107
2022-09-15 10:08:23,155 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:08:23,155 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:08:23,369 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:08:23,500 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:08:23,584 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:08:23,584 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:08:23,616 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:08:33,638 - INFO - joeynmt.prediction - Generation took 10.0125[sec]. (No references given)
2022-09-15 10:08:33,647 - INFO - joeynmt.training - Processing Predictions on Batch 55/107
2022-09-15 10:08:35,945 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:08:35,945 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:08:36,160 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:08:36,296 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:08:36,380 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:08:36,380 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:08:36,408 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:08:41,757 - INFO - joeynmt.prediction - Generation took 5.3410[sec]. (No references given)
2022-09-15 10:08:41,765 - INFO - joeynmt.training - Processing Predictions on Batch 56/107
2022-09-15 10:08:44,084 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:08:44,084 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:08:44,297 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:08:44,430 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:08:44,513 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:08:44,513 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:08:44,543 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:08:52,024 - INFO - joeynmt.prediction - Generation took 7.4735[sec]. (No references given)
2022-09-15 10:08:52,033 - INFO - joeynmt.training - Processing Predictions on Batch 57/107
2022-09-15 10:08:54,352 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:08:54,352 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:08:54,568 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:08:54,701 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:08:54,784 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:08:54,784 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:08:54,813 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:09:03,829 - INFO - joeynmt.prediction - Generation took 9.0068[sec]. (No references given)
2022-09-15 10:09:03,837 - INFO - joeynmt.training - Processing Predictions on Batch 58/107
2022-09-15 10:09:06,127 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:09:06,127 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:09:06,342 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:09:06,473 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:09:06,558 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:09:06,558 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:09:06,588 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:09:12,331 - INFO - joeynmt.prediction - Generation took 5.7344[sec]. (No references given)
2022-09-15 10:09:12,339 - INFO - joeynmt.training - Processing Predictions on Batch 59/107
2022-09-15 10:09:14,637 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:09:14,637 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:09:14,850 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:09:14,981 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:09:15,066 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:09:15,066 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:09:15,093 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:09:23,200 - INFO - joeynmt.prediction - Generation took 8.0996[sec]. (No references given)
2022-09-15 10:09:23,209 - INFO - joeynmt.training - Processing Predictions on Batch 60/107
2022-09-15 10:09:25,519 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:09:25,519 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:09:25,733 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:09:25,865 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:09:25,952 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:09:25,953 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:09:25,983 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:09:34,417 - INFO - joeynmt.prediction - Generation took 8.4249[sec]. (No references given)
2022-09-15 10:09:34,425 - INFO - joeynmt.training - Processing Predictions on Batch 61/107
2022-09-15 10:09:36,719 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:09:36,719 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:09:36,932 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:09:37,067 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:09:37,153 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:09:37,153 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:09:37,185 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:09:45,183 - INFO - joeynmt.prediction - Generation took 7.9896[sec]. (No references given)
2022-09-15 10:09:45,192 - INFO - joeynmt.training - Processing Predictions on Batch 62/107
2022-09-15 10:09:47,489 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:09:47,489 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:09:47,704 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:09:47,836 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:09:47,919 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:09:47,919 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:09:47,947 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:09:55,341 - INFO - joeynmt.prediction - Generation took 7.3859[sec]. (No references given)
2022-09-15 10:09:55,349 - INFO - joeynmt.training - Processing Predictions on Batch 63/107
2022-09-15 10:09:57,641 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:09:57,641 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:09:57,856 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:09:57,987 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:09:58,070 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:09:58,070 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:09:58,100 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:10:05,962 - INFO - joeynmt.prediction - Generation took 7.8529[sec]. (No references given)
2022-09-15 10:10:05,971 - INFO - joeynmt.training - Processing Predictions on Batch 64/107
2022-09-15 10:10:08,267 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:10:08,267 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:10:08,481 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:10:08,612 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:10:08,696 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:10:08,697 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:10:08,725 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:10:16,891 - INFO - joeynmt.prediction - Generation took 8.1581[sec]. (No references given)
2022-09-15 10:10:16,900 - INFO - joeynmt.training - Processing Predictions on Batch 65/107
2022-09-15 10:10:19,207 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:10:19,207 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:10:19,425 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:10:19,556 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:10:19,707 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:10:19,707 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:10:19,735 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:10:27,664 - INFO - joeynmt.prediction - Generation took 7.9209[sec]. (No references given)
2022-09-15 10:10:27,673 - INFO - joeynmt.training - Processing Predictions on Batch 66/107
2022-09-15 10:10:29,959 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:10:29,959 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:10:30,172 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:10:30,307 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:10:30,390 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:10:30,391 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:10:30,419 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:10:36,942 - INFO - joeynmt.prediction - Generation took 6.5147[sec]. (No references given)
2022-09-15 10:10:36,951 - INFO - joeynmt.training - Processing Predictions on Batch 67/107
2022-09-15 10:10:39,259 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:10:39,260 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:10:39,472 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:10:39,605 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:10:39,689 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:10:39,689 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:10:39,718 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:10:47,604 - INFO - joeynmt.prediction - Generation took 7.8783[sec]. (No references given)
2022-09-15 10:10:47,613 - INFO - joeynmt.training - Processing Predictions on Batch 68/107
2022-09-15 10:10:49,922 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:10:49,922 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:10:50,135 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:10:50,267 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:10:50,419 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:10:50,419 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:10:50,446 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:10:57,611 - INFO - joeynmt.prediction - Generation took 7.1580[sec]. (No references given)
2022-09-15 10:10:57,620 - INFO - joeynmt.training - Processing Predictions on Batch 69/107
2022-09-15 10:10:59,935 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:10:59,935 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:11:00,148 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:11:00,282 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:11:00,365 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:11:00,365 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:11:00,394 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:11:05,577 - INFO - joeynmt.prediction - Generation took 5.1753[sec]. (No references given)
2022-09-15 10:11:05,586 - INFO - joeynmt.training - Processing Predictions on Batch 70/107
2022-09-15 10:11:07,901 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:11:07,901 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:11:08,115 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:11:08,246 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:11:08,329 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:11:08,329 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:11:08,358 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:11:15,128 - INFO - joeynmt.prediction - Generation took 6.7618[sec]. (No references given)
2022-09-15 10:11:15,137 - INFO - joeynmt.training - Processing Predictions on Batch 71/107
2022-09-15 10:11:17,455 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:11:17,455 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:11:17,669 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:11:17,800 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:11:17,889 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:11:17,889 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:11:17,915 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:11:25,567 - INFO - joeynmt.prediction - Generation took 7.6440[sec]. (No references given)
2022-09-15 10:11:25,575 - INFO - joeynmt.training - Processing Predictions on Batch 72/107
2022-09-15 10:11:27,895 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:11:27,895 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:11:28,109 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:11:28,242 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:11:28,325 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:11:28,326 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:11:28,354 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:11:33,569 - INFO - joeynmt.prediction - Generation took 5.2075[sec]. (No references given)
2022-09-15 10:11:33,577 - INFO - joeynmt.training - Processing Predictions on Batch 73/107
2022-09-15 10:11:35,891 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:11:35,891 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:11:36,105 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:11:36,237 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:11:36,320 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:11:36,321 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:11:36,351 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:11:42,833 - INFO - joeynmt.prediction - Generation took 6.4735[sec]. (No references given)
2022-09-15 10:11:42,842 - INFO - joeynmt.training - Processing Predictions on Batch 74/107
2022-09-15 10:11:45,165 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:11:45,165 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:11:45,378 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:11:45,510 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:11:45,595 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:11:45,595 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:11:45,624 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:11:55,123 - INFO - joeynmt.prediction - Generation took 9.4907[sec]. (No references given)
2022-09-15 10:11:55,131 - INFO - joeynmt.training - Processing Predictions on Batch 75/107
2022-09-15 10:11:57,451 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:11:57,451 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:11:57,665 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:11:57,799 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:11:57,883 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:11:57,883 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:11:57,913 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:12:04,265 - INFO - joeynmt.prediction - Generation took 6.3446[sec]. (No references given)
2022-09-15 10:12:04,274 - INFO - joeynmt.training - Processing Predictions on Batch 76/107
2022-09-15 10:12:06,578 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:12:06,578 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:12:06,794 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:12:06,925 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:12:07,009 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:12:07,009 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:12:07,038 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:12:14,535 - INFO - joeynmt.prediction - Generation took 7.4888[sec]. (No references given)
2022-09-15 10:12:14,543 - INFO - joeynmt.training - Processing Predictions on Batch 77/107
2022-09-15 10:12:16,829 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:12:16,829 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:12:17,044 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:12:17,175 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:12:17,259 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:12:17,259 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:12:17,287 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:12:24,940 - INFO - joeynmt.prediction - Generation took 7.6446[sec]. (No references given)
2022-09-15 10:12:24,948 - INFO - joeynmt.training - Processing Predictions on Batch 78/107
2022-09-15 10:12:27,279 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:12:27,279 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:12:27,495 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:12:27,631 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:12:27,788 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:12:27,788 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:12:27,817 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:12:35,753 - INFO - joeynmt.prediction - Generation took 7.9275[sec]. (No references given)
2022-09-15 10:12:35,762 - INFO - joeynmt.training - Processing Predictions on Batch 79/107
2022-09-15 10:12:38,067 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:12:38,067 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:12:38,282 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:12:38,418 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:12:38,502 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:12:38,502 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:12:38,530 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:12:46,610 - INFO - joeynmt.prediction - Generation took 8.0716[sec]. (No references given)
2022-09-15 10:12:46,619 - INFO - joeynmt.training - Processing Predictions on Batch 80/107
2022-09-15 10:12:48,964 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:12:48,964 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:12:49,178 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:12:49,315 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:12:49,399 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:12:49,399 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:12:49,427 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:12:59,875 - INFO - joeynmt.prediction - Generation took 10.4388[sec]. (No references given)
2022-09-15 10:12:59,883 - INFO - joeynmt.training - Processing Predictions on Batch 81/107
2022-09-15 10:13:02,197 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:13:02,197 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:13:02,624 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:13:02,763 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:13:02,847 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:13:02,847 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:13:02,877 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:13:10,050 - INFO - joeynmt.prediction - Generation took 7.1646[sec]. (No references given)
2022-09-15 10:13:10,058 - INFO - joeynmt.training - Processing Predictions on Batch 82/107
2022-09-15 10:13:12,378 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:13:12,378 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:13:12,591 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:13:12,723 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:13:12,807 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:13:12,807 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:13:12,838 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:13:19,113 - INFO - joeynmt.prediction - Generation took 6.2664[sec]. (No references given)
2022-09-15 10:13:19,122 - INFO - joeynmt.training - Processing Predictions on Batch 83/107
2022-09-15 10:13:21,433 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:13:21,434 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:13:21,648 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:13:21,780 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:13:21,864 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:13:21,864 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:13:21,895 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:13:28,843 - INFO - joeynmt.prediction - Generation took 6.9387[sec]. (No references given)
2022-09-15 10:13:28,851 - INFO - joeynmt.training - Processing Predictions on Batch 84/107
2022-09-15 10:13:31,168 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:13:31,168 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:13:31,384 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:13:31,514 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:13:31,600 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:13:31,600 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:13:31,631 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:13:40,391 - INFO - joeynmt.prediction - Generation took 8.7518[sec]. (No references given)
2022-09-15 10:13:40,400 - INFO - joeynmt.training - Processing Predictions on Batch 85/107
2022-09-15 10:13:42,695 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:13:42,695 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:13:42,911 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:13:43,048 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:13:43,203 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:13:43,203 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:13:43,230 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:13:48,250 - INFO - joeynmt.prediction - Generation took 5.0127[sec]. (No references given)
2022-09-15 10:13:48,259 - INFO - joeynmt.training - Processing Predictions on Batch 86/107
2022-09-15 10:13:50,561 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:13:50,561 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:13:50,776 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:13:50,909 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:13:50,992 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:13:50,992 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:13:51,021 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:13:57,203 - INFO - joeynmt.prediction - Generation took 6.1743[sec]. (No references given)
2022-09-15 10:13:57,211 - INFO - joeynmt.training - Processing Predictions on Batch 87/107
2022-09-15 10:13:59,522 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:13:59,522 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:13:59,735 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:13:59,868 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:13:59,951 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:13:59,951 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:13:59,980 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:14:07,149 - INFO - joeynmt.prediction - Generation took 7.1611[sec]. (No references given)
2022-09-15 10:14:07,158 - INFO - joeynmt.training - Processing Predictions on Batch 88/107
2022-09-15 10:14:09,456 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:14:09,457 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:14:09,671 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:14:09,804 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:14:09,887 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:14:09,888 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:14:09,914 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:14:15,964 - INFO - joeynmt.prediction - Generation took 6.0422[sec]. (No references given)
2022-09-15 10:14:15,972 - INFO - joeynmt.training - Processing Predictions on Batch 89/107
2022-09-15 10:14:18,268 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:14:18,268 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:14:18,483 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:14:18,616 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:14:18,699 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:14:18,699 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:14:18,728 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:14:25,471 - INFO - joeynmt.prediction - Generation took 6.7356[sec]. (No references given)
2022-09-15 10:14:25,480 - INFO - joeynmt.training - Processing Predictions on Batch 90/107
2022-09-15 10:14:27,782 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:14:27,782 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:14:27,996 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:14:28,132 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:14:28,222 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:14:28,222 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:14:28,252 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:14:35,456 - INFO - joeynmt.prediction - Generation took 7.1954[sec]. (No references given)
2022-09-15 10:14:35,465 - INFO - joeynmt.training - Processing Predictions on Batch 91/107
2022-09-15 10:14:37,777 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:14:37,777 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:14:37,991 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:14:38,122 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:14:38,208 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:14:38,208 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:14:38,234 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:14:44,240 - INFO - joeynmt.prediction - Generation took 5.9980[sec]. (No references given)
2022-09-15 10:14:44,248 - INFO - joeynmt.training - Processing Predictions on Batch 92/107
2022-09-15 10:14:46,560 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:14:46,560 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:14:46,773 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:14:46,905 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:14:46,989 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:14:46,989 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:14:47,017 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:14:54,478 - INFO - joeynmt.prediction - Generation took 7.4535[sec]. (No references given)
2022-09-15 10:14:54,487 - INFO - joeynmt.training - Processing Predictions on Batch 93/107
2022-09-15 10:14:56,794 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:14:56,794 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:14:57,008 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:14:57,139 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:14:57,223 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:14:57,224 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:14:57,257 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:15:05,860 - INFO - joeynmt.prediction - Generation took 8.5939[sec]. (No references given)
2022-09-15 10:15:05,868 - INFO - joeynmt.training - Processing Predictions on Batch 94/107
2022-09-15 10:15:08,179 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:15:08,179 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:15:08,393 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:15:08,525 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:15:08,608 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:15:08,608 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:15:08,639 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:15:16,232 - INFO - joeynmt.prediction - Generation took 7.5853[sec]. (No references given)
2022-09-15 10:15:16,241 - INFO - joeynmt.training - Processing Predictions on Batch 95/107
2022-09-15 10:15:18,549 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:15:18,549 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:15:18,764 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:15:18,897 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:15:18,980 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:15:18,980 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:15:19,009 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:15:26,107 - INFO - joeynmt.prediction - Generation took 7.0898[sec]. (No references given)
2022-09-15 10:15:26,116 - INFO - joeynmt.training - Processing Predictions on Batch 96/107
2022-09-15 10:15:28,434 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:15:28,434 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:15:28,648 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:15:28,855 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:15:28,938 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:15:28,938 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:15:28,964 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:15:35,890 - INFO - joeynmt.prediction - Generation took 6.9183[sec]. (No references given)
2022-09-15 10:15:35,898 - INFO - joeynmt.training - Processing Predictions on Batch 97/107
2022-09-15 10:15:38,216 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:15:38,216 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:15:38,432 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:15:38,564 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:15:38,649 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:15:38,649 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:15:38,678 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:15:46,560 - INFO - joeynmt.prediction - Generation took 7.8745[sec]. (No references given)
2022-09-15 10:15:46,569 - INFO - joeynmt.training - Processing Predictions on Batch 98/107
2022-09-15 10:15:48,861 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:15:48,861 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:15:49,076 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:15:49,206 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:15:49,290 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:15:49,290 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:15:49,319 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:15:57,695 - INFO - joeynmt.prediction - Generation took 8.3688[sec]. (No references given)
2022-09-15 10:15:57,704 - INFO - joeynmt.training - Processing Predictions on Batch 99/107
2022-09-15 10:16:00,010 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:16:00,010 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:16:00,225 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:16:00,357 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:16:00,442 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:16:00,442 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:16:00,469 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:16:06,544 - INFO - joeynmt.prediction - Generation took 6.0678[sec]. (No references given)
2022-09-15 10:16:06,553 - INFO - joeynmt.training - Processing Predictions on Batch 100/107
2022-09-15 10:16:08,856 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:16:08,856 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:16:09,070 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:16:09,201 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:16:09,359 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:16:09,359 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:16:09,386 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:16:15,817 - INFO - joeynmt.prediction - Generation took 6.4224[sec]. (No references given)
2022-09-15 10:16:15,825 - INFO - joeynmt.training - Processing Predictions on Batch 101/107
2022-09-15 10:16:18,128 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:16:18,128 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:16:18,342 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:16:18,474 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:16:18,557 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:16:18,557 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:16:18,587 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:16:26,627 - INFO - joeynmt.prediction - Generation took 8.0319[sec]. (No references given)
2022-09-15 10:16:26,636 - INFO - joeynmt.training - Processing Predictions on Batch 102/107
2022-09-15 10:16:28,925 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:16:28,925 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:16:29,146 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:16:29,279 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:16:29,362 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:16:29,362 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:16:29,392 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:16:35,706 - INFO - joeynmt.prediction - Generation took 6.3052[sec]. (No references given)
2022-09-15 10:16:35,714 - INFO - joeynmt.training - Processing Predictions on Batch 103/107
2022-09-15 10:16:38,034 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:16:38,034 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:16:38,248 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:16:38,380 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:16:38,464 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:16:38,464 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:16:38,494 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:16:44,577 - INFO - joeynmt.prediction - Generation took 6.0745[sec]. (No references given)
2022-09-15 10:16:44,586 - INFO - joeynmt.training - Processing Predictions on Batch 104/107
2022-09-15 10:16:46,895 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:16:46,895 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:16:47,110 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:16:47,244 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:16:47,328 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:16:47,328 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:16:47,358 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:16:54,775 - INFO - joeynmt.prediction - Generation took 7.4082[sec]. (No references given)
2022-09-15 10:16:54,784 - INFO - joeynmt.training - Processing Predictions on Batch 105/107
2022-09-15 10:16:57,103 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:16:57,103 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:16:57,317 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:16:57,449 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:16:57,533 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:16:57,533 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:16:57,561 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:17:05,150 - INFO - joeynmt.prediction - Generation took 7.5808[sec]. (No references given)
2022-09-15 10:17:05,159 - INFO - joeynmt.training - Processing Predictions on Batch 106/107
2022-09-15 10:17:07,485 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 10:17:07,485 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 10:17:07,703 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 10:17:07,836 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt.
2022-09-15 10:17:07,920 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:17:07,921 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 10:17:07,946 - INFO - joeynmt.prediction - Predicting 205 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:17:12,473 - INFO - joeynmt.prediction - Generation took 4.5200[sec]. (No references given)
2022-09-15 10:17:12,667 - INFO - joeynmt.training - Final Query Indices picked: [428993, 66107, 412056, 175613, 336061, 259962, 175758, 4137, 57895, 311449] length: 10000
2022-09-15 10:17:12,667 - INFO - joeynmt.training - Query the samples, one at a time (interactive/file)
2022-09-15 10:17:14,837 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 4096
	effective batch size (w. parallel & accumulation): 4096
2022-09-15 10:17:14,837 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:35:42,183 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 10:35:42,185 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.70, loss:   2.64, ppl:  14.00, acc:   0.50, generation: 1100.3398[sec], evaluation: 6.6645[sec]
2022-09-15 10:35:43,770 - INFO - joeynmt.training - Example #0
2022-09-15 10:35:43,782 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 10:35:43,783 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 10:35:43,783 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को ढका हुआ है
2022-09-15 10:35:43,783 - INFO - joeynmt.training - Example #1
2022-09-15 10:35:43,793 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 10:35:43,793 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 10:35:43,793 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 10:35:43,793 - INFO - joeynmt.training - Example #2
2022-09-15 10:35:43,804 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 10:35:43,804 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 10:35:43,804 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और इम
2022-09-15 10:35:43,804 - INFO - joeynmt.training - Example #3
2022-09-15 10:35:43,815 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 10:35:43,815 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 10:35:43,815 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 10:35:43,904 - INFO - joeynmt.training - EPOCH 1
2022-09-15 10:36:07,376 - INFO - joeynmt.training - Epoch   1, Step:   203100, Batch Loss:     2.844702, Batch Acc: 0.004389, Tokens per Sec:     4853, Lr: 0.000035
2022-09-15 10:36:30,354 - INFO - joeynmt.training - Epoch   1, Step:   203200, Batch Loss:     2.671942, Batch Acc: 0.004156, Tokens per Sec:     5089, Lr: 0.000035
2022-09-15 10:36:53,292 - INFO - joeynmt.training - Epoch   1, Step:   203300, Batch Loss:     2.843921, Batch Acc: 0.004781, Tokens per Sec:     5025, Lr: 0.000035
2022-09-15 10:37:16,141 - INFO - joeynmt.training - Epoch   1, Step:   203400, Batch Loss:     2.527471, Batch Acc: 0.004171, Tokens per Sec:     5068, Lr: 0.000035
2022-09-15 10:37:38,997 - INFO - joeynmt.training - Epoch   1, Step:   203500, Batch Loss:     2.786730, Batch Acc: 0.004306, Tokens per Sec:     4999, Lr: 0.000035
2022-09-15 10:38:01,800 - INFO - joeynmt.training - Epoch   1, Step:   203600, Batch Loss:     2.666656, Batch Acc: 0.004582, Tokens per Sec:     4987, Lr: 0.000035
2022-09-15 10:38:24,601 - INFO - joeynmt.training - Epoch   1, Step:   203700, Batch Loss:     2.526506, Batch Acc: 0.005564, Tokens per Sec:     4974, Lr: 0.000035
2022-09-15 10:38:47,481 - INFO - joeynmt.training - Epoch   1, Step:   203800, Batch Loss:     2.822432, Batch Acc: 0.004568, Tokens per Sec:     5004, Lr: 0.000035
2022-09-15 10:39:10,222 - INFO - joeynmt.training - Epoch   1, Step:   203900, Batch Loss:     2.828092, Batch Acc: 0.004541, Tokens per Sec:     4987, Lr: 0.000035
2022-09-15 10:39:33,077 - INFO - joeynmt.training - Epoch   1, Step:   204000, Batch Loss:     2.910850, Batch Acc: 0.004225, Tokens per Sec:     5064, Lr: 0.000035
2022-09-15 10:39:33,078 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 10:56:51,834 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 10:56:51,835 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.72, loss:   2.64, ppl:  13.96, acc:   0.50, generation: 1031.3528[sec], evaluation: 6.8143[sec]
2022-09-15 10:56:51,839 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 10:56:53,552 - INFO - joeynmt.training - Example #0
2022-09-15 10:56:53,564 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 10:56:53,564 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 10:56:53,564 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 10:56:53,564 - INFO - joeynmt.training - Example #1
2022-09-15 10:56:53,574 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 10:56:53,575 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 10:56:53,575 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 10:56:53,575 - INFO - joeynmt.training - Example #2
2022-09-15 10:56:53,585 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 10:56:53,585 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 10:56:53,585 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और इम
2022-09-15 10:56:53,585 - INFO - joeynmt.training - Example #3
2022-09-15 10:56:53,595 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 10:56:53,595 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 10:56:53,595 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 10:57:16,701 - INFO - joeynmt.training - Epoch   1, Step:   204100, Batch Loss:     2.825936, Batch Acc: 0.003883, Tokens per Sec:     4640, Lr: 0.000035
2022-09-15 10:57:39,549 - INFO - joeynmt.training - Epoch   1, Step:   204200, Batch Loss:     2.566941, Batch Acc: 0.004638, Tokens per Sec:     4973, Lr: 0.000035
2022-09-15 10:58:02,275 - INFO - joeynmt.training - Epoch   1, Step:   204300, Batch Loss:     2.646346, Batch Acc: 0.004312, Tokens per Sec:     4980, Lr: 0.000035
2022-09-15 10:58:25,132 - INFO - joeynmt.training - Epoch   1, Step:   204400, Batch Loss:     2.720716, Batch Acc: 0.004257, Tokens per Sec:     4995, Lr: 0.000035
2022-09-15 10:58:47,920 - INFO - joeynmt.training - Epoch   1, Step:   204500, Batch Loss:     2.978940, Batch Acc: 0.003607, Tokens per Sec:     5013, Lr: 0.000035
2022-09-15 10:59:10,693 - INFO - joeynmt.training - Epoch   1, Step:   204600, Batch Loss:     2.751595, Batch Acc: 0.003861, Tokens per Sec:     4981, Lr: 0.000035
2022-09-15 10:59:33,465 - INFO - joeynmt.training - Epoch   1, Step:   204700, Batch Loss:     2.700664, Batch Acc: 0.004372, Tokens per Sec:     5112, Lr: 0.000035
2022-09-15 10:59:56,408 - INFO - joeynmt.training - Epoch   1, Step:   204800, Batch Loss:     2.989308, Batch Acc: 0.004000, Tokens per Sec:     5132, Lr: 0.000035
2022-09-15 11:00:19,290 - INFO - joeynmt.training - Epoch   1, Step:   204900, Batch Loss:     2.601750, Batch Acc: 0.004500, Tokens per Sec:     5050, Lr: 0.000035
2022-09-15 11:00:42,117 - INFO - joeynmt.training - Epoch   1, Step:   205000, Batch Loss:     2.730406, Batch Acc: 0.004233, Tokens per Sec:     5092, Lr: 0.000035
2022-09-15 11:00:42,117 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 11:18:36,780 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 11:18:36,781 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.82, loss:   2.63, ppl:  13.89, acc:   0.50, generation: 1067.4466[sec], evaluation: 6.8731[sec]
2022-09-15 11:18:36,785 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 11:18:38,517 - INFO - joeynmt.training - Example #0
2022-09-15 11:18:38,529 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 11:18:38,529 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 11:18:38,529 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 11:18:38,529 - INFO - joeynmt.training - Example #1
2022-09-15 11:18:38,539 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 11:18:38,539 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 11:18:38,540 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 11:18:38,540 - INFO - joeynmt.training - Example #2
2022-09-15 11:18:38,551 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 11:18:38,551 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 11:18:38,551 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और इम
2022-09-15 11:18:38,551 - INFO - joeynmt.training - Example #3
2022-09-15 11:18:38,562 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 11:18:38,562 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 11:18:38,562 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 11:19:01,517 - INFO - joeynmt.training - Epoch   1, Step:   205100, Batch Loss:     2.911351, Batch Acc: 0.003482, Tokens per Sec:     4622, Lr: 0.000035
2022-09-15 11:19:24,289 - INFO - joeynmt.training - Epoch   1, Step:   205200, Batch Loss:     2.701269, Batch Acc: 0.004652, Tokens per Sec:     5079, Lr: 0.000035
2022-09-15 11:19:47,060 - INFO - joeynmt.training - Epoch   1, Step:   205300, Batch Loss:     2.512839, Batch Acc: 0.004118, Tokens per Sec:     5012, Lr: 0.000035
2022-09-15 11:20:09,802 - INFO - joeynmt.training - Epoch   1, Step:   205400, Batch Loss:     2.825647, Batch Acc: 0.004228, Tokens per Sec:     5055, Lr: 0.000035
2022-09-15 11:20:32,708 - INFO - joeynmt.training - Epoch   1, Step:   205500, Batch Loss:     2.802276, Batch Acc: 0.004296, Tokens per Sec:     5010, Lr: 0.000035
2022-09-15 11:20:55,567 - INFO - joeynmt.training - Epoch   1, Step:   205600, Batch Loss:     2.439653, Batch Acc: 0.005595, Tokens per Sec:     5004, Lr: 0.000035
2022-09-15 11:21:18,269 - INFO - joeynmt.training - Epoch   1, Step:   205700, Batch Loss:     2.838956, Batch Acc: 0.004180, Tokens per Sec:     4995, Lr: 0.000035
2022-09-15 11:21:41,028 - INFO - joeynmt.training - Epoch   1, Step:   205800, Batch Loss:     2.852818, Batch Acc: 0.004451, Tokens per Sec:     5015, Lr: 0.000035
2022-09-15 11:22:03,721 - INFO - joeynmt.training - Epoch   1, Step:   205900, Batch Loss:     2.569283, Batch Acc: 0.004596, Tokens per Sec:     4967, Lr: 0.000035
2022-09-15 11:22:26,478 - INFO - joeynmt.training - Epoch   1, Step:   206000, Batch Loss:     2.683157, Batch Acc: 0.004398, Tokens per Sec:     5046, Lr: 0.000035
2022-09-15 11:22:26,478 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 11:40:23,542 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 11:40:23,544 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.80, loss:   2.63, ppl:  13.85, acc:   0.50, generation: 1069.8357[sec], evaluation: 6.8871[sec]
2022-09-15 11:40:23,547 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 11:40:25,073 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/203000.ckpt
2022-09-15 11:40:25,115 - INFO - joeynmt.training - Example #0
2022-09-15 11:40:25,127 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 11:40:25,127 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 11:40:25,127 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 11:40:25,127 - INFO - joeynmt.training - Example #1
2022-09-15 11:40:25,138 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 11:40:25,138 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 11:40:25,138 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 11:40:25,138 - INFO - joeynmt.training - Example #2
2022-09-15 11:40:25,149 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 11:40:25,149 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 11:40:25,149 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 11:40:25,149 - INFO - joeynmt.training - Example #3
2022-09-15 11:40:25,160 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 11:40:25,160 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 11:40:25,160 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 11:40:48,078 - INFO - joeynmt.training - Epoch   1, Step:   206100, Batch Loss:     2.676846, Batch Acc: 0.004510, Tokens per Sec:     4692, Lr: 0.000035
2022-09-15 11:41:10,899 - INFO - joeynmt.training - Epoch   1, Step:   206200, Batch Loss:     2.862673, Batch Acc: 0.003894, Tokens per Sec:     5019, Lr: 0.000035
2022-09-15 11:41:33,694 - INFO - joeynmt.training - Epoch   1, Step:   206300, Batch Loss:     2.681242, Batch Acc: 0.003906, Tokens per Sec:     5088, Lr: 0.000035
2022-09-15 11:41:56,521 - INFO - joeynmt.training - Epoch   1, Step:   206400, Batch Loss:     2.836965, Batch Acc: 0.004337, Tokens per Sec:     5000, Lr: 0.000035
2022-09-15 11:42:19,194 - INFO - joeynmt.training - Epoch   1, Step:   206500, Batch Loss:     2.741885, Batch Acc: 0.004100, Tokens per Sec:     4981, Lr: 0.000035
2022-09-15 11:42:41,949 - INFO - joeynmt.training - Epoch   1, Step:   206600, Batch Loss:     2.742239, Batch Acc: 0.004410, Tokens per Sec:     5002, Lr: 0.000035
2022-09-15 11:43:04,711 - INFO - joeynmt.training - Epoch   1, Step:   206700, Batch Loss:     2.807038, Batch Acc: 0.004116, Tokens per Sec:     5017, Lr: 0.000035
2022-09-15 11:43:27,552 - INFO - joeynmt.training - Epoch   1, Step:   206800, Batch Loss:     2.697541, Batch Acc: 0.004687, Tokens per Sec:     5082, Lr: 0.000035
2022-09-15 11:43:50,323 - INFO - joeynmt.training - Epoch   1, Step:   206900, Batch Loss:     2.482010, Batch Acc: 0.005690, Tokens per Sec:     5033, Lr: 0.000035
2022-09-15 11:44:13,029 - INFO - joeynmt.training - Epoch   1, Step:   207000, Batch Loss:     2.662709, Batch Acc: 0.004468, Tokens per Sec:     5165, Lr: 0.000035
2022-09-15 11:44:13,029 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 12:01:33,390 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 12:01:33,391 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.83, loss:   2.62, ppl:  13.78, acc:   0.50, generation: 1033.1221[sec], evaluation: 6.8941[sec]
2022-09-15 12:01:33,395 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 12:01:35,463 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/204000.ckpt
2022-09-15 12:01:35,504 - INFO - joeynmt.training - Example #0
2022-09-15 12:01:35,516 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 12:01:35,516 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 12:01:35,516 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 12:01:35,516 - INFO - joeynmt.training - Example #1
2022-09-15 12:01:35,528 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 12:01:35,528 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 12:01:35,528 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 12:01:35,528 - INFO - joeynmt.training - Example #2
2022-09-15 12:01:35,539 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 12:01:35,539 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 12:01:35,539 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 12:01:35,539 - INFO - joeynmt.training - Example #3
2022-09-15 12:01:35,550 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 12:01:35,550 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 12:01:35,550 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 12:01:58,328 - INFO - joeynmt.training - Epoch   1, Step:   207100, Batch Loss:     2.613340, Batch Acc: 0.005299, Tokens per Sec:     4556, Lr: 0.000035
2022-09-15 12:02:21,027 - INFO - joeynmt.training - Epoch   1, Step:   207200, Batch Loss:     2.699474, Batch Acc: 0.004153, Tokens per Sec:     5081, Lr: 0.000035
2022-09-15 12:02:43,705 - INFO - joeynmt.training - Epoch   1, Step:   207300, Batch Loss:     2.673102, Batch Acc: 0.004796, Tokens per Sec:     5057, Lr: 0.000035
2022-09-15 12:03:06,402 - INFO - joeynmt.training - Epoch   1, Step:   207400, Batch Loss:     2.987632, Batch Acc: 0.004176, Tokens per Sec:     5022, Lr: 0.000035
2022-09-15 12:03:29,101 - INFO - joeynmt.training - Epoch   1, Step:   207500, Batch Loss:     2.641753, Batch Acc: 0.004044, Tokens per Sec:     5000, Lr: 0.000035
2022-09-15 12:03:51,722 - INFO - joeynmt.training - Epoch   1, Step:   207600, Batch Loss:     2.806897, Batch Acc: 0.003659, Tokens per Sec:     5074, Lr: 0.000035
2022-09-15 12:04:14,498 - INFO - joeynmt.training - Epoch   1, Step:   207700, Batch Loss:     2.604323, Batch Acc: 0.004089, Tokens per Sec:     5079, Lr: 0.000035
2022-09-15 12:04:37,341 - INFO - joeynmt.training - Epoch   1, Step:   207800, Batch Loss:     2.778434, Batch Acc: 0.004138, Tokens per Sec:     5046, Lr: 0.000035
2022-09-15 12:05:00,176 - INFO - joeynmt.training - Epoch   1, Step:   207900, Batch Loss:     2.660902, Batch Acc: 0.005327, Tokens per Sec:     5089, Lr: 0.000035
2022-09-15 12:05:22,926 - INFO - joeynmt.training - Epoch   1, Step:   208000, Batch Loss:     2.648296, Batch Acc: 0.004784, Tokens per Sec:     5118, Lr: 0.000035
2022-09-15 12:05:22,926 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 12:22:48,526 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 12:22:48,527 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.94, loss:   2.61, ppl:  13.67, acc:   0.50, generation: 1038.1896[sec], evaluation: 7.0670[sec]
2022-09-15 12:22:48,531 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 12:22:50,736 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/205000.ckpt
2022-09-15 12:22:50,779 - INFO - joeynmt.training - Example #0
2022-09-15 12:22:50,791 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 12:22:50,791 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 12:22:50,791 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को ढकने वाले हैं
2022-09-15 12:22:50,791 - INFO - joeynmt.training - Example #1
2022-09-15 12:22:50,801 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 12:22:50,801 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 12:22:50,801 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 12:22:50,801 - INFO - joeynmt.training - Example #2
2022-09-15 12:22:50,812 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 12:22:50,812 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 12:22:50,812 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 12:22:50,812 - INFO - joeynmt.training - Example #3
2022-09-15 12:22:50,822 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 12:22:50,822 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 12:22:50,822 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 12:23:13,636 - INFO - joeynmt.training - Epoch   1, Step:   208100, Batch Loss:     2.623376, Batch Acc: 0.004410, Tokens per Sec:     4552, Lr: 0.000035
2022-09-15 12:23:36,320 - INFO - joeynmt.training - Epoch   1, Step:   208200, Batch Loss:     2.666532, Batch Acc: 0.003675, Tokens per Sec:     5050, Lr: 0.000035
2022-09-15 12:23:59,018 - INFO - joeynmt.training - Epoch   1, Step:   208300, Batch Loss:     3.106313, Batch Acc: 0.003586, Tokens per Sec:     5050, Lr: 0.000035
2022-09-15 12:24:21,697 - INFO - joeynmt.training - Epoch   1, Step:   208400, Batch Loss:     2.459206, Batch Acc: 0.005461, Tokens per Sec:     4998, Lr: 0.000035
2022-09-15 12:24:44,436 - INFO - joeynmt.training - Epoch   1, Step:   208500, Batch Loss:     2.754803, Batch Acc: 0.005070, Tokens per Sec:     4936, Lr: 0.000035
2022-09-15 12:25:07,184 - INFO - joeynmt.training - Epoch   1, Step:   208600, Batch Loss:     2.897731, Batch Acc: 0.004577, Tokens per Sec:     4995, Lr: 0.000035
2022-09-15 12:25:30,040 - INFO - joeynmt.training - Epoch   1, Step:   208700, Batch Loss:     2.667819, Batch Acc: 0.005025, Tokens per Sec:     5058, Lr: 0.000035
2022-09-15 12:25:52,723 - INFO - joeynmt.training - Epoch   1, Step:   208800, Batch Loss:     2.867310, Batch Acc: 0.003612, Tokens per Sec:     5065, Lr: 0.000035
2022-09-15 12:26:15,393 - INFO - joeynmt.training - Epoch   1, Step:   208900, Batch Loss:     2.506909, Batch Acc: 0.004854, Tokens per Sec:     5089, Lr: 0.000035
2022-09-15 12:26:38,099 - INFO - joeynmt.training - Epoch   1, Step:   209000, Batch Loss:     2.554342, Batch Acc: 0.005063, Tokens per Sec:     5020, Lr: 0.000035
2022-09-15 12:26:38,099 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 12:43:51,436 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 12:43:51,437 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.06, loss:   2.61, ppl:  13.56, acc:   0.50, generation: 1026.4569[sec], evaluation: 6.5389[sec]
2022-09-15 12:43:51,441 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 12:43:52,947 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/206000.ckpt
2022-09-15 12:43:52,990 - INFO - joeynmt.training - Example #0
2022-09-15 12:43:53,002 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 12:43:53,002 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 12:43:53,002 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को ढका हुआ है
2022-09-15 12:43:53,002 - INFO - joeynmt.training - Example #1
2022-09-15 12:43:53,013 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 12:43:53,013 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 12:43:53,013 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन प्ले करें
2022-09-15 12:43:53,013 - INFO - joeynmt.training - Example #2
2022-09-15 12:43:53,024 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 12:43:53,024 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 12:43:53,024 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 12:43:53,024 - INFO - joeynmt.training - Example #3
2022-09-15 12:43:53,034 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 12:43:53,034 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 12:43:53,034 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 12:44:15,974 - INFO - joeynmt.training - Epoch   1, Step:   209100, Batch Loss:     2.810440, Batch Acc: 0.004976, Tokens per Sec:     4735, Lr: 0.000035
2022-09-15 12:44:38,758 - INFO - joeynmt.training - Epoch   1, Step:   209200, Batch Loss:     2.452892, Batch Acc: 0.004575, Tokens per Sec:     5046, Lr: 0.000035
2022-09-15 12:45:01,521 - INFO - joeynmt.training - Epoch   1, Step:   209300, Batch Loss:     2.731553, Batch Acc: 0.003369, Tokens per Sec:     5150, Lr: 0.000035
2022-09-15 12:45:24,286 - INFO - joeynmt.training - Epoch   1, Step:   209400, Batch Loss:     2.780192, Batch Acc: 0.004312, Tokens per Sec:     5053, Lr: 0.000035
2022-09-15 12:45:47,010 - INFO - joeynmt.training - Epoch   1, Step:   209500, Batch Loss:     2.535954, Batch Acc: 0.005034, Tokens per Sec:     5027, Lr: 0.000035
2022-09-15 12:46:09,681 - INFO - joeynmt.training - Epoch   1, Step:   209600, Batch Loss:     3.013626, Batch Acc: 0.003921, Tokens per Sec:     5040, Lr: 0.000035
2022-09-15 12:46:32,394 - INFO - joeynmt.training - Epoch   1, Step:   209700, Batch Loss:     2.781589, Batch Acc: 0.004382, Tokens per Sec:     5083, Lr: 0.000035
2022-09-15 12:46:55,230 - INFO - joeynmt.training - Epoch   1, Step:   209800, Batch Loss:     2.916660, Batch Acc: 0.004916, Tokens per Sec:     5033, Lr: 0.000035
2022-09-15 12:47:17,988 - INFO - joeynmt.training - Epoch   1, Step:   209900, Batch Loss:     2.961186, Batch Acc: 0.004111, Tokens per Sec:     5002, Lr: 0.000035
2022-09-15 12:47:40,749 - INFO - joeynmt.training - Epoch   1, Step:   210000, Batch Loss:     2.678929, Batch Acc: 0.004461, Tokens per Sec:     5023, Lr: 0.000035
2022-09-15 12:47:40,749 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 13:05:17,138 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 13:05:17,139 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.05, loss:   2.60, ppl:  13.52, acc:   0.50, generation: 1049.0916[sec], evaluation: 6.5848[sec]
2022-09-15 13:05:17,143 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 13:05:18,633 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/207000.ckpt
2022-09-15 13:05:18,676 - INFO - joeynmt.training - Example #0
2022-09-15 13:05:18,692 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 13:05:18,692 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 13:05:18,692 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को ढका हुआ है
2022-09-15 13:05:18,692 - INFO - joeynmt.training - Example #1
2022-09-15 13:05:18,702 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 13:05:18,702 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 13:05:18,702 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-15 13:05:18,702 - INFO - joeynmt.training - Example #2
2022-09-15 13:05:18,712 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 13:05:18,712 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 13:05:18,712 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 13:05:18,712 - INFO - joeynmt.training - Example #3
2022-09-15 13:05:18,723 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 13:05:18,723 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 13:05:18,723 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 13:05:41,546 - INFO - joeynmt.training - Epoch   1, Step:   210100, Batch Loss:     2.685907, Batch Acc: 0.004580, Tokens per Sec:     4670, Lr: 0.000035
2022-09-15 13:06:04,302 - INFO - joeynmt.training - Epoch   1, Step:   210200, Batch Loss:     2.683832, Batch Acc: 0.004632, Tokens per Sec:     5048, Lr: 0.000035
2022-09-15 13:06:26,902 - INFO - joeynmt.training - Epoch   1, Step:   210300, Batch Loss:     2.641981, Batch Acc: 0.004000, Tokens per Sec:     5122, Lr: 0.000035
2022-09-15 13:06:49,633 - INFO - joeynmt.training - Epoch   1, Step:   210400, Batch Loss:     2.527529, Batch Acc: 0.004555, Tokens per Sec:     5138, Lr: 0.000035
2022-09-15 13:07:12,354 - INFO - joeynmt.training - Epoch   1, Step:   210500, Batch Loss:     2.825494, Batch Acc: 0.004089, Tokens per Sec:     5016, Lr: 0.000035
2022-09-15 13:07:35,010 - INFO - joeynmt.training - Epoch   1, Step:   210600, Batch Loss:     2.508984, Batch Acc: 0.005165, Tokens per Sec:     5042, Lr: 0.000035
2022-09-15 13:07:57,684 - INFO - joeynmt.training - Epoch   1, Step:   210700, Batch Loss:     2.454952, Batch Acc: 0.005173, Tokens per Sec:     5022, Lr: 0.000035
2022-09-15 13:08:20,315 - INFO - joeynmt.training - Epoch   1, Step:   210800, Batch Loss:     2.650199, Batch Acc: 0.004100, Tokens per Sec:     5141, Lr: 0.000035
2022-09-15 13:08:43,136 - INFO - joeynmt.training - Epoch   1, Step:   210900, Batch Loss:     2.390826, Batch Acc: 0.004868, Tokens per Sec:     5050, Lr: 0.000035
2022-09-15 13:09:05,841 - INFO - joeynmt.training - Epoch   1, Step:   211000, Batch Loss:     2.844853, Batch Acc: 0.004266, Tokens per Sec:     5007, Lr: 0.000035
2022-09-15 13:09:05,841 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 13:26:43,185 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 13:26:43,190 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 13:26:44,680 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/208000.ckpt
2022-09-15 13:26:44,723 - INFO - joeynmt.training - Example #0
2022-09-15 13:26:44,735 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 13:26:44,735 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 13:26:44,735 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 13:26:44,735 - INFO - joeynmt.training - Example #1
2022-09-15 13:26:44,746 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 13:26:44,746 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 13:26:44,746 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-15 13:26:44,746 - INFO - joeynmt.training - Example #2
2022-09-15 13:26:44,756 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 13:26:44,756 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 13:26:44,756 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 13:26:44,756 - INFO - joeynmt.training - Example #3
2022-09-15 13:26:44,767 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 13:26:44,767 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 13:26:44,767 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 13:27:07,726 - INFO - joeynmt.training - Epoch   1, Step:   211100, Batch Loss:     2.463798, Batch Acc: 0.004383, Tokens per Sec:     4630, Lr: 0.000035
2022-09-15 13:27:30,411 - INFO - joeynmt.training - Epoch   1, Step:   211200, Batch Loss:     2.521967, Batch Acc: 0.003665, Tokens per Sec:     5124, Lr: 0.000035
2022-09-15 13:27:53,047 - INFO - joeynmt.training - Epoch   1, Step:   211300, Batch Loss:     2.703743, Batch Acc: 0.003744, Tokens per Sec:     5038, Lr: 0.000035
2022-09-15 13:28:15,729 - INFO - joeynmt.training - Epoch   1, Step:   211400, Batch Loss:     2.691885, Batch Acc: 0.004423, Tokens per Sec:     5013, Lr: 0.000035
2022-09-15 13:28:38,550 - INFO - joeynmt.training - Epoch   1, Step:   211500, Batch Loss:     2.884061, Batch Acc: 0.005129, Tokens per Sec:     5075, Lr: 0.000035
2022-09-15 13:29:01,367 - INFO - joeynmt.training - Epoch   1, Step:   211600, Batch Loss:     2.776382, Batch Acc: 0.004904, Tokens per Sec:     5058, Lr: 0.000035
2022-09-15 13:29:24,099 - INFO - joeynmt.training - Epoch   1, Step:   211700, Batch Loss:     2.732763, Batch Acc: 0.004418, Tokens per Sec:     5038, Lr: 0.000035
2022-09-15 13:29:46,893 - INFO - joeynmt.training - Epoch   1, Step:   211800, Batch Loss:     2.830184, Batch Acc: 0.004648, Tokens per Sec:     5059, Lr: 0.000035
2022-09-15 13:30:09,615 - INFO - joeynmt.training - Epoch   1, Step:   211900, Batch Loss:     2.745494, Batch Acc: 0.005204, Tokens per Sec:     4998, Lr: 0.000035
2022-09-15 13:30:32,693 - INFO - joeynmt.training - Epoch   1, Step:   212000, Batch Loss:     2.974684, Batch Acc: 0.004355, Tokens per Sec:     5025, Lr: 0.000035
2022-09-15 13:30:32,693 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 13:48:05,550 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 13:48:05,552 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.13, loss:   2.60, ppl:  13.45, acc:   0.50, generation: 1045.8566[sec], evaluation: 6.6566[sec]
2022-09-15 13:48:07,086 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/209000.ckpt
2022-09-15 13:48:07,130 - INFO - joeynmt.training - Example #0
2022-09-15 13:48:07,142 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 13:48:07,142 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 13:48:07,142 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 13:48:07,142 - INFO - joeynmt.training - Example #1
2022-09-15 13:48:07,153 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 13:48:07,153 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 13:48:07,153 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 13:48:07,153 - INFO - joeynmt.training - Example #2
2022-09-15 13:48:07,164 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 13:48:07,164 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 13:48:07,164 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 13:48:07,164 - INFO - joeynmt.training - Example #3
2022-09-15 13:48:07,175 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 13:48:07,175 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 13:48:07,175 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 13:48:30,014 - INFO - joeynmt.training - Epoch   1, Step:   212100, Batch Loss:     2.887935, Batch Acc: 0.004343, Tokens per Sec:     4726, Lr: 0.000035
2022-09-15 13:48:52,698 - INFO - joeynmt.training - Epoch   1, Step:   212200, Batch Loss:     2.647675, Batch Acc: 0.004282, Tokens per Sec:     5014, Lr: 0.000035
2022-09-15 13:49:15,451 - INFO - joeynmt.training - Epoch   1, Step:   212300, Batch Loss:     2.995998, Batch Acc: 0.004154, Tokens per Sec:     4983, Lr: 0.000035
2022-09-15 13:49:38,186 - INFO - joeynmt.training - Epoch   1, Step:   212400, Batch Loss:     2.556431, Batch Acc: 0.005248, Tokens per Sec:     5071, Lr: 0.000035
2022-09-15 13:50:00,868 - INFO - joeynmt.training - Epoch   1, Step:   212500, Batch Loss:     2.611299, Batch Acc: 0.004494, Tokens per Sec:     5042, Lr: 0.000035
2022-09-15 13:50:23,658 - INFO - joeynmt.training - Epoch   1, Step:   212600, Batch Loss:     2.737591, Batch Acc: 0.003998, Tokens per Sec:     5038, Lr: 0.000035
2022-09-15 13:50:46,404 - INFO - joeynmt.training - Epoch   1, Step:   212700, Batch Loss:     3.007515, Batch Acc: 0.004466, Tokens per Sec:     5060, Lr: 0.000035
2022-09-15 13:51:09,059 - INFO - joeynmt.training - Epoch   1, Step:   212800, Batch Loss:     2.590774, Batch Acc: 0.003928, Tokens per Sec:     5012, Lr: 0.000035
2022-09-15 13:51:31,620 - INFO - joeynmt.training - Epoch   1, Step:   212900, Batch Loss:     2.829754, Batch Acc: 0.003211, Tokens per Sec:     4984, Lr: 0.000035
2022-09-15 13:51:54,276 - INFO - joeynmt.training - Epoch   1, Step:   213000, Batch Loss:     2.613357, Batch Acc: 0.003987, Tokens per Sec:     5070, Lr: 0.000035
2022-09-15 13:51:54,276 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:09:23,050 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 14:09:23,051 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.27, loss:   2.59, ppl:  13.38, acc:   0.50, generation: 1041.3058[sec], evaluation: 7.1228[sec]
2022-09-15 14:09:23,055 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 14:09:24,913 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/210000.ckpt
2022-09-15 14:09:24,955 - INFO - joeynmt.training - Example #0
2022-09-15 14:09:24,967 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 14:09:24,967 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 14:09:24,967 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 14:09:24,967 - INFO - joeynmt.training - Example #1
2022-09-15 14:09:24,977 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 14:09:24,977 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 14:09:24,977 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 14:09:24,977 - INFO - joeynmt.training - Example #2
2022-09-15 14:09:24,988 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 14:09:24,988 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 14:09:24,988 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 14:09:24,988 - INFO - joeynmt.training - Example #3
2022-09-15 14:09:24,998 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 14:09:24,998 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 14:09:24,998 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 14:09:47,911 - INFO - joeynmt.training - Epoch   1, Step:   213100, Batch Loss:     2.702238, Batch Acc: 0.005484, Tokens per Sec:     4622, Lr: 0.000035
2022-09-15 14:10:10,613 - INFO - joeynmt.training - Epoch   1, Step:   213200, Batch Loss:     2.730499, Batch Acc: 0.004415, Tokens per Sec:     5078, Lr: 0.000035
2022-09-15 14:10:33,410 - INFO - joeynmt.training - Epoch   1, Step:   213300, Batch Loss:     2.705510, Batch Acc: 0.004357, Tokens per Sec:     5115, Lr: 0.000035
2022-09-15 14:10:55,982 - INFO - joeynmt.training - Epoch   1, Step:   213400, Batch Loss:     2.674547, Batch Acc: 0.004055, Tokens per Sec:     4994, Lr: 0.000035
2022-09-15 14:11:18,707 - INFO - joeynmt.training - Epoch   1, Step:   213500, Batch Loss:     2.717232, Batch Acc: 0.003593, Tokens per Sec:     5034, Lr: 0.000035
2022-09-15 14:11:41,551 - INFO - joeynmt.training - Epoch   1, Step:   213600, Batch Loss:     2.906924, Batch Acc: 0.004370, Tokens per Sec:     5069, Lr: 0.000035
2022-09-15 14:12:04,187 - INFO - joeynmt.training - Epoch   1, Step:   213700, Batch Loss:     2.869003, Batch Acc: 0.004258, Tokens per Sec:     5032, Lr: 0.000035
2022-09-15 14:12:26,912 - INFO - joeynmt.training - Epoch   1, Step:   213800, Batch Loss:     2.582577, Batch Acc: 0.004022, Tokens per Sec:     5175, Lr: 0.000035
2022-09-15 14:12:49,545 - INFO - joeynmt.training - Epoch   1, Step:   213900, Batch Loss:     2.709260, Batch Acc: 0.004509, Tokens per Sec:     4998, Lr: 0.000035
2022-09-15 14:13:12,166 - INFO - joeynmt.training - Epoch   1, Step:   214000, Batch Loss:     2.821120, Batch Acc: 0.004480, Tokens per Sec:     5033, Lr: 0.000035
2022-09-15 14:13:12,166 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:30:08,352 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 14:30:08,353 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.30, loss:   2.59, ppl:  13.34, acc:   0.50, generation: 1009.1896[sec], evaluation: 6.6536[sec]
2022-09-15 14:30:08,357 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 14:30:09,826 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/212000.ckpt
2022-09-15 14:30:09,868 - INFO - joeynmt.training - Example #0
2022-09-15 14:30:09,880 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 14:30:09,880 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 14:30:09,880 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 14:30:09,880 - INFO - joeynmt.training - Example #1
2022-09-15 14:30:09,891 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 14:30:09,891 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 14:30:09,891 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 14:30:09,891 - INFO - joeynmt.training - Example #2
2022-09-15 14:30:09,901 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 14:30:09,901 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 14:30:09,901 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 14:30:09,901 - INFO - joeynmt.training - Example #3
2022-09-15 14:30:09,912 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 14:30:09,912 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 14:30:09,912 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 14:30:32,664 - INFO - joeynmt.training - Epoch   1, Step:   214100, Batch Loss:     2.802183, Batch Acc: 0.004540, Tokens per Sec:     4694, Lr: 0.000035
2022-09-15 14:30:55,317 - INFO - joeynmt.training - Epoch   1, Step:   214200, Batch Loss:     2.645797, Batch Acc: 0.004035, Tokens per Sec:     5098, Lr: 0.000035
2022-09-15 14:31:17,953 - INFO - joeynmt.training - Epoch   1, Step:   214300, Batch Loss:     2.736487, Batch Acc: 0.005549, Tokens per Sec:     4936, Lr: 0.000035
2022-09-15 14:31:40,656 - INFO - joeynmt.training - Epoch   1, Step:   214400, Batch Loss:     2.771089, Batch Acc: 0.004522, Tokens per Sec:     4987, Lr: 0.000035
2022-09-15 14:32:03,380 - INFO - joeynmt.training - Epoch   1, Step:   214500, Batch Loss:     2.843379, Batch Acc: 0.004324, Tokens per Sec:     4997, Lr: 0.000035
2022-09-15 14:32:26,078 - INFO - joeynmt.training - Epoch   1, Step:   214600, Batch Loss:     2.666295, Batch Acc: 0.004616, Tokens per Sec:     5039, Lr: 0.000035
2022-09-15 14:32:48,836 - INFO - joeynmt.training - Epoch   1, Step:   214700, Batch Loss:     2.633807, Batch Acc: 0.005239, Tokens per Sec:     5024, Lr: 0.000035
2022-09-15 14:33:11,563 - INFO - joeynmt.training - Epoch   1, Step:   214800, Batch Loss:     2.763646, Batch Acc: 0.004533, Tokens per Sec:     5057, Lr: 0.000035
2022-09-15 14:33:34,299 - INFO - joeynmt.training - Epoch   1, Step:   214900, Batch Loss:     2.718639, Batch Acc: 0.005379, Tokens per Sec:     5077, Lr: 0.000035
2022-09-15 14:33:57,036 - INFO - joeynmt.training - Epoch   1, Step:   215000, Batch Loss:     2.434473, Batch Acc: 0.004544, Tokens per Sec:     5043, Lr: 0.000035
2022-09-15 14:33:57,036 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:51:01,387 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 14:51:01,388 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.33, loss:   2.59, ppl:  13.28, acc:   0.50, generation: 1017.3616[sec], evaluation: 6.6447[sec]
2022-09-15 14:51:01,392 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 14:51:02,865 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/211000.ckpt
2022-09-15 14:51:02,907 - INFO - joeynmt.training - Example #0
2022-09-15 14:51:02,919 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 14:51:02,919 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 14:51:02,919 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 14:51:02,919 - INFO - joeynmt.training - Example #1
2022-09-15 14:51:02,930 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 14:51:02,930 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 14:51:02,930 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 14:51:02,930 - INFO - joeynmt.training - Example #2
2022-09-15 14:51:02,940 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 14:51:02,940 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 14:51:02,941 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 14:51:02,941 - INFO - joeynmt.training - Example #3
2022-09-15 14:51:02,951 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 14:51:02,951 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 14:51:02,951 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 14:51:25,757 - INFO - joeynmt.training - Epoch   1, Step:   215100, Batch Loss:     2.657511, Batch Acc: 0.004744, Tokens per Sec:     4697, Lr: 0.000035
2022-09-15 14:51:48,339 - INFO - joeynmt.training - Epoch   1, Step:   215200, Batch Loss:     2.752233, Batch Acc: 0.004048, Tokens per Sec:     4999, Lr: 0.000035
2022-09-15 14:52:10,942 - INFO - joeynmt.training - Epoch   1, Step:   215300, Batch Loss:     2.779366, Batch Acc: 0.005074, Tokens per Sec:     5040, Lr: 0.000035
2022-09-15 14:52:33,648 - INFO - joeynmt.training - Epoch   1, Step:   215400, Batch Loss:     2.769231, Batch Acc: 0.004889, Tokens per Sec:     4991, Lr: 0.000035
2022-09-15 14:52:56,380 - INFO - joeynmt.training - Epoch   1, Step:   215500, Batch Loss:     2.896629, Batch Acc: 0.004662, Tokens per Sec:     5048, Lr: 0.000035
2022-09-15 14:53:18,998 - INFO - joeynmt.training - Epoch   1, Step:   215600, Batch Loss:     2.639499, Batch Acc: 0.005242, Tokens per Sec:     5052, Lr: 0.000035
2022-09-15 14:53:41,656 - INFO - joeynmt.training - Epoch   1, Step:   215700, Batch Loss:     2.757908, Batch Acc: 0.004206, Tokens per Sec:     5016, Lr: 0.000035
2022-09-15 14:54:04,309 - INFO - joeynmt.training - Epoch   1, Step:   215800, Batch Loss:     2.993190, Batch Acc: 0.003389, Tokens per Sec:     5028, Lr: 0.000035
2022-09-15 14:54:26,978 - INFO - joeynmt.training - Epoch   1, Step:   215900, Batch Loss:     2.855729, Batch Acc: 0.004219, Tokens per Sec:     5029, Lr: 0.000035
2022-09-15 14:54:49,685 - INFO - joeynmt.training - Epoch   1, Step:   216000, Batch Loss:     2.705198, Batch Acc: 0.004842, Tokens per Sec:     5039, Lr: 0.000035
2022-09-15 14:54:49,685 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 15:11:48,254 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 15:11:48,255 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.37, loss:   2.58, ppl:  13.22, acc:   0.51, generation: 1011.0610[sec], evaluation: 7.1646[sec]
2022-09-15 15:11:48,259 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 15:11:50,218 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/213000.ckpt
2022-09-15 15:11:50,262 - INFO - joeynmt.training - Example #0
2022-09-15 15:11:50,274 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 15:11:50,274 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 15:11:50,274 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 15:11:50,274 - INFO - joeynmt.training - Example #1
2022-09-15 15:11:50,285 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 15:11:50,285 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 15:11:50,285 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 15:11:50,285 - INFO - joeynmt.training - Example #2
2022-09-15 15:11:50,296 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 15:11:50,296 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 15:11:50,296 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 15:11:50,296 - INFO - joeynmt.training - Example #3
2022-09-15 15:11:50,307 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 15:11:50,307 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 15:11:50,307 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 15:12:13,042 - INFO - joeynmt.training - Epoch   1, Step:   216100, Batch Loss:     2.774840, Batch Acc: 0.004360, Tokens per Sec:     4627, Lr: 0.000035
2022-09-15 15:12:35,706 - INFO - joeynmt.training - Epoch   1, Step:   216200, Batch Loss:     2.655014, Batch Acc: 0.004818, Tokens per Sec:     5101, Lr: 0.000035
2022-09-15 15:12:58,445 - INFO - joeynmt.training - Epoch   1, Step:   216300, Batch Loss:     2.583521, Batch Acc: 0.004832, Tokens per Sec:     5043, Lr: 0.000035
2022-09-15 15:13:21,081 - INFO - joeynmt.training - Epoch   1, Step:   216400, Batch Loss:     2.645409, Batch Acc: 0.003918, Tokens per Sec:     5007, Lr: 0.000035
2022-09-15 15:13:43,676 - INFO - joeynmt.training - Epoch   1, Step:   216500, Batch Loss:     2.723419, Batch Acc: 0.005011, Tokens per Sec:     5052, Lr: 0.000035
2022-09-15 15:14:06,359 - INFO - joeynmt.training - Epoch   1, Step:   216600, Batch Loss:     2.859181, Batch Acc: 0.004566, Tokens per Sec:     5108, Lr: 0.000035
2022-09-15 15:14:28,997 - INFO - joeynmt.training - Epoch   1, Step:   216700, Batch Loss:     2.869669, Batch Acc: 0.004502, Tokens per Sec:     5073, Lr: 0.000035
2022-09-15 15:14:51,853 - INFO - joeynmt.training - Epoch   1, Step:   216800, Batch Loss:     2.558280, Batch Acc: 0.003912, Tokens per Sec:     5067, Lr: 0.000035
2022-09-15 15:15:14,526 - INFO - joeynmt.training - Epoch   1, Step:   216900, Batch Loss:     2.675048, Batch Acc: 0.003879, Tokens per Sec:     5014, Lr: 0.000035
2022-09-15 15:15:37,143 - INFO - joeynmt.training - Epoch   1, Step:   217000, Batch Loss:     2.529016, Batch Acc: 0.005227, Tokens per Sec:     4974, Lr: 0.000035
2022-09-15 15:15:37,143 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 15:33:15,534 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 15:33:15,536 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.47, loss:   2.58, ppl:  13.15, acc:   0.51, generation: 1051.3188[sec], evaluation: 6.7261[sec]
2022-09-15 15:33:15,539 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 15:33:17,068 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/214000.ckpt
2022-09-15 15:33:17,110 - INFO - joeynmt.training - Example #0
2022-09-15 15:33:17,122 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 15:33:17,122 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 15:33:17,122 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 15:33:17,122 - INFO - joeynmt.training - Example #1
2022-09-15 15:33:17,132 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 15:33:17,132 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 15:33:17,132 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-15 15:33:17,132 - INFO - joeynmt.training - Example #2
2022-09-15 15:33:17,143 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 15:33:17,143 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 15:33:17,143 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 15:33:17,143 - INFO - joeynmt.training - Example #3
2022-09-15 15:33:17,153 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 15:33:17,153 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 15:33:17,153 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 15:33:39,999 - INFO - joeynmt.training - Epoch   1, Step:   217100, Batch Loss:     2.717064, Batch Acc: 0.004793, Tokens per Sec:     4657, Lr: 0.000035
2022-09-15 15:34:02,669 - INFO - joeynmt.training - Epoch   1, Step:   217200, Batch Loss:     2.765243, Batch Acc: 0.004583, Tokens per Sec:     5121, Lr: 0.000035
2022-09-15 15:34:25,333 - INFO - joeynmt.training - Epoch   1, Step:   217300, Batch Loss:     2.734393, Batch Acc: 0.004661, Tokens per Sec:     5037, Lr: 0.000035
2022-09-15 15:34:47,867 - INFO - joeynmt.training - Epoch   1, Step:   217400, Batch Loss:     2.777575, Batch Acc: 0.003723, Tokens per Sec:     5043, Lr: 0.000035
2022-09-15 15:35:00,464 - INFO - joeynmt.training - Epoch   1: total training loss 39533.30
2022-09-15 15:35:00,464 - INFO - joeynmt.training - Training ended after   1 epochs.
2022-09-15 15:35:00,464 - INFO - joeynmt.training - Best validation result (greedy) at step   217000:  13.15 ppl.
2022-09-15 15:35:00,472 - INFO - joeynmt.training - Loading from ckpt file: /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt
2022-09-15 15:35:00,485 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 15:35:00,485 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 15:35:00,704 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 15:35:00,707 - INFO - joeynmt.model - Total params: 19302144
2022-09-15 15:35:00,844 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 15:35:00,889 - INFO - joeynmt.prediction - Decoding on dev set...
2022-09-15 15:35:00,889 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 15:55:05,319 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 15:55:05,321 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  22.79, generation: 1197.5678[sec], evaluation: 6.5261[sec]
2022-09-15 15:55:05,418 - INFO - joeynmt.prediction - Translations saved to: /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/00217000.hyps.dev.
2022-09-15 15:55:05,418 - INFO - joeynmt.prediction - Decoding on test set...
2022-09-15 15:55:05,418 - INFO - joeynmt.prediction - Predicting 40858 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:14:38,444 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 16:14:38,445 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  22.23, generation: 1165.5453[sec], evaluation: 7.1478[sec]
2022-09-15 16:14:38,545 - INFO - joeynmt.prediction - Translations saved to: /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/00217000.hyps.test.
2022-09-15 16:14:38,552 - INFO - joeynmt.training - ACTIVE LEARNING MODEL END - MARGIN 0
2022-09-15 16:14:38,553 - INFO - joeynmt.training - ACTIVE LEARNING MODEL START - MARGIN 1
2022-09-15 16:14:38,554 - INFO - joeynmt.training - Random Indices picked: [121958 146867 131932 365838 259178 119879 110268 207892  54886 137337] length: 26753
2022-09-15 16:14:38,554 - INFO - joeynmt.training - Processing Predictions on Batch 0/105
2022-09-15 16:14:40,911 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:14:40,911 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:14:41,133 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:14:41,272 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:14:41,362 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:14:41,362 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:14:41,389 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:14:46,982 - INFO - joeynmt.prediction - Generation took 5.5853[sec]. (No references given)
2022-09-15 16:14:46,990 - INFO - joeynmt.training - Processing Predictions on Batch 1/105
2022-09-15 16:14:49,353 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:14:49,353 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:14:49,566 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:14:49,699 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:14:49,786 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:14:49,786 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:14:49,810 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:14:55,358 - INFO - joeynmt.prediction - Generation took 5.5398[sec]. (No references given)
2022-09-15 16:14:55,367 - INFO - joeynmt.training - Processing Predictions on Batch 2/105
2022-09-15 16:14:57,705 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:14:57,705 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:14:57,918 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:14:58,051 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:14:58,138 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:14:58,138 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:14:58,164 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:15:04,894 - INFO - joeynmt.prediction - Generation took 6.7217[sec]. (No references given)
2022-09-15 16:15:04,903 - INFO - joeynmt.training - Processing Predictions on Batch 3/105
2022-09-15 16:15:07,238 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:15:07,238 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:15:07,452 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:15:07,585 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:15:07,672 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:15:07,672 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:15:07,701 - INFO - joeynmt.prediction - Predicting 224 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:15:13,907 - INFO - joeynmt.prediction - Generation took 6.1976[sec]. (No references given)
2022-09-15 16:15:13,916 - INFO - joeynmt.training - Processing Predictions on Batch 4/105
2022-09-15 16:15:16,251 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:15:16,251 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:15:16,464 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:15:16,597 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:15:16,684 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:15:16,685 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:15:16,712 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:15:24,780 - INFO - joeynmt.prediction - Generation took 8.0589[sec]. (No references given)
2022-09-15 16:15:24,789 - INFO - joeynmt.training - Processing Predictions on Batch 5/105
2022-09-15 16:15:27,174 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:15:27,174 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:15:27,388 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:15:27,522 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:15:27,616 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:15:27,616 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:15:27,644 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:15:34,840 - INFO - joeynmt.prediction - Generation took 7.1870[sec]. (No references given)
2022-09-15 16:15:34,849 - INFO - joeynmt.training - Processing Predictions on Batch 6/105
2022-09-15 16:15:37,236 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:15:37,236 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:15:37,450 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:15:37,584 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:15:37,670 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:15:37,670 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:15:37,696 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:15:44,444 - INFO - joeynmt.prediction - Generation took 6.7398[sec]. (No references given)
2022-09-15 16:15:44,453 - INFO - joeynmt.training - Processing Predictions on Batch 7/105
2022-09-15 16:15:46,850 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:15:46,850 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:15:47,063 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:15:47,198 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:15:47,284 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:15:47,284 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:15:47,310 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:15:54,106 - INFO - joeynmt.prediction - Generation took 6.7877[sec]. (No references given)
2022-09-15 16:15:54,116 - INFO - joeynmt.training - Processing Predictions on Batch 8/105
2022-09-15 16:15:56,508 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:15:56,508 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:15:56,721 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:15:56,856 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:15:56,941 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:15:56,942 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:15:56,971 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:16:04,758 - INFO - joeynmt.prediction - Generation took 7.7786[sec]. (No references given)
2022-09-15 16:16:04,767 - INFO - joeynmt.training - Processing Predictions on Batch 9/105
2022-09-15 16:16:07,198 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:16:07,198 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:16:07,412 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:16:07,547 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:16:07,632 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:16:07,632 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:16:07,661 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:16:15,334 - INFO - joeynmt.prediction - Generation took 7.6641[sec]. (No references given)
2022-09-15 16:16:15,343 - INFO - joeynmt.training - Processing Predictions on Batch 10/105
2022-09-15 16:16:17,714 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:16:17,714 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:16:17,928 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:16:18,062 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:16:18,147 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:16:18,147 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:16:18,176 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:16:25,766 - INFO - joeynmt.prediction - Generation took 7.5827[sec]. (No references given)
2022-09-15 16:16:25,775 - INFO - joeynmt.training - Processing Predictions on Batch 11/105
2022-09-15 16:16:28,139 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:16:28,139 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:16:28,357 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:16:28,491 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:16:28,576 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:16:28,576 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:16:28,604 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:16:34,752 - INFO - joeynmt.prediction - Generation took 6.1394[sec]. (No references given)
2022-09-15 16:16:34,761 - INFO - joeynmt.training - Processing Predictions on Batch 12/105
2022-09-15 16:16:37,126 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:16:37,126 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:16:37,341 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:16:37,476 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:16:37,560 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:16:37,560 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:16:37,585 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:16:43,234 - INFO - joeynmt.prediction - Generation took 5.6416[sec]. (No references given)
2022-09-15 16:16:43,243 - INFO - joeynmt.training - Processing Predictions on Batch 13/105
2022-09-15 16:16:45,623 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:16:45,623 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:16:45,839 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:16:45,975 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:16:46,061 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:16:46,061 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:16:46,089 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:16:55,256 - INFO - joeynmt.prediction - Generation took 9.1588[sec]. (No references given)
2022-09-15 16:16:55,265 - INFO - joeynmt.training - Processing Predictions on Batch 14/105
2022-09-15 16:16:57,651 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:16:57,651 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:16:57,865 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:16:58,000 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:16:58,087 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:16:58,087 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:16:58,118 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:17:06,512 - INFO - joeynmt.prediction - Generation took 8.3856[sec]. (No references given)
2022-09-15 16:17:06,521 - INFO - joeynmt.training - Processing Predictions on Batch 15/105
2022-09-15 16:17:08,907 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:17:08,907 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:17:09,122 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:17:09,256 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:17:09,341 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:17:09,341 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:17:09,370 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:17:18,888 - INFO - joeynmt.prediction - Generation took 9.5087[sec]. (No references given)
2022-09-15 16:17:18,897 - INFO - joeynmt.training - Processing Predictions on Batch 16/105
2022-09-15 16:17:21,269 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:17:21,269 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:17:21,484 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:17:21,618 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:17:21,703 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:17:21,703 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:17:21,729 - INFO - joeynmt.prediction - Predicting 223 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:17:27,349 - INFO - joeynmt.prediction - Generation took 5.6126[sec]. (No references given)
2022-09-15 16:17:27,358 - INFO - joeynmt.training - Processing Predictions on Batch 17/105
2022-09-15 16:17:29,735 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:17:29,735 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:17:29,950 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:17:30,084 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:17:30,169 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:17:30,170 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:17:30,200 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:17:38,781 - INFO - joeynmt.prediction - Generation took 8.5721[sec]. (No references given)
2022-09-15 16:17:38,790 - INFO - joeynmt.training - Processing Predictions on Batch 18/105
2022-09-15 16:17:41,150 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:17:41,150 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:17:41,363 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:17:41,499 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:17:41,584 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:17:41,584 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:17:41,614 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:17:49,241 - INFO - joeynmt.prediction - Generation took 7.6190[sec]. (No references given)
2022-09-15 16:17:49,250 - INFO - joeynmt.training - Processing Predictions on Batch 19/105
2022-09-15 16:17:51,637 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:17:51,637 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:17:51,853 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:17:51,988 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:17:52,073 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:17:52,073 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:17:52,103 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:18:00,851 - INFO - joeynmt.prediction - Generation took 8.7392[sec]. (No references given)
2022-09-15 16:18:00,860 - INFO - joeynmt.training - Processing Predictions on Batch 20/105
2022-09-15 16:18:03,226 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:18:03,226 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:18:03,441 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:18:03,576 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:18:03,662 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:18:03,662 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:18:03,692 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:18:10,228 - INFO - joeynmt.prediction - Generation took 6.5276[sec]. (No references given)
2022-09-15 16:18:10,237 - INFO - joeynmt.training - Processing Predictions on Batch 21/105
2022-09-15 16:18:12,613 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:18:12,613 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:18:12,828 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:18:12,962 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:18:13,047 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:18:13,047 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:18:13,074 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:18:19,893 - INFO - joeynmt.prediction - Generation took 6.8105[sec]. (No references given)
2022-09-15 16:18:19,902 - INFO - joeynmt.training - Processing Predictions on Batch 22/105
2022-09-15 16:18:22,267 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:18:22,267 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:18:22,481 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:18:22,615 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:18:22,700 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:18:22,701 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:18:22,730 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:18:31,963 - INFO - joeynmt.prediction - Generation took 9.2248[sec]. (No references given)
2022-09-15 16:18:31,973 - INFO - joeynmt.training - Processing Predictions on Batch 23/105
2022-09-15 16:18:34,321 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:18:34,321 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:18:34,535 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:18:34,670 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:18:34,755 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:18:34,755 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:18:34,782 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:18:43,593 - INFO - joeynmt.prediction - Generation took 8.8027[sec]. (No references given)
2022-09-15 16:18:43,603 - INFO - joeynmt.training - Processing Predictions on Batch 24/105
2022-09-15 16:18:45,957 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:18:45,958 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:18:46,172 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:18:46,306 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:18:46,391 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:18:46,391 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:18:46,422 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:18:53,581 - INFO - joeynmt.prediction - Generation took 7.1510[sec]. (No references given)
2022-09-15 16:18:53,591 - INFO - joeynmt.training - Processing Predictions on Batch 25/105
2022-09-15 16:18:55,942 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:18:55,942 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:18:56,156 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:18:56,291 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:18:56,376 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:18:56,376 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:18:56,407 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:19:04,732 - INFO - joeynmt.prediction - Generation took 8.3165[sec]. (No references given)
2022-09-15 16:19:04,741 - INFO - joeynmt.training - Processing Predictions on Batch 26/105
2022-09-15 16:19:07,098 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:19:07,098 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:19:07,312 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:19:07,447 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:19:07,533 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:19:07,533 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:19:07,561 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:19:15,928 - INFO - joeynmt.prediction - Generation took 8.3582[sec]. (No references given)
2022-09-15 16:19:15,937 - INFO - joeynmt.training - Processing Predictions on Batch 27/105
2022-09-15 16:19:18,314 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:19:18,314 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:19:18,528 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:19:19,110 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:19:19,195 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:19:19,195 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:19:19,226 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:19:25,078 - INFO - joeynmt.prediction - Generation took 5.8439[sec]. (No references given)
2022-09-15 16:19:25,088 - INFO - joeynmt.training - Processing Predictions on Batch 28/105
2022-09-15 16:19:27,459 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:19:27,459 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:19:27,681 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:19:27,814 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:19:27,904 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:19:27,904 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:19:27,932 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:19:33,809 - INFO - joeynmt.prediction - Generation took 5.8691[sec]. (No references given)
2022-09-15 16:19:33,818 - INFO - joeynmt.training - Processing Predictions on Batch 29/105
2022-09-15 16:19:36,169 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:19:36,169 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:19:36,383 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:19:36,516 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:19:36,602 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:19:36,602 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:19:36,629 - INFO - joeynmt.prediction - Predicting 222 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:19:44,202 - INFO - joeynmt.prediction - Generation took 7.5654[sec]. (No references given)
2022-09-15 16:19:44,211 - INFO - joeynmt.training - Processing Predictions on Batch 30/105
2022-09-15 16:19:46,582 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:19:46,582 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:19:46,796 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:19:46,929 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:19:47,015 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:19:47,015 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:19:47,044 - INFO - joeynmt.prediction - Predicting 224 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:19:54,610 - INFO - joeynmt.prediction - Generation took 7.5574[sec]. (No references given)
2022-09-15 16:19:54,619 - INFO - joeynmt.training - Processing Predictions on Batch 31/105
2022-09-15 16:19:56,980 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:19:56,980 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:19:57,194 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:19:57,328 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:19:57,414 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:19:57,414 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:19:57,444 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:20:06,397 - INFO - joeynmt.prediction - Generation took 8.9443[sec]. (No references given)
2022-09-15 16:20:06,407 - INFO - joeynmt.training - Processing Predictions on Batch 32/105
2022-09-15 16:20:08,789 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:20:08,789 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:20:09,002 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:20:09,135 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:20:09,223 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:20:09,223 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:20:09,253 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:20:17,342 - INFO - joeynmt.prediction - Generation took 8.0809[sec]. (No references given)
2022-09-15 16:20:17,352 - INFO - joeynmt.training - Processing Predictions on Batch 33/105
2022-09-15 16:20:19,729 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:20:19,729 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:20:19,943 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:20:20,076 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:20:20,162 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:20:20,163 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:20:20,192 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:20:28,485 - INFO - joeynmt.prediction - Generation took 8.2847[sec]. (No references given)
2022-09-15 16:20:28,494 - INFO - joeynmt.training - Processing Predictions on Batch 34/105
2022-09-15 16:20:30,854 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:20:30,854 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:20:31,067 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:20:31,199 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:20:31,286 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:20:31,286 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:20:31,318 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:20:38,462 - INFO - joeynmt.prediction - Generation took 7.1359[sec]. (No references given)
2022-09-15 16:20:38,472 - INFO - joeynmt.training - Processing Predictions on Batch 35/105
2022-09-15 16:20:40,849 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:20:40,849 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:20:41,062 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:20:41,194 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:20:41,280 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:20:41,280 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:20:41,308 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:20:48,796 - INFO - joeynmt.prediction - Generation took 7.4795[sec]. (No references given)
2022-09-15 16:20:48,805 - INFO - joeynmt.training - Processing Predictions on Batch 36/105
2022-09-15 16:20:51,166 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:20:51,166 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:20:51,380 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:20:51,512 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:20:51,599 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:20:51,599 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:20:51,626 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:20:59,119 - INFO - joeynmt.prediction - Generation took 7.4842[sec]. (No references given)
2022-09-15 16:20:59,128 - INFO - joeynmt.training - Processing Predictions on Batch 37/105
2022-09-15 16:21:01,490 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:21:01,490 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:21:01,703 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:21:01,836 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:21:01,922 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:21:01,922 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:21:01,949 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:21:07,379 - INFO - joeynmt.prediction - Generation took 5.4223[sec]. (No references given)
2022-09-15 16:21:07,388 - INFO - joeynmt.training - Processing Predictions on Batch 38/105
2022-09-15 16:21:09,737 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:21:09,737 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:21:09,949 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:21:10,083 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:21:10,171 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:21:10,171 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:21:10,197 - INFO - joeynmt.prediction - Predicting 224 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:21:17,989 - INFO - joeynmt.prediction - Generation took 7.7835[sec]. (No references given)
2022-09-15 16:21:17,998 - INFO - joeynmt.training - Processing Predictions on Batch 39/105
2022-09-15 16:21:20,352 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:21:20,352 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:21:20,566 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:21:20,701 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:21:20,788 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:21:20,788 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:21:20,816 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:21:27,498 - INFO - joeynmt.prediction - Generation took 6.6729[sec]. (No references given)
2022-09-15 16:21:27,507 - INFO - joeynmt.training - Processing Predictions on Batch 40/105
2022-09-15 16:21:29,839 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:21:29,839 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:21:30,053 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:21:30,187 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:21:30,274 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:21:30,274 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:21:30,300 - INFO - joeynmt.prediction - Predicting 222 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:21:37,266 - INFO - joeynmt.prediction - Generation took 6.9577[sec]. (No references given)
2022-09-15 16:21:37,276 - INFO - joeynmt.training - Processing Predictions on Batch 41/105
2022-09-15 16:21:39,626 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:21:39,626 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:21:39,839 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:21:39,973 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:21:40,059 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:21:40,066 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:21:40,098 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:21:48,860 - INFO - joeynmt.prediction - Generation took 8.7538[sec]. (No references given)
2022-09-15 16:21:48,869 - INFO - joeynmt.training - Processing Predictions on Batch 42/105
2022-09-15 16:21:51,217 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:21:51,217 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:21:51,431 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:21:51,564 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:21:51,650 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:21:51,650 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:21:51,679 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:21:58,325 - INFO - joeynmt.prediction - Generation took 6.6376[sec]. (No references given)
2022-09-15 16:21:58,334 - INFO - joeynmt.training - Processing Predictions on Batch 43/105
2022-09-15 16:22:00,693 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:22:00,694 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:22:00,907 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:22:01,040 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:22:01,127 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:22:01,127 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:22:01,156 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:22:06,811 - INFO - joeynmt.prediction - Generation took 5.6472[sec]. (No references given)
2022-09-15 16:22:06,820 - INFO - joeynmt.training - Processing Predictions on Batch 44/105
2022-09-15 16:22:09,168 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:22:09,168 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:22:09,382 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:22:09,514 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:22:09,601 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:22:09,601 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:22:09,627 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:22:17,517 - INFO - joeynmt.prediction - Generation took 7.8814[sec]. (No references given)
2022-09-15 16:22:17,526 - INFO - joeynmt.training - Processing Predictions on Batch 45/105
2022-09-15 16:22:19,894 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:22:19,894 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:22:20,107 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:22:20,240 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:22:20,328 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:22:20,328 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:22:20,353 - INFO - joeynmt.prediction - Predicting 216 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:22:26,310 - INFO - joeynmt.prediction - Generation took 5.9500[sec]. (No references given)
2022-09-15 16:22:26,319 - INFO - joeynmt.training - Processing Predictions on Batch 46/105
2022-09-15 16:22:28,653 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:22:28,653 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:22:28,867 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:22:29,000 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:22:29,086 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:22:29,086 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:22:29,114 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:22:38,494 - INFO - joeynmt.prediction - Generation took 9.3715[sec]. (No references given)
2022-09-15 16:22:38,503 - INFO - joeynmt.training - Processing Predictions on Batch 47/105
2022-09-15 16:22:40,876 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:22:40,876 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:22:41,089 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:22:41,222 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:22:41,308 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:22:41,308 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:22:41,336 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:22:49,340 - INFO - joeynmt.prediction - Generation took 7.9963[sec]. (No references given)
2022-09-15 16:22:49,349 - INFO - joeynmt.training - Processing Predictions on Batch 48/105
2022-09-15 16:22:51,701 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:22:51,701 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:22:51,914 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:22:52,047 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:22:52,584 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:22:52,584 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:22:52,611 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:23:00,425 - INFO - joeynmt.prediction - Generation took 7.8051[sec]. (No references given)
2022-09-15 16:23:00,434 - INFO - joeynmt.training - Processing Predictions on Batch 49/105
2022-09-15 16:23:02,816 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:23:02,816 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:23:03,030 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:23:03,165 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:23:03,250 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:23:03,250 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:23:03,281 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:23:10,808 - INFO - joeynmt.prediction - Generation took 7.5184[sec]. (No references given)
2022-09-15 16:23:10,817 - INFO - joeynmt.training - Processing Predictions on Batch 50/105
2022-09-15 16:23:13,189 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:23:13,189 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:23:13,403 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:23:13,540 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:23:13,625 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:23:13,625 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:23:13,650 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:23:20,376 - INFO - joeynmt.prediction - Generation took 6.7175[sec]. (No references given)
2022-09-15 16:23:20,385 - INFO - joeynmt.training - Processing Predictions on Batch 51/105
2022-09-15 16:23:22,733 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:23:22,733 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:23:22,946 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:23:23,080 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:23:23,167 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:23:23,167 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:23:23,197 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:23:30,647 - INFO - joeynmt.prediction - Generation took 7.4408[sec]. (No references given)
2022-09-15 16:23:30,656 - INFO - joeynmt.training - Processing Predictions on Batch 52/105
2022-09-15 16:23:33,010 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:23:33,010 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:23:33,224 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:23:33,359 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:23:33,444 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:23:33,444 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:23:33,468 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:23:39,386 - INFO - joeynmt.prediction - Generation took 5.9099[sec]. (No references given)
2022-09-15 16:23:39,395 - INFO - joeynmt.training - Processing Predictions on Batch 53/105
2022-09-15 16:23:41,760 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:23:41,760 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:23:41,973 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:23:42,109 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:23:42,193 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:23:42,193 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:23:42,222 - INFO - joeynmt.prediction - Predicting 219 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:23:49,494 - INFO - joeynmt.prediction - Generation took 7.2649[sec]. (No references given)
2022-09-15 16:23:49,503 - INFO - joeynmt.training - Processing Predictions on Batch 54/105
2022-09-15 16:23:51,872 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:23:51,872 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:23:52,085 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:23:52,220 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:23:52,305 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:23:52,305 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:23:52,334 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:23:58,704 - INFO - joeynmt.prediction - Generation took 6.3617[sec]. (No references given)
2022-09-15 16:23:58,713 - INFO - joeynmt.training - Processing Predictions on Batch 55/105
2022-09-15 16:24:01,073 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:24:01,073 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:24:01,287 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:24:01,422 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:24:01,508 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:24:01,508 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:24:01,539 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:24:08,768 - INFO - joeynmt.prediction - Generation took 7.2214[sec]. (No references given)
2022-09-15 16:24:08,777 - INFO - joeynmt.training - Processing Predictions on Batch 56/105
2022-09-15 16:24:11,144 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:24:11,144 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:24:11,358 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:24:11,498 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:24:11,582 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:24:11,583 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:24:11,613 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:24:21,235 - INFO - joeynmt.prediction - Generation took 9.6134[sec]. (No references given)
2022-09-15 16:24:21,245 - INFO - joeynmt.training - Processing Predictions on Batch 57/105
2022-09-15 16:24:23,635 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:24:23,635 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:24:23,849 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:24:23,984 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:24:24,070 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:24:24,070 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:24:24,100 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:24:30,185 - INFO - joeynmt.prediction - Generation took 6.0762[sec]. (No references given)
2022-09-15 16:24:30,194 - INFO - joeynmt.training - Processing Predictions on Batch 58/105
2022-09-15 16:24:32,560 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:24:32,560 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:24:32,774 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:24:32,908 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:24:32,993 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:24:32,993 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:24:33,020 - INFO - joeynmt.prediction - Predicting 217 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:24:39,160 - INFO - joeynmt.prediction - Generation took 6.1320[sec]. (No references given)
2022-09-15 16:24:39,169 - INFO - joeynmt.training - Processing Predictions on Batch 59/105
2022-09-15 16:24:41,520 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:24:41,520 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:24:41,732 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:24:41,867 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:24:41,952 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:24:41,952 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:24:41,977 - INFO - joeynmt.prediction - Predicting 216 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:24:50,026 - INFO - joeynmt.prediction - Generation took 8.0424[sec]. (No references given)
2022-09-15 16:24:50,035 - INFO - joeynmt.training - Processing Predictions on Batch 60/105
2022-09-15 16:24:52,412 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:24:52,413 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:24:52,626 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:24:52,760 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:24:52,845 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:24:52,846 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:24:52,872 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:24:59,671 - INFO - joeynmt.prediction - Generation took 6.7919[sec]. (No references given)
2022-09-15 16:24:59,680 - INFO - joeynmt.training - Processing Predictions on Batch 61/105
2022-09-15 16:25:02,053 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:25:02,053 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:25:02,266 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:25:02,402 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:25:02,488 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:25:02,488 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:25:02,514 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:25:09,643 - INFO - joeynmt.prediction - Generation took 7.1215[sec]. (No references given)
2022-09-15 16:25:09,652 - INFO - joeynmt.training - Processing Predictions on Batch 62/105
2022-09-15 16:25:12,032 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:25:12,032 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:25:12,246 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:25:12,380 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:25:12,465 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:25:12,465 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:25:12,497 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:25:20,936 - INFO - joeynmt.prediction - Generation took 8.4311[sec]. (No references given)
2022-09-15 16:25:20,946 - INFO - joeynmt.training - Processing Predictions on Batch 63/105
2022-09-15 16:25:23,325 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:25:23,326 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:25:23,539 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:25:23,673 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:25:23,758 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:25:23,758 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:25:23,787 - INFO - joeynmt.prediction - Predicting 223 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:25:31,446 - INFO - joeynmt.prediction - Generation took 7.6508[sec]. (No references given)
2022-09-15 16:25:31,456 - INFO - joeynmt.training - Processing Predictions on Batch 64/105
2022-09-15 16:25:33,813 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:25:33,813 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:25:34,026 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:25:34,161 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:25:34,700 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:25:34,701 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:25:34,731 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:25:43,884 - INFO - joeynmt.prediction - Generation took 9.1448[sec]. (No references given)
2022-09-15 16:25:43,893 - INFO - joeynmt.training - Processing Predictions on Batch 65/105
2022-09-15 16:25:46,260 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:25:46,260 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:25:46,475 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:25:46,608 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:25:46,694 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:25:46,694 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:25:46,719 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:25:53,095 - INFO - joeynmt.prediction - Generation took 6.3680[sec]. (No references given)
2022-09-15 16:25:53,104 - INFO - joeynmt.training - Processing Predictions on Batch 66/105
2022-09-15 16:25:55,472 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:25:55,472 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:25:55,686 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:25:55,819 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:25:55,905 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:25:55,905 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:25:55,934 - INFO - joeynmt.prediction - Predicting 224 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:26:01,015 - INFO - joeynmt.prediction - Generation took 5.0733[sec]. (No references given)
2022-09-15 16:26:01,024 - INFO - joeynmt.training - Processing Predictions on Batch 67/105
2022-09-15 16:26:03,386 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:26:03,387 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:26:03,602 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:26:03,736 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:26:03,822 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:26:03,822 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:26:03,848 - INFO - joeynmt.prediction - Predicting 224 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:26:11,423 - INFO - joeynmt.prediction - Generation took 7.5676[sec]. (No references given)
2022-09-15 16:26:11,433 - INFO - joeynmt.training - Processing Predictions on Batch 68/105
2022-09-15 16:26:13,812 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:26:13,812 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:26:14,025 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:26:14,158 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:26:14,244 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:26:14,244 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:26:14,274 - INFO - joeynmt.prediction - Predicting 220 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:26:21,101 - INFO - joeynmt.prediction - Generation took 6.8200[sec]. (No references given)
2022-09-15 16:26:21,111 - INFO - joeynmt.training - Processing Predictions on Batch 69/105
2022-09-15 16:26:23,464 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:26:23,464 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:26:23,677 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:26:23,810 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:26:23,895 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:26:23,895 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:26:23,924 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:26:32,623 - INFO - joeynmt.prediction - Generation took 8.6902[sec]. (No references given)
2022-09-15 16:26:32,632 - INFO - joeynmt.training - Processing Predictions on Batch 70/105
2022-09-15 16:26:34,986 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:26:34,986 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:26:35,201 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:26:35,334 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:26:35,421 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:26:35,421 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:26:35,448 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:26:43,092 - INFO - joeynmt.prediction - Generation took 7.6372[sec]. (No references given)
2022-09-15 16:26:43,102 - INFO - joeynmt.training - Processing Predictions on Batch 71/105
2022-09-15 16:26:45,462 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:26:45,462 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:26:45,676 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:26:45,809 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:26:45,894 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:26:45,894 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:26:45,923 - INFO - joeynmt.prediction - Predicting 224 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:26:52,930 - INFO - joeynmt.prediction - Generation took 6.9992[sec]. (No references given)
2022-09-15 16:26:52,939 - INFO - joeynmt.training - Processing Predictions on Batch 72/105
2022-09-15 16:26:55,294 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:26:55,294 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:26:55,508 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:26:55,640 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:26:55,727 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:26:55,727 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:26:55,756 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:27:02,951 - INFO - joeynmt.prediction - Generation took 7.1865[sec]. (No references given)
2022-09-15 16:27:02,960 - INFO - joeynmt.training - Processing Predictions on Batch 73/105
2022-09-15 16:27:05,298 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:27:05,298 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:27:05,511 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:27:05,646 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:27:05,732 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:27:05,732 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:27:05,762 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:27:13,001 - INFO - joeynmt.prediction - Generation took 7.2300[sec]. (No references given)
2022-09-15 16:27:13,010 - INFO - joeynmt.training - Processing Predictions on Batch 74/105
2022-09-15 16:27:15,354 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:27:15,355 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:27:15,569 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:27:15,702 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:27:15,788 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:27:15,788 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:27:15,817 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:27:23,390 - INFO - joeynmt.prediction - Generation took 7.5656[sec]. (No references given)
2022-09-15 16:27:23,400 - INFO - joeynmt.training - Processing Predictions on Batch 75/105
2022-09-15 16:27:25,759 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:27:25,759 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:27:25,974 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:27:26,107 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:27:26,193 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:27:26,193 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:27:26,223 - INFO - joeynmt.prediction - Predicting 218 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:27:33,476 - INFO - joeynmt.prediction - Generation took 7.2449[sec]. (No references given)
2022-09-15 16:27:33,485 - INFO - joeynmt.training - Processing Predictions on Batch 76/105
2022-09-15 16:27:35,867 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:27:35,867 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:27:36,082 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:27:36,215 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:27:36,303 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:27:36,303 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:27:36,331 - INFO - joeynmt.prediction - Predicting 224 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:27:43,970 - INFO - joeynmt.prediction - Generation took 7.6307[sec]. (No references given)
2022-09-15 16:27:43,979 - INFO - joeynmt.training - Processing Predictions on Batch 77/105
2022-09-15 16:27:46,368 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:27:46,368 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:27:46,582 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:27:46,716 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:27:46,802 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:27:46,802 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:27:46,829 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:27:54,233 - INFO - joeynmt.prediction - Generation took 7.3965[sec]. (No references given)
2022-09-15 16:27:54,242 - INFO - joeynmt.training - Processing Predictions on Batch 78/105
2022-09-15 16:27:56,620 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:27:56,620 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:27:56,835 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:27:56,969 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:27:57,056 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:27:57,056 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:27:57,082 - INFO - joeynmt.prediction - Predicting 217 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:28:02,728 - INFO - joeynmt.prediction - Generation took 5.6378[sec]. (No references given)
2022-09-15 16:28:02,738 - INFO - joeynmt.training - Processing Predictions on Batch 79/105
2022-09-15 16:28:05,108 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:28:05,108 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:28:05,323 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:28:05,457 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:28:05,544 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:28:05,544 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:28:05,572 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:28:12,595 - INFO - joeynmt.prediction - Generation took 7.0152[sec]. (No references given)
2022-09-15 16:28:12,604 - INFO - joeynmt.training - Processing Predictions on Batch 80/105
2022-09-15 16:28:14,996 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:28:14,996 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:28:15,210 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:28:15,344 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:28:15,430 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:28:15,430 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:28:15,459 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:28:23,891 - INFO - joeynmt.prediction - Generation took 8.4234[sec]. (No references given)
2022-09-15 16:28:23,900 - INFO - joeynmt.training - Processing Predictions on Batch 81/105
2022-09-15 16:28:26,278 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:28:26,278 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:28:26,493 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:28:26,626 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:28:26,712 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:28:26,713 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:28:26,741 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:28:34,786 - INFO - joeynmt.prediction - Generation took 8.0358[sec]. (No references given)
2022-09-15 16:28:34,795 - INFO - joeynmt.training - Processing Predictions on Batch 82/105
2022-09-15 16:28:37,174 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:28:37,174 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:28:37,389 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:28:37,524 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:28:37,611 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:28:37,611 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:28:37,637 - INFO - joeynmt.prediction - Predicting 220 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:28:45,108 - INFO - joeynmt.prediction - Generation took 7.4634[sec]. (No references given)
2022-09-15 16:28:45,117 - INFO - joeynmt.training - Processing Predictions on Batch 83/105
2022-09-15 16:28:47,494 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:28:47,494 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:28:47,709 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:28:47,842 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:28:47,937 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:28:47,937 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:28:47,964 - INFO - joeynmt.prediction - Predicting 219 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:28:56,702 - INFO - joeynmt.prediction - Generation took 8.7299[sec]. (No references given)
2022-09-15 16:28:56,711 - INFO - joeynmt.training - Processing Predictions on Batch 84/105
2022-09-15 16:28:59,047 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:28:59,047 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:28:59,266 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:28:59,399 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:28:59,484 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:28:59,485 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:28:59,513 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:29:08,256 - INFO - joeynmt.prediction - Generation took 8.7359[sec]. (No references given)
2022-09-15 16:29:08,266 - INFO - joeynmt.training - Processing Predictions on Batch 85/105
2022-09-15 16:29:10,609 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:29:10,610 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:29:10,825 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:29:10,959 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:29:11,046 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:29:11,047 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:29:11,076 - INFO - joeynmt.prediction - Predicting 219 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:29:18,879 - INFO - joeynmt.prediction - Generation took 7.7945[sec]. (No references given)
2022-09-15 16:29:18,888 - INFO - joeynmt.training - Processing Predictions on Batch 86/105
2022-09-15 16:29:21,251 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:29:21,251 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:29:21,464 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:29:21,597 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:29:21,684 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:29:21,685 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:29:21,711 - INFO - joeynmt.prediction - Predicting 216 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:29:29,626 - INFO - joeynmt.prediction - Generation took 7.9067[sec]. (No references given)
2022-09-15 16:29:29,635 - INFO - joeynmt.training - Processing Predictions on Batch 87/105
2022-09-15 16:29:32,021 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:29:32,021 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:29:32,239 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:29:32,371 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:29:32,458 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:29:32,458 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:29:32,487 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:29:39,615 - INFO - joeynmt.prediction - Generation took 7.1198[sec]. (No references given)
2022-09-15 16:29:39,624 - INFO - joeynmt.training - Processing Predictions on Batch 88/105
2022-09-15 16:29:42,006 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:29:42,006 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:29:42,221 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:29:42,354 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:29:42,442 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:29:42,442 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:29:42,470 - INFO - joeynmt.prediction - Predicting 218 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:29:51,333 - INFO - joeynmt.prediction - Generation took 8.8554[sec]. (No references given)
2022-09-15 16:29:51,342 - INFO - joeynmt.training - Processing Predictions on Batch 89/105
2022-09-15 16:29:53,709 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:29:53,709 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:29:53,923 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:29:54,056 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:29:54,142 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:29:54,143 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:29:54,169 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:30:02,171 - INFO - joeynmt.prediction - Generation took 7.9945[sec]. (No references given)
2022-09-15 16:30:02,180 - INFO - joeynmt.training - Processing Predictions on Batch 90/105
2022-09-15 16:30:04,535 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:30:04,535 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:30:04,749 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:30:04,882 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:30:04,968 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:30:04,968 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:30:04,997 - INFO - joeynmt.prediction - Predicting 223 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:30:11,097 - INFO - joeynmt.prediction - Generation took 6.0926[sec]. (No references given)
2022-09-15 16:30:11,107 - INFO - joeynmt.training - Processing Predictions on Batch 91/105
2022-09-15 16:30:13,485 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:30:13,485 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:30:13,700 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:30:13,832 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:30:13,919 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:30:13,919 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:30:13,947 - INFO - joeynmt.prediction - Predicting 223 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:30:21,470 - INFO - joeynmt.prediction - Generation took 7.5149[sec]. (No references given)
2022-09-15 16:30:21,479 - INFO - joeynmt.training - Processing Predictions on Batch 92/105
2022-09-15 16:30:23,845 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:30:23,845 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:30:24,060 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:30:24,194 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:30:24,280 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:30:24,280 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:30:24,305 - INFO - joeynmt.prediction - Predicting 220 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:30:30,495 - INFO - joeynmt.prediction - Generation took 6.1827[sec]. (No references given)
2022-09-15 16:30:30,504 - INFO - joeynmt.training - Processing Predictions on Batch 93/105
2022-09-15 16:30:32,874 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:30:32,874 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:30:33,088 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:30:33,222 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:30:33,309 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:30:33,309 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:30:33,336 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:30:42,586 - INFO - joeynmt.prediction - Generation took 9.2418[sec]. (No references given)
2022-09-15 16:30:42,596 - INFO - joeynmt.training - Processing Predictions on Batch 94/105
2022-09-15 16:30:44,960 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:30:44,961 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:30:45,174 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:30:45,308 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:30:45,396 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:30:45,396 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:30:45,423 - INFO - joeynmt.prediction - Predicting 220 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:30:51,383 - INFO - joeynmt.prediction - Generation took 5.9523[sec]. (No references given)
2022-09-15 16:30:51,393 - INFO - joeynmt.training - Processing Predictions on Batch 95/105
2022-09-15 16:30:53,762 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:30:53,762 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:30:53,975 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:30:54,109 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:30:54,196 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:30:54,196 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:30:54,221 - INFO - joeynmt.prediction - Predicting 214 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:30:59,485 - INFO - joeynmt.prediction - Generation took 5.2568[sec]. (No references given)
2022-09-15 16:30:59,494 - INFO - joeynmt.training - Processing Predictions on Batch 96/105
2022-09-15 16:31:01,815 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:31:01,815 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:31:02,030 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:31:02,163 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:31:02,249 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:31:02,249 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:31:02,274 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:31:10,080 - INFO - joeynmt.prediction - Generation took 7.7975[sec]. (No references given)
2022-09-15 16:31:10,089 - INFO - joeynmt.training - Processing Predictions on Batch 97/105
2022-09-15 16:31:12,432 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:31:12,432 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:31:12,646 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:31:12,779 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:31:12,865 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:31:12,865 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:31:12,891 - INFO - joeynmt.prediction - Predicting 218 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:31:18,956 - INFO - joeynmt.prediction - Generation took 6.0573[sec]. (No references given)
2022-09-15 16:31:18,965 - INFO - joeynmt.training - Processing Predictions on Batch 98/105
2022-09-15 16:31:21,307 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:31:21,307 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:31:21,522 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:31:21,655 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:31:21,742 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:31:21,742 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:31:21,770 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:31:30,253 - INFO - joeynmt.prediction - Generation took 8.4747[sec]. (No references given)
2022-09-15 16:31:30,262 - INFO - joeynmt.training - Processing Predictions on Batch 99/105
2022-09-15 16:31:32,589 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:31:32,589 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:31:32,804 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:31:32,936 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:31:33,022 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:31:33,023 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:31:33,049 - INFO - joeynmt.prediction - Predicting 217 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:31:40,476 - INFO - joeynmt.prediction - Generation took 7.4194[sec]. (No references given)
2022-09-15 16:31:40,486 - INFO - joeynmt.training - Processing Predictions on Batch 100/105
2022-09-15 16:31:42,830 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:31:42,830 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:31:43,044 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:31:43,177 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:31:43,264 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:31:43,264 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:31:43,290 - INFO - joeynmt.prediction - Predicting 219 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:31:51,066 - INFO - joeynmt.prediction - Generation took 7.7682[sec]. (No references given)
2022-09-15 16:31:51,075 - INFO - joeynmt.training - Processing Predictions on Batch 101/105
2022-09-15 16:31:53,410 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:31:53,410 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:31:53,632 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:31:53,766 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:31:53,853 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:31:53,853 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:31:53,882 - INFO - joeynmt.prediction - Predicting 219 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:32:01,194 - INFO - joeynmt.prediction - Generation took 7.3033[sec]. (No references given)
2022-09-15 16:32:01,203 - INFO - joeynmt.training - Processing Predictions on Batch 102/105
2022-09-15 16:32:03,551 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:32:03,551 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:32:03,767 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:32:03,901 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:32:03,987 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:32:03,987 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:32:04,015 - INFO - joeynmt.prediction - Predicting 220 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:32:12,008 - INFO - joeynmt.prediction - Generation took 7.9849[sec]. (No references given)
2022-09-15 16:32:12,017 - INFO - joeynmt.training - Processing Predictions on Batch 103/105
2022-09-15 16:32:14,373 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:32:14,374 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:32:14,588 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:32:14,721 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:32:14,807 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:32:14,807 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:32:14,835 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:32:22,635 - INFO - joeynmt.prediction - Generation took 7.7929[sec]. (No references given)
2022-09-15 16:32:22,645 - INFO - joeynmt.training - Processing Predictions on Batch 104/105
2022-09-15 16:32:25,009 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 16:32:25,009 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 16:32:25,223 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 16:32:25,357 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt.
2022-09-15 16:32:25,444 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:32:25,444 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 16:32:25,458 - INFO - joeynmt.prediction - Predicting 107 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:32:29,081 - INFO - joeynmt.prediction - Generation took 3.6190[sec]. (No references given)
2022-09-15 16:32:29,257 - INFO - joeynmt.training - Final Query Indices picked: [365310, 257506, 31126, 308206, 125736, 407191, 148961, 113154, 31401, 22010] length: 10000
2022-09-15 16:32:29,257 - INFO - joeynmt.training - Query the samples, one at a time (interactive/file)
2022-09-15 16:32:31,967 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 4096
	effective batch size (w. parallel & accumulation): 4096
2022-09-15 16:32:31,967 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:50:15,535 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 16:50:15,536 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.47, loss:   2.58, ppl:  13.18, acc:   0.51, generation: 1056.6399[sec], evaluation: 6.5814[sec]
2022-09-15 16:50:17,223 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/215000.ckpt
2022-09-15 16:50:17,265 - INFO - joeynmt.training - Example #0
2022-09-15 16:50:17,277 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 16:50:17,277 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 16:50:17,277 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 16:50:17,277 - INFO - joeynmt.training - Example #1
2022-09-15 16:50:17,287 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 16:50:17,287 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 16:50:17,287 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-15 16:50:17,287 - INFO - joeynmt.training - Example #2
2022-09-15 16:50:17,297 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 16:50:17,298 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 16:50:17,298 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 16:50:17,298 - INFO - joeynmt.training - Example #3
2022-09-15 16:50:17,308 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 16:50:17,308 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 16:50:17,308 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 16:50:17,402 - INFO - joeynmt.training - EPOCH 1
2022-09-15 16:50:28,115 - INFO - joeynmt.training - Epoch   1, Step:   217500, Batch Loss:     2.635656, Batch Acc: 0.012480, Tokens per Sec:     4690, Lr: 0.000035
2022-09-15 16:50:51,065 - INFO - joeynmt.training - Epoch   1, Step:   217600, Batch Loss:     2.822011, Batch Acc: 0.004004, Tokens per Sec:     4962, Lr: 0.000035
2022-09-15 16:51:14,030 - INFO - joeynmt.training - Epoch   1, Step:   217700, Batch Loss:     2.775847, Batch Acc: 0.005368, Tokens per Sec:     4989, Lr: 0.000035
2022-09-15 16:51:36,961 - INFO - joeynmt.training - Epoch   1, Step:   217800, Batch Loss:     2.746911, Batch Acc: 0.004328, Tokens per Sec:     4937, Lr: 0.000035
2022-09-15 16:51:59,783 - INFO - joeynmt.training - Epoch   1, Step:   217900, Batch Loss:     2.768840, Batch Acc: 0.005587, Tokens per Sec:     4965, Lr: 0.000035
2022-09-15 16:52:22,696 - INFO - joeynmt.training - Epoch   1, Step:   218000, Batch Loss:     2.726299, Batch Acc: 0.005837, Tokens per Sec:     4980, Lr: 0.000035
2022-09-15 16:52:22,697 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 17:10:09,733 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 17:10:09,735 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.50, loss:   2.58, ppl:  13.17, acc:   0.51, generation: 1059.7780[sec], evaluation: 6.9131[sec]
2022-09-15 17:10:11,224 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/216000.ckpt
2022-09-15 17:10:11,267 - INFO - joeynmt.training - Example #0
2022-09-15 17:10:11,279 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 17:10:11,279 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 17:10:11,279 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 17:10:11,279 - INFO - joeynmt.training - Example #1
2022-09-15 17:10:11,290 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 17:10:11,290 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 17:10:11,290 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 17:10:11,290 - INFO - joeynmt.training - Example #2
2022-09-15 17:10:11,301 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 17:10:11,301 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 17:10:11,301 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 17:10:11,301 - INFO - joeynmt.training - Example #3
2022-09-15 17:10:11,311 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 17:10:11,311 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 17:10:11,311 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 17:10:34,281 - INFO - joeynmt.training - Epoch   1, Step:   218100, Batch Loss:     2.664490, Batch Acc: 0.004172, Tokens per Sec:     4679, Lr: 0.000035
2022-09-15 17:10:57,135 - INFO - joeynmt.training - Epoch   1, Step:   218200, Batch Loss:     2.817613, Batch Acc: 0.004693, Tokens per Sec:     5017, Lr: 0.000035
2022-09-15 17:11:20,001 - INFO - joeynmt.training - Epoch   1, Step:   218300, Batch Loss:     2.880108, Batch Acc: 0.003704, Tokens per Sec:     5006, Lr: 0.000035
2022-09-15 17:11:42,804 - INFO - joeynmt.training - Epoch   1, Step:   218400, Batch Loss:     2.501879, Batch Acc: 0.005170, Tokens per Sec:     5013, Lr: 0.000035
2022-09-15 17:12:05,693 - INFO - joeynmt.training - Epoch   1, Step:   218500, Batch Loss:     2.636468, Batch Acc: 0.003490, Tokens per Sec:     4969, Lr: 0.000035
2022-09-15 17:12:28,508 - INFO - joeynmt.training - Epoch   1, Step:   218600, Batch Loss:     2.617545, Batch Acc: 0.004253, Tokens per Sec:     4999, Lr: 0.000035
2022-09-15 17:12:51,271 - INFO - joeynmt.training - Epoch   1, Step:   218700, Batch Loss:     2.447542, Batch Acc: 0.004952, Tokens per Sec:     4995, Lr: 0.000035
2022-09-15 17:13:14,043 - INFO - joeynmt.training - Epoch   1, Step:   218800, Batch Loss:     2.747959, Batch Acc: 0.004202, Tokens per Sec:     5048, Lr: 0.000035
2022-09-15 17:13:37,118 - INFO - joeynmt.training - Epoch   1, Step:   218900, Batch Loss:     2.785246, Batch Acc: 0.004225, Tokens per Sec:     4954, Lr: 0.000035
2022-09-15 17:13:59,827 - INFO - joeynmt.training - Epoch   1, Step:   219000, Batch Loss:     2.717460, Batch Acc: 0.003926, Tokens per Sec:     4947, Lr: 0.000035
2022-09-15 17:13:59,827 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 17:30:21,705 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 17:30:21,707 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.50, loss:   2.57, ppl:  13.13, acc:   0.51, generation: 974.9579[sec], evaluation: 6.5782[sec]
2022-09-15 17:30:21,711 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 17:30:23,211 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217456.ckpt
2022-09-15 17:30:23,254 - INFO - joeynmt.training - Example #0
2022-09-15 17:30:23,266 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 17:30:23,266 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 17:30:23,266 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 17:30:23,266 - INFO - joeynmt.training - Example #1
2022-09-15 17:30:23,277 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 17:30:23,277 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 17:30:23,277 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-15 17:30:23,277 - INFO - joeynmt.training - Example #2
2022-09-15 17:30:23,288 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 17:30:23,288 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 17:30:23,288 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 17:30:23,288 - INFO - joeynmt.training - Example #3
2022-09-15 17:30:23,298 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 17:30:23,298 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 17:30:23,298 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 17:30:46,420 - INFO - joeynmt.training - Epoch   1, Step:   219100, Batch Loss:     2.655686, Batch Acc: 0.004130, Tokens per Sec:     4576, Lr: 0.000035
2022-09-15 17:31:09,133 - INFO - joeynmt.training - Epoch   1, Step:   219200, Batch Loss:     2.734074, Batch Acc: 0.004454, Tokens per Sec:     4972, Lr: 0.000035
2022-09-15 17:31:31,859 - INFO - joeynmt.training - Epoch   1, Step:   219300, Batch Loss:     2.415550, Batch Acc: 0.004452, Tokens per Sec:     4972, Lr: 0.000035
2022-09-15 17:31:54,552 - INFO - joeynmt.training - Epoch   1, Step:   219400, Batch Loss:     2.829047, Batch Acc: 0.004443, Tokens per Sec:     4979, Lr: 0.000035
2022-09-15 17:32:17,282 - INFO - joeynmt.training - Epoch   1, Step:   219500, Batch Loss:     2.749494, Batch Acc: 0.004681, Tokens per Sec:     5047, Lr: 0.000035
2022-09-15 17:32:40,038 - INFO - joeynmt.training - Epoch   1, Step:   219600, Batch Loss:     2.916791, Batch Acc: 0.004342, Tokens per Sec:     4989, Lr: 0.000035
2022-09-15 17:33:02,810 - INFO - joeynmt.training - Epoch   1, Step:   219700, Batch Loss:     2.729829, Batch Acc: 0.004810, Tokens per Sec:     4958, Lr: 0.000035
2022-09-15 17:33:25,640 - INFO - joeynmt.training - Epoch   1, Step:   219800, Batch Loss:     2.581290, Batch Acc: 0.005686, Tokens per Sec:     5023, Lr: 0.000035
2022-09-15 17:33:48,496 - INFO - joeynmt.training - Epoch   1, Step:   219900, Batch Loss:     2.835344, Batch Acc: 0.004015, Tokens per Sec:     4980, Lr: 0.000035
2022-09-15 17:34:11,103 - INFO - joeynmt.training - Epoch   1, Step:   220000, Batch Loss:     2.770189, Batch Acc: 0.004896, Tokens per Sec:     4997, Lr: 0.000035
2022-09-15 17:34:11,104 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 17:51:13,324 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 17:51:13,326 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.53, loss:   2.57, ppl:  13.00, acc:   0.51, generation: 1014.8727[sec], evaluation: 7.0054[sec]
2022-09-15 17:51:13,329 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 17:51:14,817 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/218000.ckpt
2022-09-15 17:51:14,859 - INFO - joeynmt.training - Example #0
2022-09-15 17:51:14,872 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 17:51:14,872 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 17:51:14,872 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 17:51:14,872 - INFO - joeynmt.training - Example #1
2022-09-15 17:51:14,883 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 17:51:14,883 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 17:51:14,883 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-15 17:51:14,883 - INFO - joeynmt.training - Example #2
2022-09-15 17:51:14,894 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 17:51:14,894 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 17:51:14,894 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 17:51:14,894 - INFO - joeynmt.training - Example #3
2022-09-15 17:51:14,905 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 17:51:14,905 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 17:51:14,905 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 17:51:37,650 - INFO - joeynmt.training - Epoch   1, Step:   220100, Batch Loss:     2.650484, Batch Acc: 0.004250, Tokens per Sec:     4624, Lr: 0.000035
2022-09-15 17:52:00,379 - INFO - joeynmt.training - Epoch   1, Step:   220200, Batch Loss:     2.653666, Batch Acc: 0.003867, Tokens per Sec:     5108, Lr: 0.000035
2022-09-15 17:52:23,103 - INFO - joeynmt.training - Epoch   1, Step:   220300, Batch Loss:     2.709321, Batch Acc: 0.004968, Tokens per Sec:     5058, Lr: 0.000035
2022-09-15 17:52:45,799 - INFO - joeynmt.training - Epoch   1, Step:   220400, Batch Loss:     2.629911, Batch Acc: 0.004569, Tokens per Sec:     5024, Lr: 0.000035
2022-09-15 17:53:08,684 - INFO - joeynmt.training - Epoch   1, Step:   220500, Batch Loss:     2.610523, Batch Acc: 0.004001, Tokens per Sec:     5046, Lr: 0.000035
2022-09-15 17:53:31,545 - INFO - joeynmt.training - Epoch   1, Step:   220600, Batch Loss:     2.577582, Batch Acc: 0.004927, Tokens per Sec:     4971, Lr: 0.000035
2022-09-15 17:53:54,238 - INFO - joeynmt.training - Epoch   1, Step:   220700, Batch Loss:     2.536879, Batch Acc: 0.004795, Tokens per Sec:     5064, Lr: 0.000035
2022-09-15 17:54:16,987 - INFO - joeynmt.training - Epoch   1, Step:   220800, Batch Loss:     2.632214, Batch Acc: 0.003825, Tokens per Sec:     5045, Lr: 0.000035
2022-09-15 17:54:39,815 - INFO - joeynmt.training - Epoch   1, Step:   220900, Batch Loss:     2.789020, Batch Acc: 0.005239, Tokens per Sec:     4992, Lr: 0.000035
2022-09-15 17:55:02,533 - INFO - joeynmt.training - Epoch   1, Step:   221000, Batch Loss:     2.564980, Batch Acc: 0.004678, Tokens per Sec:     5043, Lr: 0.000035
2022-09-15 17:55:02,533 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:11:01,982 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 18:11:01,983 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.42, loss:   2.57, ppl:  13.01, acc:   0.51, generation: 952.5663[sec], evaluation: 6.5406[sec]
2022-09-15 18:11:03,458 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/217000.ckpt
2022-09-15 18:11:03,502 - INFO - joeynmt.training - Example #0
2022-09-15 18:11:03,514 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 18:11:03,514 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 18:11:03,514 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 18:11:03,514 - INFO - joeynmt.training - Example #1
2022-09-15 18:11:03,525 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 18:11:03,525 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 18:11:03,525 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-15 18:11:03,525 - INFO - joeynmt.training - Example #2
2022-09-15 18:11:03,536 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 18:11:03,536 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 18:11:03,536 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 18:11:03,536 - INFO - joeynmt.training - Example #3
2022-09-15 18:11:03,547 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 18:11:03,547 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 18:11:03,547 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 18:11:26,475 - INFO - joeynmt.training - Epoch   1, Step:   221100, Batch Loss:     2.626991, Batch Acc: 0.004447, Tokens per Sec:     4665, Lr: 0.000035
2022-09-15 18:11:49,341 - INFO - joeynmt.training - Epoch   1, Step:   221200, Batch Loss:     2.846021, Batch Acc: 0.004258, Tokens per Sec:     4950, Lr: 0.000035
2022-09-15 18:12:12,009 - INFO - joeynmt.training - Epoch   1, Step:   221300, Batch Loss:     2.744149, Batch Acc: 0.003824, Tokens per Sec:     5076, Lr: 0.000035
2022-09-15 18:12:34,631 - INFO - joeynmt.training - Epoch   1, Step:   221400, Batch Loss:     2.813383, Batch Acc: 0.003637, Tokens per Sec:     4983, Lr: 0.000035
2022-09-15 18:12:57,450 - INFO - joeynmt.training - Epoch   1, Step:   221500, Batch Loss:     2.893158, Batch Acc: 0.003334, Tokens per Sec:     4983, Lr: 0.000035
2022-09-15 18:13:20,191 - INFO - joeynmt.training - Epoch   1, Step:   221600, Batch Loss:     2.541721, Batch Acc: 0.004684, Tokens per Sec:     5013, Lr: 0.000035
2022-09-15 18:13:42,958 - INFO - joeynmt.training - Epoch   1, Step:   221700, Batch Loss:     2.724781, Batch Acc: 0.005651, Tokens per Sec:     4998, Lr: 0.000035
2022-09-15 18:14:05,758 - INFO - joeynmt.training - Epoch   1, Step:   221800, Batch Loss:     2.653367, Batch Acc: 0.004629, Tokens per Sec:     5050, Lr: 0.000035
2022-09-15 18:14:28,494 - INFO - joeynmt.training - Epoch   1, Step:   221900, Batch Loss:     2.666756, Batch Acc: 0.004933, Tokens per Sec:     5038, Lr: 0.000035
2022-09-15 18:14:51,197 - INFO - joeynmt.training - Epoch   1, Step:   222000, Batch Loss:     2.596555, Batch Acc: 0.004213, Tokens per Sec:     4997, Lr: 0.000035
2022-09-15 18:14:51,198 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:31:42,356 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 18:31:42,358 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.56, loss:   2.56, ppl:  13.00, acc:   0.51, generation: 1003.7668[sec], evaluation: 6.6717[sec]
2022-09-15 18:31:42,362 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 18:31:43,855 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/219000.ckpt
2022-09-15 18:31:43,901 - INFO - joeynmt.training - Example #0
2022-09-15 18:31:43,913 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 18:31:43,913 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 18:31:43,913 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 18:31:43,913 - INFO - joeynmt.training - Example #1
2022-09-15 18:31:43,924 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 18:31:43,924 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 18:31:43,925 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 18:31:43,925 - INFO - joeynmt.training - Example #2
2022-09-15 18:31:43,936 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 18:31:43,936 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 18:31:43,936 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 18:31:43,936 - INFO - joeynmt.training - Example #3
2022-09-15 18:31:43,947 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 18:31:43,947 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 18:31:43,947 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 18:32:06,768 - INFO - joeynmt.training - Epoch   1, Step:   222100, Batch Loss:     2.616599, Batch Acc: 0.005317, Tokens per Sec:     4708, Lr: 0.000035
2022-09-15 18:32:29,581 - INFO - joeynmt.training - Epoch   1, Step:   222200, Batch Loss:     2.547136, Batch Acc: 0.004966, Tokens per Sec:     5066, Lr: 0.000035
2022-09-15 18:32:52,233 - INFO - joeynmt.training - Epoch   1, Step:   222300, Batch Loss:     2.463085, Batch Acc: 0.004680, Tokens per Sec:     5009, Lr: 0.000035
2022-09-15 18:33:14,920 - INFO - joeynmt.training - Epoch   1, Step:   222400, Batch Loss:     2.907538, Batch Acc: 0.004709, Tokens per Sec:     5064, Lr: 0.000035
2022-09-15 18:33:37,669 - INFO - joeynmt.training - Epoch   1, Step:   222500, Batch Loss:     2.761390, Batch Acc: 0.004933, Tokens per Sec:     5035, Lr: 0.000035
2022-09-15 18:34:00,431 - INFO - joeynmt.training - Epoch   1, Step:   222600, Batch Loss:     2.815477, Batch Acc: 0.005034, Tokens per Sec:     5132, Lr: 0.000035
2022-09-15 18:34:23,182 - INFO - joeynmt.training - Epoch   1, Step:   222700, Batch Loss:     2.966098, Batch Acc: 0.003820, Tokens per Sec:     5005, Lr: 0.000035
2022-09-15 18:34:45,946 - INFO - joeynmt.training - Epoch   1, Step:   222800, Batch Loss:     2.661414, Batch Acc: 0.004344, Tokens per Sec:     4996, Lr: 0.000035
2022-09-15 18:35:08,631 - INFO - joeynmt.training - Epoch   1, Step:   222900, Batch Loss:     2.698258, Batch Acc: 0.005514, Tokens per Sec:     4997, Lr: 0.000035
2022-09-15 18:35:31,461 - INFO - joeynmt.training - Epoch   1, Step:   223000, Batch Loss:     2.886015, Batch Acc: 0.003986, Tokens per Sec:     5066, Lr: 0.000035
2022-09-15 18:35:31,462 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:51:36,559 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 18:51:36,561 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.52, loss:   2.56, ppl:  12.98, acc:   0.51, generation: 957.6382[sec], evaluation: 7.1183[sec]
2022-09-15 18:51:36,565 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 18:51:38,744 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/221000.ckpt
2022-09-15 18:51:38,785 - INFO - joeynmt.training - Example #0
2022-09-15 18:51:38,798 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 18:51:38,798 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 18:51:38,798 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 18:51:38,798 - INFO - joeynmt.training - Example #1
2022-09-15 18:51:38,810 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 18:51:38,810 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 18:51:38,810 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 18:51:38,810 - INFO - joeynmt.training - Example #2
2022-09-15 18:51:38,821 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 18:51:38,821 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 18:51:38,821 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 18:51:38,821 - INFO - joeynmt.training - Example #3
2022-09-15 18:51:38,833 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 18:51:38,833 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 18:51:38,833 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 18:52:01,634 - INFO - joeynmt.training - Epoch   1, Step:   223100, Batch Loss:     2.992178, Batch Acc: 0.003474, Tokens per Sec:     4616, Lr: 0.000035
2022-09-15 18:52:24,326 - INFO - joeynmt.training - Epoch   1, Step:   223200, Batch Loss:     2.495091, Batch Acc: 0.004283, Tokens per Sec:     5053, Lr: 0.000035
2022-09-15 18:52:47,210 - INFO - joeynmt.training - Epoch   1, Step:   223300, Batch Loss:     2.771231, Batch Acc: 0.004299, Tokens per Sec:     5032, Lr: 0.000035
2022-09-15 18:53:09,956 - INFO - joeynmt.training - Epoch   1, Step:   223400, Batch Loss:     2.727968, Batch Acc: 0.005702, Tokens per Sec:     5035, Lr: 0.000035
2022-09-15 18:53:32,752 - INFO - joeynmt.training - Epoch   1, Step:   223500, Batch Loss:     2.623777, Batch Acc: 0.005128, Tokens per Sec:     5039, Lr: 0.000035
2022-09-15 18:53:55,605 - INFO - joeynmt.training - Epoch   1, Step:   223600, Batch Loss:     2.848084, Batch Acc: 0.003919, Tokens per Sec:     4991, Lr: 0.000035
2022-09-15 18:54:18,426 - INFO - joeynmt.training - Epoch   1, Step:   223700, Batch Loss:     2.698387, Batch Acc: 0.004701, Tokens per Sec:     5117, Lr: 0.000035
2022-09-15 18:54:41,174 - INFO - joeynmt.training - Epoch   1, Step:   223800, Batch Loss:     2.777075, Batch Acc: 0.004212, Tokens per Sec:     5135, Lr: 0.000035
2022-09-15 18:55:03,878 - INFO - joeynmt.training - Epoch   1, Step:   223900, Batch Loss:     2.739459, Batch Acc: 0.004461, Tokens per Sec:     5084, Lr: 0.000035
2022-09-15 18:55:26,617 - INFO - joeynmt.training - Epoch   1, Step:   224000, Batch Loss:     2.666784, Batch Acc: 0.004635, Tokens per Sec:     5085, Lr: 0.000035
2022-09-15 18:55:26,617 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 19:12:48,636 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 19:12:48,642 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 19:12:50,132 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/220000.ckpt
2022-09-15 19:12:50,174 - INFO - joeynmt.training - Example #0
2022-09-15 19:12:50,186 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 19:12:50,186 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 19:12:50,186 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 19:12:50,186 - INFO - joeynmt.training - Example #1
2022-09-15 19:12:50,197 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 19:12:50,197 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 19:12:50,197 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 19:12:50,197 - INFO - joeynmt.training - Example #2
2022-09-15 19:12:50,207 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 19:12:50,207 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 19:12:50,207 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 19:12:50,207 - INFO - joeynmt.training - Example #3
2022-09-15 19:12:50,218 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 19:12:50,218 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 19:12:50,218 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 19:13:13,029 - INFO - joeynmt.training - Epoch   1, Step:   224100, Batch Loss:     2.779548, Batch Acc: 0.004334, Tokens per Sec:     4674, Lr: 0.000035
2022-09-15 19:13:35,769 - INFO - joeynmt.training - Epoch   1, Step:   224200, Batch Loss:     2.866333, Batch Acc: 0.004504, Tokens per Sec:     5018, Lr: 0.000035
2022-09-15 19:13:58,390 - INFO - joeynmt.training - Epoch   1, Step:   224300, Batch Loss:     2.689110, Batch Acc: 0.004229, Tokens per Sec:     4966, Lr: 0.000035
2022-09-15 19:14:21,024 - INFO - joeynmt.training - Epoch   1, Step:   224400, Batch Loss:     2.678366, Batch Acc: 0.005505, Tokens per Sec:     5065, Lr: 0.000035
2022-09-15 19:14:43,774 - INFO - joeynmt.training - Epoch   1, Step:   224500, Batch Loss:     2.960385, Batch Acc: 0.004718, Tokens per Sec:     5041, Lr: 0.000035
2022-09-15 19:15:06,454 - INFO - joeynmt.training - Epoch   1, Step:   224600, Batch Loss:     2.481631, Batch Acc: 0.004341, Tokens per Sec:     5058, Lr: 0.000035
2022-09-15 19:15:29,271 - INFO - joeynmt.training - Epoch   1, Step:   224700, Batch Loss:     2.922106, Batch Acc: 0.003894, Tokens per Sec:     4953, Lr: 0.000035
2022-09-15 19:15:51,959 - INFO - joeynmt.training - Epoch   1, Step:   224800, Batch Loss:     2.661968, Batch Acc: 0.005065, Tokens per Sec:     5073, Lr: 0.000035
2022-09-15 19:16:14,729 - INFO - joeynmt.training - Epoch   1, Step:   224900, Batch Loss:     2.726246, Batch Acc: 0.004264, Tokens per Sec:     5006, Lr: 0.000035
2022-09-15 19:16:37,409 - INFO - joeynmt.training - Epoch   1, Step:   225000, Batch Loss:     2.768099, Batch Acc: 0.004269, Tokens per Sec:     5092, Lr: 0.000035
2022-09-15 19:16:37,409 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 19:33:03,952 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 19:33:03,954 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.68, loss:   2.56, ppl:  12.92, acc:   0.51, generation: 979.0447[sec], evaluation: 7.1550[sec]
2022-09-15 19:33:03,958 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 19:33:05,438 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/222000.ckpt
2022-09-15 19:33:05,481 - INFO - joeynmt.training - Example #0
2022-09-15 19:33:05,493 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 19:33:05,493 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 19:33:05,493 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 19:33:05,493 - INFO - joeynmt.training - Example #1
2022-09-15 19:33:05,503 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 19:33:05,503 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 19:33:05,504 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 19:33:05,504 - INFO - joeynmt.training - Example #2
2022-09-15 19:33:05,514 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 19:33:05,514 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 19:33:05,514 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 19:33:05,514 - INFO - joeynmt.training - Example #3
2022-09-15 19:33:05,525 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 19:33:05,525 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 19:33:05,525 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 19:33:28,416 - INFO - joeynmt.training - Epoch   1, Step:   225100, Batch Loss:     2.791409, Batch Acc: 0.004683, Tokens per Sec:     4767, Lr: 0.000035
2022-09-15 19:33:51,072 - INFO - joeynmt.training - Epoch   1, Step:   225200, Batch Loss:     2.813292, Batch Acc: 0.003927, Tokens per Sec:     5025, Lr: 0.000035
2022-09-15 19:34:13,835 - INFO - joeynmt.training - Epoch   1, Step:   225300, Batch Loss:     2.628616, Batch Acc: 0.004585, Tokens per Sec:     5059, Lr: 0.000035
2022-09-15 19:34:36,547 - INFO - joeynmt.training - Epoch   1, Step:   225400, Batch Loss:     2.726148, Batch Acc: 0.004254, Tokens per Sec:     5144, Lr: 0.000035
2022-09-15 19:34:59,295 - INFO - joeynmt.training - Epoch   1, Step:   225500, Batch Loss:     2.642256, Batch Acc: 0.004078, Tokens per Sec:     5002, Lr: 0.000035
2022-09-15 19:35:21,946 - INFO - joeynmt.training - Epoch   1, Step:   225600, Batch Loss:     2.644285, Batch Acc: 0.004883, Tokens per Sec:     5054, Lr: 0.000035
2022-09-15 19:35:44,639 - INFO - joeynmt.training - Epoch   1, Step:   225700, Batch Loss:     2.760471, Batch Acc: 0.004052, Tokens per Sec:     4959, Lr: 0.000035
2022-09-15 19:36:07,313 - INFO - joeynmt.training - Epoch   1, Step:   225800, Batch Loss:     2.704920, Batch Acc: 0.004629, Tokens per Sec:     5069, Lr: 0.000035
2022-09-15 19:36:29,992 - INFO - joeynmt.training - Epoch   1, Step:   225900, Batch Loss:     2.624461, Batch Acc: 0.003871, Tokens per Sec:     5092, Lr: 0.000035
2022-09-15 19:36:52,695 - INFO - joeynmt.training - Epoch   1, Step:   226000, Batch Loss:     2.558743, Batch Acc: 0.004468, Tokens per Sec:     5087, Lr: 0.000035
2022-09-15 19:36:52,696 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 19:54:13,229 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 19:54:13,234 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 19:54:14,930 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/223000.ckpt
2022-09-15 19:54:14,973 - INFO - joeynmt.training - Example #0
2022-09-15 19:54:14,985 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 19:54:14,985 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 19:54:14,985 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 19:54:14,985 - INFO - joeynmt.training - Example #1
2022-09-15 19:54:14,996 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 19:54:14,996 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 19:54:14,996 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-15 19:54:14,996 - INFO - joeynmt.training - Example #2
2022-09-15 19:54:15,007 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 19:54:15,007 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 19:54:15,007 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 19:54:15,007 - INFO - joeynmt.training - Example #3
2022-09-15 19:54:15,017 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 19:54:15,017 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 19:54:15,017 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 19:54:37,669 - INFO - joeynmt.training - Epoch   1, Step:   226100, Batch Loss:     2.607901, Batch Acc: 0.003806, Tokens per Sec:     4699, Lr: 0.000035
2022-09-15 19:55:00,386 - INFO - joeynmt.training - Epoch   1, Step:   226200, Batch Loss:     2.860208, Batch Acc: 0.003656, Tokens per Sec:     4984, Lr: 0.000035
2022-09-15 19:55:23,039 - INFO - joeynmt.training - Epoch   1, Step:   226300, Batch Loss:     2.789877, Batch Acc: 0.004573, Tokens per Sec:     5019, Lr: 0.000035
2022-09-15 19:55:45,702 - INFO - joeynmt.training - Epoch   1, Step:   226400, Batch Loss:     2.619346, Batch Acc: 0.004504, Tokens per Sec:     5104, Lr: 0.000035
2022-09-15 19:56:08,432 - INFO - joeynmt.training - Epoch   1, Step:   226500, Batch Loss:     2.614097, Batch Acc: 0.004696, Tokens per Sec:     5087, Lr: 0.000035
2022-09-15 19:56:31,164 - INFO - joeynmt.training - Epoch   1, Step:   226600, Batch Loss:     2.752042, Batch Acc: 0.004166, Tokens per Sec:     5037, Lr: 0.000035
2022-09-15 19:56:53,860 - INFO - joeynmt.training - Epoch   1, Step:   226700, Batch Loss:     2.753032, Batch Acc: 0.004076, Tokens per Sec:     5059, Lr: 0.000035
2022-09-15 19:57:16,521 - INFO - joeynmt.training - Epoch   1, Step:   226800, Batch Loss:     2.564301, Batch Acc: 0.005075, Tokens per Sec:     4973, Lr: 0.000035
2022-09-15 19:57:39,210 - INFO - joeynmt.training - Epoch   1, Step:   226900, Batch Loss:     2.760202, Batch Acc: 0.003790, Tokens per Sec:     5117, Lr: 0.000035
2022-09-15 19:58:01,886 - INFO - joeynmt.training - Epoch   1, Step:   227000, Batch Loss:     2.479619, Batch Acc: 0.004974, Tokens per Sec:     5009, Lr: 0.000035
2022-09-15 19:58:01,887 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 20:14:21,200 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 20:14:21,201 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.75, loss:   2.55, ppl:  12.83, acc:   0.51, generation: 972.3650[sec], evaluation: 6.6046[sec]
2022-09-15 20:14:22,766 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/224000.ckpt
2022-09-15 20:14:22,807 - INFO - joeynmt.training - Example #0
2022-09-15 20:14:22,820 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 20:14:22,820 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 20:14:22,820 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 20:14:22,820 - INFO - joeynmt.training - Example #1
2022-09-15 20:14:22,831 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 20:14:22,831 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 20:14:22,831 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-15 20:14:22,831 - INFO - joeynmt.training - Example #2
2022-09-15 20:14:22,842 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 20:14:22,842 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 20:14:22,842 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 20:14:22,842 - INFO - joeynmt.training - Example #3
2022-09-15 20:14:22,853 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 20:14:22,853 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 20:14:22,853 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 20:14:45,707 - INFO - joeynmt.training - Epoch   1, Step:   227100, Batch Loss:     2.718508, Batch Acc: 0.004232, Tokens per Sec:     4706, Lr: 0.000035
2022-09-15 20:15:08,329 - INFO - joeynmt.training - Epoch   1, Step:   227200, Batch Loss:     2.439079, Batch Acc: 0.005502, Tokens per Sec:     4989, Lr: 0.000035
2022-09-15 20:15:31,067 - INFO - joeynmt.training - Epoch   1, Step:   227300, Batch Loss:     2.456637, Batch Acc: 0.004855, Tokens per Sec:     5054, Lr: 0.000035
2022-09-15 20:15:53,656 - INFO - joeynmt.training - Epoch   1, Step:   227400, Batch Loss:     2.732264, Batch Acc: 0.004521, Tokens per Sec:     4965, Lr: 0.000035
2022-09-15 20:16:16,401 - INFO - joeynmt.training - Epoch   1, Step:   227500, Batch Loss:     2.710164, Batch Acc: 0.003757, Tokens per Sec:     5126, Lr: 0.000035
2022-09-15 20:16:39,259 - INFO - joeynmt.training - Epoch   1, Step:   227600, Batch Loss:     2.621650, Batch Acc: 0.003961, Tokens per Sec:     5025, Lr: 0.000035
2022-09-15 20:17:01,918 - INFO - joeynmt.training - Epoch   1, Step:   227700, Batch Loss:     2.823076, Batch Acc: 0.003333, Tokens per Sec:     4952, Lr: 0.000035
2022-09-15 20:17:24,619 - INFO - joeynmt.training - Epoch   1, Step:   227800, Batch Loss:     2.662319, Batch Acc: 0.004088, Tokens per Sec:     5032, Lr: 0.000035
2022-09-15 20:17:47,344 - INFO - joeynmt.training - Epoch   1, Step:   227900, Batch Loss:     2.452660, Batch Acc: 0.005244, Tokens per Sec:     5102, Lr: 0.000035
2022-09-15 20:18:09,991 - INFO - joeynmt.training - Epoch   1, Step:   228000, Batch Loss:     2.746833, Batch Acc: 0.005051, Tokens per Sec:     4983, Lr: 0.000035
2022-09-15 20:18:09,991 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 20:33:47,953 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 20:33:47,955 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.78, loss:   2.55, ppl:  12.80, acc:   0.51, generation: 930.4438[sec], evaluation: 6.7348[sec]
2022-09-15 20:33:49,448 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/225000.ckpt
2022-09-15 20:33:49,491 - INFO - joeynmt.training - Example #0
2022-09-15 20:33:49,503 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 20:33:49,503 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 20:33:49,503 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 20:33:49,503 - INFO - joeynmt.training - Example #1
2022-09-15 20:33:49,513 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 20:33:49,514 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 20:33:49,514 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-15 20:33:49,514 - INFO - joeynmt.training - Example #2
2022-09-15 20:33:49,524 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 20:33:49,524 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 20:33:49,524 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 20:33:49,524 - INFO - joeynmt.training - Example #3
2022-09-15 20:33:49,535 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 20:33:49,535 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 20:33:49,535 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 20:34:12,323 - INFO - joeynmt.training - Epoch   1, Step:   228100, Batch Loss:     2.728388, Batch Acc: 0.003913, Tokens per Sec:     4762, Lr: 0.000035
2022-09-15 20:34:35,127 - INFO - joeynmt.training - Epoch   1, Step:   228200, Batch Loss:     2.571475, Batch Acc: 0.004228, Tokens per Sec:     5020, Lr: 0.000035
2022-09-15 20:34:57,905 - INFO - joeynmt.training - Epoch   1, Step:   228300, Batch Loss:     2.490786, Batch Acc: 0.004742, Tokens per Sec:     5082, Lr: 0.000035
2022-09-15 20:35:20,555 - INFO - joeynmt.training - Epoch   1, Step:   228400, Batch Loss:     2.567570, Batch Acc: 0.004175, Tokens per Sec:     5086, Lr: 0.000035
2022-09-15 20:35:43,239 - INFO - joeynmt.training - Epoch   1, Step:   228500, Batch Loss:     2.753747, Batch Acc: 0.004098, Tokens per Sec:     5035, Lr: 0.000035
2022-09-15 20:36:05,973 - INFO - joeynmt.training - Epoch   1, Step:   228600, Batch Loss:     2.684994, Batch Acc: 0.004946, Tokens per Sec:     5052, Lr: 0.000035
2022-09-15 20:36:28,817 - INFO - joeynmt.training - Epoch   1, Step:   228700, Batch Loss:     2.946831, Batch Acc: 0.003242, Tokens per Sec:     5037, Lr: 0.000035
2022-09-15 20:36:51,562 - INFO - joeynmt.training - Epoch   1, Step:   228800, Batch Loss:     2.823613, Batch Acc: 0.004995, Tokens per Sec:     5052, Lr: 0.000035
2022-09-15 20:37:14,261 - INFO - joeynmt.training - Epoch   1, Step:   228900, Batch Loss:     2.726834, Batch Acc: 0.004521, Tokens per Sec:     5106, Lr: 0.000035
2022-09-15 20:37:37,008 - INFO - joeynmt.training - Epoch   1, Step:   229000, Batch Loss:     2.788924, Batch Acc: 0.004913, Tokens per Sec:     5155, Lr: 0.000035
2022-09-15 20:37:37,008 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 20:53:50,186 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 20:53:51,643 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/227000.ckpt
2022-09-15 20:53:51,686 - INFO - joeynmt.training - Example #0
2022-09-15 20:53:51,698 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 20:53:51,698 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 20:53:51,698 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 20:53:51,698 - INFO - joeynmt.training - Example #1
2022-09-15 20:53:51,708 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 20:53:51,709 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 20:53:51,709 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-15 20:53:51,709 - INFO - joeynmt.training - Example #2
2022-09-15 20:53:51,719 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 20:53:51,719 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 20:53:51,719 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 20:53:51,719 - INFO - joeynmt.training - Example #3
2022-09-15 20:53:51,730 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 20:53:51,730 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 20:53:51,730 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 20:54:14,710 - INFO - joeynmt.training - Epoch   1, Step:   229100, Batch Loss:     2.669822, Batch Acc: 0.004946, Tokens per Sec:     4659, Lr: 0.000035
2022-09-15 20:54:37,362 - INFO - joeynmt.training - Epoch   1, Step:   229200, Batch Loss:     2.715612, Batch Acc: 0.004253, Tokens per Sec:     5034, Lr: 0.000035
2022-09-15 20:55:00,081 - INFO - joeynmt.training - Epoch   1, Step:   229300, Batch Loss:     2.892075, Batch Acc: 0.004515, Tokens per Sec:     5079, Lr: 0.000035
2022-09-15 20:55:22,787 - INFO - joeynmt.training - Epoch   1, Step:   229400, Batch Loss:     2.464202, Batch Acc: 0.005236, Tokens per Sec:     5072, Lr: 0.000035
2022-09-15 20:55:45,573 - INFO - joeynmt.training - Epoch   1, Step:   229500, Batch Loss:     2.714499, Batch Acc: 0.003995, Tokens per Sec:     4988, Lr: 0.000035
2022-09-15 20:56:08,250 - INFO - joeynmt.training - Epoch   1, Step:   229600, Batch Loss:     2.896785, Batch Acc: 0.004811, Tokens per Sec:     5022, Lr: 0.000035
2022-09-15 20:56:30,939 - INFO - joeynmt.training - Epoch   1, Step:   229700, Batch Loss:     2.426706, Batch Acc: 0.004833, Tokens per Sec:     4952, Lr: 0.000035
2022-09-15 20:56:53,577 - INFO - joeynmt.training - Epoch   1, Step:   229800, Batch Loss:     2.371260, Batch Acc: 0.004522, Tokens per Sec:     5070, Lr: 0.000035
2022-09-15 20:57:16,215 - INFO - joeynmt.training - Epoch   1, Step:   229900, Batch Loss:     2.572614, Batch Acc: 0.003399, Tokens per Sec:     5108, Lr: 0.000035
2022-09-15 20:57:38,961 - INFO - joeynmt.training - Epoch   1, Step:   230000, Batch Loss:     2.723143, Batch Acc: 0.004055, Tokens per Sec:     5084, Lr: 0.000035
2022-09-15 20:57:38,962 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 21:13:24,997 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 21:13:24,999 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.85, loss:   2.54, ppl:  12.68, acc:   0.51, generation: 939.0432[sec], evaluation: 6.6499[sec]
2022-09-15 21:13:25,002 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 21:13:26,675 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/229000.ckpt
2022-09-15 21:13:26,718 - INFO - joeynmt.training - Example #0
2022-09-15 21:13:26,730 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 21:13:26,730 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 21:13:26,730 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 21:13:26,730 - INFO - joeynmt.training - Example #1
2022-09-15 21:13:26,740 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 21:13:26,740 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 21:13:26,740 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन प्ले करें
2022-09-15 21:13:26,740 - INFO - joeynmt.training - Example #2
2022-09-15 21:13:26,750 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 21:13:26,750 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 21:13:26,751 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 21:13:26,751 - INFO - joeynmt.training - Example #3
2022-09-15 21:13:26,761 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 21:13:26,761 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 21:13:26,761 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 21:13:49,712 - INFO - joeynmt.training - Epoch   1, Step:   230100, Batch Loss:     2.911563, Batch Acc: 0.003366, Tokens per Sec:     4701, Lr: 0.000035
2022-09-15 21:14:12,422 - INFO - joeynmt.training - Epoch   1, Step:   230200, Batch Loss:     2.700085, Batch Acc: 0.004291, Tokens per Sec:     5018, Lr: 0.000035
2022-09-15 21:14:35,023 - INFO - joeynmt.training - Epoch   1, Step:   230300, Batch Loss:     2.741578, Batch Acc: 0.003566, Tokens per Sec:     5025, Lr: 0.000035
2022-09-15 21:14:57,645 - INFO - joeynmt.training - Epoch   1, Step:   230400, Batch Loss:     2.961499, Batch Acc: 0.003855, Tokens per Sec:     4966, Lr: 0.000035
2022-09-15 21:15:20,353 - INFO - joeynmt.training - Epoch   1, Step:   230500, Batch Loss:     2.698880, Batch Acc: 0.003900, Tokens per Sec:     4969, Lr: 0.000035
2022-09-15 21:15:43,025 - INFO - joeynmt.training - Epoch   1, Step:   230600, Batch Loss:     2.740025, Batch Acc: 0.003924, Tokens per Sec:     5059, Lr: 0.000035
2022-09-15 21:16:05,702 - INFO - joeynmt.training - Epoch   1, Step:   230700, Batch Loss:     2.638030, Batch Acc: 0.004203, Tokens per Sec:     5004, Lr: 0.000035
2022-09-15 21:16:28,515 - INFO - joeynmt.training - Epoch   1, Step:   230800, Batch Loss:     2.411545, Batch Acc: 0.003882, Tokens per Sec:     5025, Lr: 0.000035
2022-09-15 21:16:51,332 - INFO - joeynmt.training - Epoch   1, Step:   230900, Batch Loss:     2.809927, Batch Acc: 0.003287, Tokens per Sec:     4974, Lr: 0.000035
2022-09-15 21:17:14,091 - INFO - joeynmt.training - Epoch   1, Step:   231000, Batch Loss:     2.785292, Batch Acc: 0.004497, Tokens per Sec:     5013, Lr: 0.000035
2022-09-15 21:17:14,092 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 21:33:02,820 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 21:33:02,821 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.82, loss:   2.54, ppl:  12.69, acc:   0.51, generation: 941.1041[sec], evaluation: 7.2778[sec]
2022-09-15 21:33:04,704 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/228000.ckpt
2022-09-15 21:33:04,745 - INFO - joeynmt.training - Example #0
2022-09-15 21:33:04,757 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 21:33:04,757 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 21:33:04,757 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 21:33:04,757 - INFO - joeynmt.training - Example #1
2022-09-15 21:33:04,768 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 21:33:04,768 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 21:33:04,768 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन प्ले करें
2022-09-15 21:33:04,768 - INFO - joeynmt.training - Example #2
2022-09-15 21:33:04,779 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 21:33:04,779 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 21:33:04,779 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 21:33:04,779 - INFO - joeynmt.training - Example #3
2022-09-15 21:33:04,789 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 21:33:04,789 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 21:33:04,789 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 21:33:27,605 - INFO - joeynmt.training - Epoch   1, Step:   231100, Batch Loss:     2.508094, Batch Acc: 0.004485, Tokens per Sec:     4643, Lr: 0.000035
2022-09-15 21:33:50,196 - INFO - joeynmt.training - Epoch   1, Step:   231200, Batch Loss:     2.395399, Batch Acc: 0.005592, Tokens per Sec:     5050, Lr: 0.000035
2022-09-15 21:34:13,033 - INFO - joeynmt.training - Epoch   1, Step:   231300, Batch Loss:     2.774572, Batch Acc: 0.003838, Tokens per Sec:     4985, Lr: 0.000035
2022-09-15 21:34:35,676 - INFO - joeynmt.training - Epoch   1, Step:   231400, Batch Loss:     2.694928, Batch Acc: 0.003970, Tokens per Sec:     5106, Lr: 0.000035
2022-09-15 21:34:58,430 - INFO - joeynmt.training - Epoch   1, Step:   231500, Batch Loss:     2.872280, Batch Acc: 0.003931, Tokens per Sec:     5120, Lr: 0.000035
2022-09-15 21:35:21,142 - INFO - joeynmt.training - Epoch   1, Step:   231600, Batch Loss:     2.607928, Batch Acc: 0.004327, Tokens per Sec:     4976, Lr: 0.000035
2022-09-15 21:35:43,836 - INFO - joeynmt.training - Epoch   1, Step:   231700, Batch Loss:     2.644164, Batch Acc: 0.004787, Tokens per Sec:     5063, Lr: 0.000035
2022-09-15 21:36:06,516 - INFO - joeynmt.training - Epoch   1, Step:   231800, Batch Loss:     2.511132, Batch Acc: 0.005069, Tokens per Sec:     5054, Lr: 0.000035
2022-09-15 21:36:29,301 - INFO - joeynmt.training - Epoch   1, Step:   231900, Batch Loss:     2.946932, Batch Acc: 0.003796, Tokens per Sec:     4983, Lr: 0.000035
2022-09-15 21:36:52,262 - INFO - joeynmt.training - Epoch   1, Step:   232000, Batch Loss:     2.500422, Batch Acc: 0.004281, Tokens per Sec:     5015, Lr: 0.000035
2022-09-15 21:36:52,262 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 21:52:35,354 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 21:52:35,355 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.80, loss:   2.54, ppl:  12.65, acc:   0.51, generation: 935.9537[sec], evaluation: 6.7922[sec]
2022-09-15 21:52:35,359 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 21:52:36,952 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/226000.ckpt
2022-09-15 21:52:36,998 - INFO - joeynmt.training - Example #0
2022-09-15 21:52:37,010 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 21:52:37,010 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 21:52:37,010 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 21:52:37,010 - INFO - joeynmt.training - Example #1
2022-09-15 21:52:37,021 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 21:52:37,021 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 21:52:37,021 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन प्ले करें
2022-09-15 21:52:37,021 - INFO - joeynmt.training - Example #2
2022-09-15 21:52:37,032 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 21:52:37,032 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 21:52:37,032 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 21:52:37,032 - INFO - joeynmt.training - Example #3
2022-09-15 21:52:37,042 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 21:52:37,042 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 21:52:37,042 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 21:52:51,002 - INFO - joeynmt.training - Epoch   1: total training loss 39148.67
2022-09-15 21:52:51,002 - INFO - joeynmt.training - Training ended after   1 epochs.
2022-09-15 21:52:51,002 - INFO - joeynmt.training - Best validation result (greedy) at step   232000:  12.65 ppl.
2022-09-15 21:52:51,009 - INFO - joeynmt.training - Loading from ckpt file: /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt
2022-09-15 21:52:51,023 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 21:52:51,023 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 21:52:51,236 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 21:52:51,239 - INFO - joeynmt.model - Total params: 19302144
2022-09-15 21:52:51,372 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 21:52:51,414 - INFO - joeynmt.prediction - Decoding on dev set...
2022-09-15 21:52:51,414 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:12:37,731 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 22:12:37,732 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  23.17, generation: 1179.2723[sec], evaluation: 6.7091[sec]
2022-09-15 22:12:37,827 - INFO - joeynmt.prediction - Translations saved to: /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/00232000.hyps.dev.
2022-09-15 22:12:37,827 - INFO - joeynmt.prediction - Decoding on test set...
2022-09-15 22:12:37,827 - INFO - joeynmt.prediction - Predicting 40858 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:31:41,195 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 22:31:41,196 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  22.57, generation: 1135.8544[sec], evaluation: 7.1793[sec]
2022-09-15 22:31:41,296 - INFO - joeynmt.prediction - Translations saved to: /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/00232000.hyps.test.
2022-09-15 22:31:41,304 - INFO - joeynmt.training - ACTIVE LEARNING MODEL END - MARGIN 1
2022-09-15 22:31:41,305 - INFO - joeynmt.training - ACTIVE LEARNING MODEL START - MARGIN 2
2022-09-15 22:31:41,306 - INFO - joeynmt.training - Random Indices picked: [121958 146867 131932 365838 259178 119879 110268 207892  54886 137337] length: 26153
2022-09-15 22:31:41,306 - INFO - joeynmt.training - Processing Predictions on Batch 0/103
2022-09-15 22:31:43,736 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:31:43,736 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:31:43,949 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:31:44,090 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:31:44,181 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:31:44,181 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:31:44,184 - INFO - joeynmt.prediction - Predicting 13 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:31:44,607 - INFO - joeynmt.prediction - Generation took 0.4220[sec]. (No references given)
2022-09-15 22:31:44,615 - INFO - joeynmt.training - Processing Predictions on Batch 1/103
2022-09-15 22:31:47,031 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:31:47,031 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:31:47,245 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:31:47,377 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:31:47,463 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:31:47,463 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:31:47,465 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:31:48,198 - INFO - joeynmt.prediction - Generation took 0.7313[sec]. (No references given)
2022-09-15 22:31:48,205 - INFO - joeynmt.training - Processing Predictions on Batch 2/103
2022-09-15 22:31:50,630 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:31:50,630 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:31:50,845 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:31:50,976 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:31:51,063 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:31:51,063 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:31:51,064 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:31:52,104 - INFO - joeynmt.prediction - Generation took 1.0387[sec]. (No references given)
2022-09-15 22:31:52,112 - INFO - joeynmt.training - Processing Predictions on Batch 3/103
2022-09-15 22:31:54,516 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:31:54,517 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:31:54,734 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:31:54,867 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:31:54,953 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:31:54,953 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:31:54,955 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:31:55,319 - INFO - joeynmt.prediction - Generation took 0.3625[sec]. (No references given)
2022-09-15 22:31:55,326 - INFO - joeynmt.training - Processing Predictions on Batch 4/103
2022-09-15 22:31:57,730 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:31:57,730 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:31:57,945 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:31:58,077 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:31:58,163 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:31:58,164 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:31:58,166 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:31:58,457 - INFO - joeynmt.prediction - Generation took 0.2896[sec]. (No references given)
2022-09-15 22:31:58,464 - INFO - joeynmt.training - Processing Predictions on Batch 5/103
2022-09-15 22:32:00,857 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:32:00,857 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:32:01,070 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:32:01,203 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:32:01,289 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:01,289 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:01,290 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:32:02,311 - INFO - joeynmt.prediction - Generation took 1.0193[sec]. (No references given)
2022-09-15 22:32:02,319 - INFO - joeynmt.training - Processing Predictions on Batch 6/103
2022-09-15 22:32:04,719 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:32:04,719 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:32:04,932 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:32:05,064 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:32:05,150 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:05,150 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:05,153 - INFO - joeynmt.prediction - Predicting 12 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:32:05,764 - INFO - joeynmt.prediction - Generation took 0.6095[sec]. (No references given)
2022-09-15 22:32:05,771 - INFO - joeynmt.training - Processing Predictions on Batch 7/103
2022-09-15 22:32:08,156 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:32:08,156 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:32:08,369 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:32:08,501 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:32:08,588 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:08,588 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:08,589 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:32:08,926 - INFO - joeynmt.prediction - Generation took 0.3354[sec]. (No references given)
2022-09-15 22:32:08,933 - INFO - joeynmt.training - Processing Predictions on Batch 8/103
2022-09-15 22:32:11,356 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:32:11,356 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:32:11,569 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:32:11,700 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:32:11,788 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:11,788 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:11,789 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:32:12,321 - INFO - joeynmt.prediction - Generation took 0.5308[sec]. (No references given)
2022-09-15 22:32:12,329 - INFO - joeynmt.training - Processing Predictions on Batch 9/103
2022-09-15 22:32:14,717 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:32:14,717 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:32:14,930 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:32:15,060 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:32:15,149 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:15,149 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:15,150 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:32:15,492 - INFO - joeynmt.prediction - Generation took 0.3404[sec]. (No references given)
2022-09-15 22:32:15,499 - INFO - joeynmt.training - Processing Predictions on Batch 10/103
2022-09-15 22:32:17,909 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:32:17,909 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:32:18,122 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:32:18,252 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:32:18,341 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:18,341 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:18,342 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:32:18,621 - INFO - joeynmt.prediction - Generation took 0.2783[sec]. (No references given)
2022-09-15 22:32:18,629 - INFO - joeynmt.training - Processing Predictions on Batch 11/103
2022-09-15 22:32:21,001 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:32:21,002 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:32:21,215 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:32:21,345 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:32:21,433 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:21,433 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:21,434 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:32:21,793 - INFO - joeynmt.prediction - Generation took 0.3577[sec]. (No references given)
2022-09-15 22:32:21,801 - INFO - joeynmt.training - Processing Predictions on Batch 12/103
2022-09-15 22:32:24,180 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:32:24,180 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:32:24,393 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:32:24,523 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:32:24,611 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:24,611 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:24,613 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:32:24,994 - INFO - joeynmt.prediction - Generation took 0.3795[sec]. (No references given)
2022-09-15 22:32:25,001 - INFO - joeynmt.training - Processing Predictions on Batch 13/103
2022-09-15 22:32:27,362 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:32:27,362 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:32:27,575 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:32:27,707 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:32:27,795 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:27,795 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:27,797 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:32:28,235 - INFO - joeynmt.prediction - Generation took 0.4366[sec]. (No references given)
2022-09-15 22:32:28,242 - INFO - joeynmt.training - Processing Predictions on Batch 14/103
2022-09-15 22:32:30,617 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:32:30,617 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:32:30,831 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:32:30,961 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:32:31,049 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:31,049 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:31,049 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:32:31,444 - INFO - joeynmt.prediction - Generation took 0.3938[sec]. (No references given)
2022-09-15 22:32:31,452 - INFO - joeynmt.training - Processing Predictions on Batch 15/103
2022-09-15 22:32:33,814 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:32:33,814 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:32:34,028 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:32:34,161 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:32:34,249 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:34,249 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:34,250 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:32:34,646 - INFO - joeynmt.prediction - Generation took 0.3948[sec]. (No references given)
2022-09-15 22:32:34,654 - INFO - joeynmt.training - Processing Predictions on Batch 16/103
2022-09-15 22:32:37,054 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:32:37,054 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:32:37,268 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:32:37,401 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:32:37,487 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:37,488 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:37,490 - INFO - joeynmt.prediction - Predicting 11 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:32:38,200 - INFO - joeynmt.prediction - Generation took 0.7084[sec]. (No references given)
2022-09-15 22:32:38,207 - INFO - joeynmt.training - Processing Predictions on Batch 17/103
2022-09-15 22:32:40,577 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:32:40,577 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:32:40,791 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:32:41,390 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:32:41,478 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:41,478 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:41,479 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:32:41,821 - INFO - joeynmt.prediction - Generation took 0.3402[sec]. (No references given)
2022-09-15 22:32:41,829 - INFO - joeynmt.training - Processing Predictions on Batch 18/103
2022-09-15 22:32:44,206 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:32:44,206 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:32:44,419 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:32:44,552 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:32:44,641 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:44,641 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:44,643 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:32:45,080 - INFO - joeynmt.prediction - Generation took 0.4357[sec]. (No references given)
2022-09-15 22:32:45,087 - INFO - joeynmt.training - Processing Predictions on Batch 19/103
2022-09-15 22:32:47,452 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:32:47,452 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:32:47,666 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:32:47,799 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:32:47,886 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:47,886 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:47,887 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:32:48,177 - INFO - joeynmt.prediction - Generation took 0.2885[sec]. (No references given)
2022-09-15 22:32:48,185 - INFO - joeynmt.training - Processing Predictions on Batch 20/103
2022-09-15 22:32:50,570 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:32:50,570 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:32:50,787 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:32:50,921 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:32:51,008 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:51,009 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:51,010 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:32:51,349 - INFO - joeynmt.prediction - Generation took 0.3380[sec]. (No references given)
2022-09-15 22:32:51,357 - INFO - joeynmt.training - Processing Predictions on Batch 21/103
2022-09-15 22:32:53,744 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:32:53,744 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:32:53,957 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:32:54,092 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:32:54,179 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:54,179 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:54,180 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:32:55,206 - INFO - joeynmt.prediction - Generation took 1.0242[sec]. (No references given)
2022-09-15 22:32:55,214 - INFO - joeynmt.training - Processing Predictions on Batch 22/103
2022-09-15 22:32:57,611 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:32:57,611 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:32:57,824 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:32:57,957 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:32:58,044 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:58,044 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:32:58,045 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:32:58,343 - INFO - joeynmt.prediction - Generation took 0.2965[sec]. (No references given)
2022-09-15 22:32:58,350 - INFO - joeynmt.training - Processing Predictions on Batch 23/103
2022-09-15 22:33:00,724 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:33:00,724 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:33:00,938 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:33:01,070 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:33:01,159 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:01,159 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:01,160 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:33:01,471 - INFO - joeynmt.prediction - Generation took 0.3100[sec]. (No references given)
2022-09-15 22:33:01,479 - INFO - joeynmt.training - Processing Predictions on Batch 24/103
2022-09-15 22:33:03,858 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:33:03,858 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:33:04,072 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:33:04,202 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:33:04,292 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:04,292 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:04,292 - INFO - joeynmt.prediction - Predicting 3 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:33:04,712 - INFO - joeynmt.prediction - Generation took 0.4182[sec]. (No references given)
2022-09-15 22:33:04,719 - INFO - joeynmt.training - Processing Predictions on Batch 25/103
2022-09-15 22:33:07,074 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:33:07,074 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:33:07,287 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:33:07,419 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:33:07,509 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:07,510 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:07,511 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:33:08,118 - INFO - joeynmt.prediction - Generation took 0.6062[sec]. (No references given)
2022-09-15 22:33:08,126 - INFO - joeynmt.training - Processing Predictions on Batch 26/103
2022-09-15 22:33:10,510 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:33:10,510 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:33:10,724 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:33:10,855 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:33:10,943 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:10,943 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:10,945 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:33:11,454 - INFO - joeynmt.prediction - Generation took 0.5081[sec]. (No references given)
2022-09-15 22:33:11,462 - INFO - joeynmt.training - Processing Predictions on Batch 27/103
2022-09-15 22:33:13,826 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:33:13,826 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:33:14,039 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:33:14,170 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:33:14,258 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:14,259 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:14,261 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:33:15,430 - INFO - joeynmt.prediction - Generation took 1.1676[sec]. (No references given)
2022-09-15 22:33:15,437 - INFO - joeynmt.training - Processing Predictions on Batch 28/103
2022-09-15 22:33:17,816 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:33:17,816 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:33:18,033 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:33:18,167 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:33:18,254 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:18,255 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:18,256 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:33:18,649 - INFO - joeynmt.prediction - Generation took 0.3927[sec]. (No references given)
2022-09-15 22:33:18,657 - INFO - joeynmt.training - Processing Predictions on Batch 29/103
2022-09-15 22:33:21,024 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:33:21,024 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:33:21,237 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:33:21,368 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:33:21,456 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:21,456 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:21,460 - INFO - joeynmt.prediction - Predicting 14 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:33:22,660 - INFO - joeynmt.prediction - Generation took 1.1993[sec]. (No references given)
2022-09-15 22:33:22,668 - INFO - joeynmt.training - Processing Predictions on Batch 30/103
2022-09-15 22:33:25,044 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:33:25,044 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:33:25,257 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:33:25,388 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:33:25,476 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:25,476 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:25,477 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:33:25,786 - INFO - joeynmt.prediction - Generation took 0.3075[sec]. (No references given)
2022-09-15 22:33:25,793 - INFO - joeynmt.training - Processing Predictions on Batch 31/103
2022-09-15 22:33:28,163 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:33:28,163 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:33:28,376 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:33:28,509 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:33:28,596 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:28,596 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:28,599 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:33:29,167 - INFO - joeynmt.prediction - Generation took 0.5671[sec]. (No references given)
2022-09-15 22:33:29,175 - INFO - joeynmt.training - Processing Predictions on Batch 32/103
2022-09-15 22:33:31,557 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:33:31,557 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:33:31,771 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:33:31,904 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:33:31,991 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:31,991 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:31,994 - INFO - joeynmt.prediction - Predicting 11 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:33:32,834 - INFO - joeynmt.prediction - Generation took 0.8387[sec]. (No references given)
2022-09-15 22:33:32,842 - INFO - joeynmt.training - Processing Predictions on Batch 33/103
2022-09-15 22:33:35,201 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:33:35,201 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:33:35,417 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:33:35,550 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:33:35,637 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:35,637 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:35,638 - INFO - joeynmt.prediction - Predicting 3 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:33:35,911 - INFO - joeynmt.prediction - Generation took 0.2717[sec]. (No references given)
2022-09-15 22:33:35,919 - INFO - joeynmt.training - Processing Predictions on Batch 34/103
2022-09-15 22:33:38,295 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:33:38,295 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:33:38,512 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:33:38,645 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:33:38,732 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:38,733 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:38,734 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:33:39,195 - INFO - joeynmt.prediction - Generation took 0.4601[sec]. (No references given)
2022-09-15 22:33:39,203 - INFO - joeynmt.training - Processing Predictions on Batch 35/103
2022-09-15 22:33:41,576 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:33:41,576 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:33:41,789 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:33:41,923 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:33:42,013 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:42,013 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:42,014 - INFO - joeynmt.prediction - Predicting 3 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:33:42,426 - INFO - joeynmt.prediction - Generation took 0.4110[sec]. (No references given)
2022-09-15 22:33:42,434 - INFO - joeynmt.training - Processing Predictions on Batch 36/103
2022-09-15 22:33:44,813 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:33:44,813 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:33:45,026 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:33:45,158 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:33:45,248 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:45,248 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:45,251 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:33:45,807 - INFO - joeynmt.prediction - Generation took 0.5540[sec]. (No references given)
2022-09-15 22:33:45,814 - INFO - joeynmt.training - Processing Predictions on Batch 37/103
2022-09-15 22:33:48,185 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:33:48,185 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:33:48,399 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:33:48,529 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:33:48,618 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:48,621 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:48,622 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:33:49,131 - INFO - joeynmt.prediction - Generation took 0.5026[sec]. (No references given)
2022-09-15 22:33:49,138 - INFO - joeynmt.training - Processing Predictions on Batch 38/103
2022-09-15 22:33:51,514 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:33:51,514 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:33:51,730 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:33:51,861 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:33:51,949 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:51,950 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:51,951 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:33:52,511 - INFO - joeynmt.prediction - Generation took 0.5579[sec]. (No references given)
2022-09-15 22:33:52,518 - INFO - joeynmt.training - Processing Predictions on Batch 39/103
2022-09-15 22:33:54,889 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:33:54,889 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:33:55,104 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:33:55,235 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:33:55,323 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:55,323 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:55,326 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:33:55,729 - INFO - joeynmt.prediction - Generation took 0.4021[sec]. (No references given)
2022-09-15 22:33:55,737 - INFO - joeynmt.training - Processing Predictions on Batch 40/103
2022-09-15 22:33:58,110 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:33:58,111 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:33:58,324 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:33:58,456 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:33:58,543 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:58,543 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:33:58,545 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:33:58,939 - INFO - joeynmt.prediction - Generation took 0.3926[sec]. (No references given)
2022-09-15 22:33:58,946 - INFO - joeynmt.training - Processing Predictions on Batch 41/103
2022-09-15 22:34:01,324 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:34:01,324 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:34:01,538 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:34:01,674 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:34:01,761 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:01,761 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:01,763 - INFO - joeynmt.prediction - Predicting 13 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:34:02,761 - INFO - joeynmt.prediction - Generation took 0.9966[sec]. (No references given)
2022-09-15 22:34:02,769 - INFO - joeynmt.training - Processing Predictions on Batch 42/103
2022-09-15 22:34:05,152 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:34:05,152 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:34:05,365 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:34:05,497 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:34:05,584 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:05,584 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:05,585 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:34:06,086 - INFO - joeynmt.prediction - Generation took 0.4998[sec]. (No references given)
2022-09-15 22:34:06,094 - INFO - joeynmt.training - Processing Predictions on Batch 43/103
2022-09-15 22:34:08,450 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:34:08,450 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:34:08,667 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:34:08,799 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:34:08,886 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:08,886 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:08,888 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:34:09,984 - INFO - joeynmt.prediction - Generation took 1.0946[sec]. (No references given)
2022-09-15 22:34:09,992 - INFO - joeynmt.training - Processing Predictions on Batch 44/103
2022-09-15 22:34:12,387 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:34:12,387 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:34:12,605 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:34:12,737 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:34:12,827 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:12,828 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:12,831 - INFO - joeynmt.prediction - Predicting 12 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:34:13,441 - INFO - joeynmt.prediction - Generation took 0.6086[sec]. (No references given)
2022-09-15 22:34:13,448 - INFO - joeynmt.training - Processing Predictions on Batch 45/103
2022-09-15 22:34:15,812 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:34:15,812 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:34:16,025 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:34:16,157 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:34:16,244 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:16,244 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:16,246 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:34:16,727 - INFO - joeynmt.prediction - Generation took 0.4794[sec]. (No references given)
2022-09-15 22:34:16,735 - INFO - joeynmt.training - Processing Predictions on Batch 46/103
2022-09-15 22:34:19,115 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:34:19,115 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:34:19,328 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:34:19,461 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:34:19,547 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:19,548 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:19,548 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:34:19,743 - INFO - joeynmt.prediction - Generation took 0.1936[sec]. (No references given)
2022-09-15 22:34:19,750 - INFO - joeynmt.training - Processing Predictions on Batch 47/103
2022-09-15 22:34:22,122 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:34:22,122 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:34:22,336 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:34:22,466 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:34:22,554 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:22,555 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:22,555 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:34:23,020 - INFO - joeynmt.prediction - Generation took 0.4636[sec]. (No references given)
2022-09-15 22:34:23,028 - INFO - joeynmt.training - Processing Predictions on Batch 48/103
2022-09-15 22:34:25,390 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:34:25,390 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:34:25,604 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:34:25,735 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:34:25,823 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:25,823 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:25,824 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:34:26,273 - INFO - joeynmt.prediction - Generation took 0.4484[sec]. (No references given)
2022-09-15 22:34:26,281 - INFO - joeynmt.training - Processing Predictions on Batch 49/103
2022-09-15 22:34:28,644 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:34:28,644 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:34:28,858 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:34:28,990 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:34:29,078 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:29,078 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:29,080 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:34:29,514 - INFO - joeynmt.prediction - Generation took 0.4328[sec]. (No references given)
2022-09-15 22:34:29,521 - INFO - joeynmt.training - Processing Predictions on Batch 50/103
2022-09-15 22:34:31,910 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:34:31,910 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:34:32,125 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:34:32,256 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:34:32,343 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:32,343 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:32,344 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:34:32,808 - INFO - joeynmt.prediction - Generation took 0.4622[sec]. (No references given)
2022-09-15 22:34:32,815 - INFO - joeynmt.training - Processing Predictions on Batch 51/103
2022-09-15 22:34:35,182 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:34:35,182 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:34:35,396 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:34:35,527 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:34:35,614 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:35,614 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:35,617 - INFO - joeynmt.prediction - Predicting 11 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:34:36,105 - INFO - joeynmt.prediction - Generation took 0.4864[sec]. (No references given)
2022-09-15 22:34:36,112 - INFO - joeynmt.training - Processing Predictions on Batch 52/103
2022-09-15 22:34:38,499 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:34:38,499 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:34:38,715 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:34:38,846 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:34:38,933 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:38,933 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:38,935 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:34:39,953 - INFO - joeynmt.prediction - Generation took 1.0168[sec]. (No references given)
2022-09-15 22:34:39,961 - INFO - joeynmt.training - Processing Predictions on Batch 53/103
2022-09-15 22:34:42,333 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:34:42,333 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:34:42,549 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:34:42,683 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:34:42,770 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:42,770 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:42,774 - INFO - joeynmt.prediction - Predicting 14 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:34:43,460 - INFO - joeynmt.prediction - Generation took 0.6845[sec]. (No references given)
2022-09-15 22:34:43,468 - INFO - joeynmt.training - Processing Predictions on Batch 54/103
2022-09-15 22:34:45,845 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:34:45,846 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:34:46,059 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:34:46,191 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:34:46,279 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:46,280 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:46,281 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:34:46,699 - INFO - joeynmt.prediction - Generation took 0.4164[sec]. (No references given)
2022-09-15 22:34:46,707 - INFO - joeynmt.training - Processing Predictions on Batch 55/103
2022-09-15 22:34:49,071 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:34:49,071 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:34:49,287 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:34:49,887 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:34:49,974 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:49,974 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:49,974 - INFO - joeynmt.prediction - Predicting 2 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:34:50,088 - INFO - joeynmt.prediction - Generation took 0.1126[sec]. (No references given)
2022-09-15 22:34:50,095 - INFO - joeynmt.training - Processing Predictions on Batch 56/103
2022-09-15 22:34:52,478 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:34:52,478 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:34:52,693 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:34:52,824 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:34:52,910 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:52,910 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:52,912 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:34:53,246 - INFO - joeynmt.prediction - Generation took 0.3266[sec]. (No references given)
2022-09-15 22:34:53,253 - INFO - joeynmt.training - Processing Predictions on Batch 57/103
2022-09-15 22:34:55,626 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:34:55,627 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:34:55,840 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:34:55,972 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:34:56,058 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:56,058 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:56,061 - INFO - joeynmt.prediction - Predicting 13 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:34:56,745 - INFO - joeynmt.prediction - Generation took 0.6821[sec]. (No references given)
2022-09-15 22:34:56,752 - INFO - joeynmt.training - Processing Predictions on Batch 58/103
2022-09-15 22:34:59,164 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:34:59,164 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:34:59,378 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:34:59,511 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:34:59,597 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:59,597 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:34:59,600 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:35:00,142 - INFO - joeynmt.prediction - Generation took 0.5407[sec]. (No references given)
2022-09-15 22:35:00,149 - INFO - joeynmt.training - Processing Predictions on Batch 59/103
2022-09-15 22:35:02,524 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:35:02,525 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:35:02,739 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:35:02,874 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:35:02,960 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:02,966 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:02,968 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:35:03,724 - INFO - joeynmt.prediction - Generation took 0.7497[sec]. (No references given)
2022-09-15 22:35:03,731 - INFO - joeynmt.training - Processing Predictions on Batch 60/103
2022-09-15 22:35:06,094 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:35:06,094 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:35:06,313 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:35:06,446 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:35:06,531 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:06,531 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:06,533 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:35:07,052 - INFO - joeynmt.prediction - Generation took 0.5174[sec]. (No references given)
2022-09-15 22:35:07,059 - INFO - joeynmt.training - Processing Predictions on Batch 61/103
2022-09-15 22:35:09,418 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:35:09,418 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:35:09,633 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:35:09,765 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:35:09,850 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:09,850 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:09,852 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:35:10,313 - INFO - joeynmt.prediction - Generation took 0.4604[sec]. (No references given)
2022-09-15 22:35:10,321 - INFO - joeynmt.training - Processing Predictions on Batch 62/103
2022-09-15 22:35:12,691 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:35:12,691 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:35:12,905 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:35:13,037 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:35:13,123 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:13,123 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:13,125 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:35:13,618 - INFO - joeynmt.prediction - Generation took 0.4921[sec]. (No references given)
2022-09-15 22:35:13,626 - INFO - joeynmt.training - Processing Predictions on Batch 63/103
2022-09-15 22:35:15,975 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:35:15,975 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:35:16,190 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:35:16,322 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:35:16,409 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:16,409 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:16,410 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:35:16,825 - INFO - joeynmt.prediction - Generation took 0.4135[sec]. (No references given)
2022-09-15 22:35:16,832 - INFO - joeynmt.training - Processing Predictions on Batch 64/103
2022-09-15 22:35:19,204 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:35:19,204 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:35:19,419 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:35:19,549 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:35:19,637 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:19,637 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:19,638 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:35:20,192 - INFO - joeynmt.prediction - Generation took 0.5526[sec]. (No references given)
2022-09-15 22:35:20,199 - INFO - joeynmt.training - Processing Predictions on Batch 65/103
2022-09-15 22:35:22,553 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:35:22,554 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:35:22,768 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:35:22,899 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:35:22,986 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:22,986 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:22,988 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:35:23,313 - INFO - joeynmt.prediction - Generation took 0.3239[sec]. (No references given)
2022-09-15 22:35:23,321 - INFO - joeynmt.training - Processing Predictions on Batch 66/103
2022-09-15 22:35:25,679 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:35:25,679 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:35:25,893 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:35:26,024 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:35:26,110 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:26,110 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:26,112 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:35:26,643 - INFO - joeynmt.prediction - Generation took 0.5298[sec]. (No references given)
2022-09-15 22:35:26,651 - INFO - joeynmt.training - Processing Predictions on Batch 67/103
2022-09-15 22:35:29,011 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:35:29,011 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:35:29,225 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:35:29,357 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:35:29,442 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:29,443 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:29,445 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:35:30,020 - INFO - joeynmt.prediction - Generation took 0.5740[sec]. (No references given)
2022-09-15 22:35:30,027 - INFO - joeynmt.training - Processing Predictions on Batch 68/103
2022-09-15 22:35:32,398 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:35:32,398 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:35:32,614 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:35:32,744 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:35:32,830 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:32,830 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:32,831 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:35:33,338 - INFO - joeynmt.prediction - Generation took 0.5058[sec]. (No references given)
2022-09-15 22:35:33,346 - INFO - joeynmt.training - Processing Predictions on Batch 69/103
2022-09-15 22:35:35,719 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:35:35,719 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:35:35,933 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:35:36,064 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:35:36,149 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:36,150 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:36,150 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:35:37,208 - INFO - joeynmt.prediction - Generation took 1.0570[sec]. (No references given)
2022-09-15 22:35:37,216 - INFO - joeynmt.training - Processing Predictions on Batch 70/103
2022-09-15 22:35:39,581 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:35:39,581 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:35:39,794 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:35:39,927 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:35:40,012 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:40,012 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:40,015 - INFO - joeynmt.prediction - Predicting 17 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:35:40,715 - INFO - joeynmt.prediction - Generation took 0.6988[sec]. (No references given)
2022-09-15 22:35:40,722 - INFO - joeynmt.training - Processing Predictions on Batch 71/103
2022-09-15 22:35:43,083 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:35:43,083 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:35:43,303 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:35:43,437 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:35:43,523 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:43,523 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:43,525 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:35:43,935 - INFO - joeynmt.prediction - Generation took 0.4087[sec]. (No references given)
2022-09-15 22:35:43,942 - INFO - joeynmt.training - Processing Predictions on Batch 72/103
2022-09-15 22:35:46,306 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:35:46,306 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:35:46,520 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:35:46,652 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:35:46,738 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:46,738 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:46,740 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:35:47,807 - INFO - joeynmt.prediction - Generation took 1.0659[sec]. (No references given)
2022-09-15 22:35:47,815 - INFO - joeynmt.training - Processing Predictions on Batch 73/103
2022-09-15 22:35:50,158 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:35:50,158 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:35:50,372 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:35:50,505 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:35:50,589 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:50,589 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:50,590 - INFO - joeynmt.prediction - Predicting 3 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:35:50,980 - INFO - joeynmt.prediction - Generation took 0.3884[sec]. (No references given)
2022-09-15 22:35:50,987 - INFO - joeynmt.training - Processing Predictions on Batch 74/103
2022-09-15 22:35:53,351 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:35:53,351 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:35:53,565 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:35:53,699 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:35:53,783 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:53,783 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:53,784 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:35:54,157 - INFO - joeynmt.prediction - Generation took 0.3720[sec]. (No references given)
2022-09-15 22:35:54,164 - INFO - joeynmt.training - Processing Predictions on Batch 75/103
2022-09-15 22:35:56,502 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:35:56,502 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:35:56,718 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:35:56,849 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:35:56,935 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:56,935 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:35:56,937 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:35:57,509 - INFO - joeynmt.prediction - Generation took 0.5710[sec]. (No references given)
2022-09-15 22:35:57,517 - INFO - joeynmt.training - Processing Predictions on Batch 76/103
2022-09-15 22:35:59,871 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:35:59,871 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:36:00,085 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:36:00,215 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:36:00,301 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:00,301 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:00,303 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:36:00,822 - INFO - joeynmt.prediction - Generation took 0.5178[sec]. (No references given)
2022-09-15 22:36:00,830 - INFO - joeynmt.training - Processing Predictions on Batch 77/103
2022-09-15 22:36:03,173 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:36:03,174 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:36:03,387 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:36:03,518 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:36:03,605 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:03,605 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:03,606 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:36:04,011 - INFO - joeynmt.prediction - Generation took 0.4042[sec]. (No references given)
2022-09-15 22:36:04,019 - INFO - joeynmt.training - Processing Predictions on Batch 78/103
2022-09-15 22:36:06,388 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:36:06,388 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:36:06,601 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:36:06,733 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:36:06,818 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:06,818 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:06,819 - INFO - joeynmt.prediction - Predicting 3 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:36:07,013 - INFO - joeynmt.prediction - Generation took 0.1931[sec]. (No references given)
2022-09-15 22:36:07,021 - INFO - joeynmt.training - Processing Predictions on Batch 79/103
2022-09-15 22:36:09,372 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:36:09,372 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:36:09,587 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:36:09,718 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:36:09,803 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:09,804 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:09,805 - INFO - joeynmt.prediction - Predicting 3 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:36:10,265 - INFO - joeynmt.prediction - Generation took 0.4588[sec]. (No references given)
2022-09-15 22:36:10,272 - INFO - joeynmt.training - Processing Predictions on Batch 80/103
2022-09-15 22:36:12,658 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:36:12,658 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:36:12,874 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:36:13,004 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:36:13,089 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:13,089 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:13,091 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:36:13,652 - INFO - joeynmt.prediction - Generation took 0.5596[sec]. (No references given)
2022-09-15 22:36:13,659 - INFO - joeynmt.training - Processing Predictions on Batch 81/103
2022-09-15 22:36:16,001 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:36:16,002 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:36:16,221 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:36:16,352 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:36:16,436 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:16,436 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:16,438 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:36:16,858 - INFO - joeynmt.prediction - Generation took 0.4196[sec]. (No references given)
2022-09-15 22:36:16,866 - INFO - joeynmt.training - Processing Predictions on Batch 82/103
2022-09-15 22:36:19,224 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:36:19,224 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:36:19,437 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:36:19,569 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:36:19,653 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:19,653 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:19,656 - INFO - joeynmt.prediction - Predicting 13 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:36:20,870 - INFO - joeynmt.prediction - Generation took 1.2124[sec]. (No references given)
2022-09-15 22:36:20,877 - INFO - joeynmt.training - Processing Predictions on Batch 83/103
2022-09-15 22:36:23,216 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:36:23,217 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:36:23,432 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:36:23,564 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:36:23,649 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:23,649 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:23,651 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:36:24,122 - INFO - joeynmt.prediction - Generation took 0.4703[sec]. (No references given)
2022-09-15 22:36:24,130 - INFO - joeynmt.training - Processing Predictions on Batch 84/103
2022-09-15 22:36:26,485 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:36:26,485 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:36:26,699 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:36:26,830 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:36:26,914 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:26,915 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:26,917 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:36:27,387 - INFO - joeynmt.prediction - Generation took 0.4691[sec]. (No references given)
2022-09-15 22:36:27,395 - INFO - joeynmt.training - Processing Predictions on Batch 85/103
2022-09-15 22:36:29,743 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:36:29,743 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:36:29,958 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:36:30,091 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:36:30,175 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:30,175 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:30,177 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:36:30,522 - INFO - joeynmt.prediction - Generation took 0.3440[sec]. (No references given)
2022-09-15 22:36:30,529 - INFO - joeynmt.training - Processing Predictions on Batch 86/103
2022-09-15 22:36:32,879 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:36:32,879 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:36:33,097 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:36:33,229 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:36:33,313 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:33,313 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:33,314 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:36:34,313 - INFO - joeynmt.prediction - Generation took 0.9975[sec]. (No references given)
2022-09-15 22:36:34,320 - INFO - joeynmt.training - Processing Predictions on Batch 87/103
2022-09-15 22:36:36,675 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:36:36,675 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:36:36,889 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:36:37,022 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:36:37,107 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:37,107 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:37,110 - INFO - joeynmt.prediction - Predicting 11 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:36:37,488 - INFO - joeynmt.prediction - Generation took 0.3772[sec]. (No references given)
2022-09-15 22:36:37,496 - INFO - joeynmt.training - Processing Predictions on Batch 88/103
2022-09-15 22:36:39,880 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:36:39,880 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:36:40,094 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:36:40,226 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:36:40,312 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:40,312 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:40,314 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:36:40,743 - INFO - joeynmt.prediction - Generation took 0.4277[sec]. (No references given)
2022-09-15 22:36:40,750 - INFO - joeynmt.training - Processing Predictions on Batch 89/103
2022-09-15 22:36:43,104 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:36:43,104 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:36:43,319 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:36:43,451 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:36:43,538 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:43,538 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:43,540 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:36:44,277 - INFO - joeynmt.prediction - Generation took 0.7363[sec]. (No references given)
2022-09-15 22:36:44,285 - INFO - joeynmt.training - Processing Predictions on Batch 90/103
2022-09-15 22:36:46,639 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:36:46,639 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:36:46,854 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:36:46,985 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:36:47,072 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:47,072 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:47,072 - INFO - joeynmt.prediction - Predicting 2 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:36:47,360 - INFO - joeynmt.prediction - Generation took 0.2870[sec]. (No references given)
2022-09-15 22:36:47,368 - INFO - joeynmt.training - Processing Predictions on Batch 91/103
2022-09-15 22:36:49,718 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:36:49,718 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:36:49,932 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:36:50,062 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:36:50,148 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:50,148 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:50,149 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:36:50,612 - INFO - joeynmt.prediction - Generation took 0.4610[sec]. (No references given)
2022-09-15 22:36:50,619 - INFO - joeynmt.training - Processing Predictions on Batch 92/103
2022-09-15 22:36:52,985 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:36:52,985 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:36:53,204 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:36:53,335 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:36:53,419 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:53,420 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:53,421 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:36:53,849 - INFO - joeynmt.prediction - Generation took 0.4259[sec]. (No references given)
2022-09-15 22:36:53,856 - INFO - joeynmt.training - Processing Predictions on Batch 93/103
2022-09-15 22:36:56,213 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:36:56,213 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:36:56,427 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:36:57,023 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:36:57,107 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:57,107 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:36:57,110 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:36:58,196 - INFO - joeynmt.prediction - Generation took 1.0846[sec]. (No references given)
2022-09-15 22:36:58,203 - INFO - joeynmt.training - Processing Predictions on Batch 94/103
2022-09-15 22:37:00,585 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:37:00,585 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:37:00,799 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:37:00,930 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:37:01,016 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:37:01,016 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:37:01,018 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:37:02,076 - INFO - joeynmt.prediction - Generation took 1.0573[sec]. (No references given)
2022-09-15 22:37:02,084 - INFO - joeynmt.training - Processing Predictions on Batch 95/103
2022-09-15 22:37:04,458 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:37:04,458 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:37:04,672 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:37:04,802 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:37:04,888 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:37:04,888 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:37:04,890 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:37:05,285 - INFO - joeynmt.prediction - Generation took 0.3943[sec]. (No references given)
2022-09-15 22:37:05,293 - INFO - joeynmt.training - Processing Predictions on Batch 96/103
2022-09-15 22:37:07,680 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:37:07,680 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:37:07,893 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:37:08,024 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:37:08,109 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:37:08,109 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:37:08,110 - INFO - joeynmt.prediction - Predicting 3 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:37:09,093 - INFO - joeynmt.prediction - Generation took 0.9821[sec]. (No references given)
2022-09-15 22:37:09,101 - INFO - joeynmt.training - Processing Predictions on Batch 97/103
2022-09-15 22:37:11,468 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:37:11,469 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:37:11,682 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:37:11,814 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:37:11,899 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:37:11,899 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:37:11,900 - INFO - joeynmt.prediction - Predicting 2 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:37:12,369 - INFO - joeynmt.prediction - Generation took 0.4685[sec]. (No references given)
2022-09-15 22:37:12,376 - INFO - joeynmt.training - Processing Predictions on Batch 98/103
2022-09-15 22:37:14,763 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:37:14,764 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:37:14,978 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:37:15,109 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:37:15,194 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:37:15,194 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:37:15,197 - INFO - joeynmt.prediction - Predicting 11 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:37:15,723 - INFO - joeynmt.prediction - Generation took 0.5244[sec]. (No references given)
2022-09-15 22:37:15,730 - INFO - joeynmt.training - Processing Predictions on Batch 99/103
2022-09-15 22:37:18,108 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:37:18,108 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:37:18,321 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:37:18,452 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:37:18,537 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:37:18,537 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:37:18,539 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:37:18,950 - INFO - joeynmt.prediction - Generation took 0.4095[sec]. (No references given)
2022-09-15 22:37:18,957 - INFO - joeynmt.training - Processing Predictions on Batch 100/103
2022-09-15 22:37:21,333 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:37:21,334 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:37:21,547 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:37:21,684 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:37:21,768 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:37:21,768 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:37:21,771 - INFO - joeynmt.prediction - Predicting 12 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:37:22,507 - INFO - joeynmt.prediction - Generation took 0.7349[sec]. (No references given)
2022-09-15 22:37:22,514 - INFO - joeynmt.training - Processing Predictions on Batch 101/103
2022-09-15 22:37:24,902 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:37:24,902 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:37:25,115 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:37:25,247 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:37:25,331 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:37:25,331 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:37:25,332 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:37:25,770 - INFO - joeynmt.prediction - Generation took 0.4365[sec]. (No references given)
2022-09-15 22:37:25,777 - INFO - joeynmt.training - Processing Predictions on Batch 102/103
2022-09-15 22:37:28,162 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:37:28,162 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:37:28,377 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:37:28,510 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt.
2022-09-15 22:37:28,594 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:37:28,594 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:37:28,594 - INFO - joeynmt.prediction - Predicting 2 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:37:28,835 - INFO - joeynmt.prediction - Generation took 0.2395[sec]. (No references given)
2022-09-15 22:37:29,008 - INFO - joeynmt.training - Final Query Indices picked: [337109, 85981, 289884, 223023, 237464, 100153, 135024, 279477, 355289, 392159] length: 10000
2022-09-15 22:37:29,008 - INFO - joeynmt.training - Query the samples, one at a time (interactive/file)
2022-09-15 22:37:31,265 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 4096
	effective batch size (w. parallel & accumulation): 4096
2022-09-15 22:37:31,266 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:53:17,469 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 22:53:17,470 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.84, loss:   2.54, ppl:  12.67, acc:   0.51, generation: 938.8837[sec], evaluation: 6.9129[sec]
2022-09-15 22:53:18,999 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/231000.ckpt
2022-09-15 22:53:19,041 - INFO - joeynmt.training - Example #0
2022-09-15 22:53:19,052 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 22:53:19,052 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 22:53:19,052 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 22:53:19,052 - INFO - joeynmt.training - Example #1
2022-09-15 22:53:19,063 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 22:53:19,063 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 22:53:19,063 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 22:53:19,063 - INFO - joeynmt.training - Example #2
2022-09-15 22:53:19,074 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 22:53:19,074 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 22:53:19,074 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 22:53:19,074 - INFO - joeynmt.training - Example #3
2022-09-15 22:53:19,085 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 22:53:19,085 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 22:53:19,085 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 22:53:19,177 - INFO - joeynmt.training - EPOCH 1
2022-09-15 22:53:28,411 - INFO - joeynmt.training - Epoch   1, Step:   232100, Batch Loss:     2.517315, Batch Acc: 0.011719, Tokens per Sec:     4908, Lr: 0.000035
2022-09-15 22:53:51,338 - INFO - joeynmt.training - Epoch   1, Step:   232200, Batch Loss:     2.812592, Batch Acc: 0.004552, Tokens per Sec:     4945, Lr: 0.000035
2022-09-15 22:54:14,249 - INFO - joeynmt.training - Epoch   1, Step:   232300, Batch Loss:     2.766699, Batch Acc: 0.005037, Tokens per Sec:     5000, Lr: 0.000035
2022-09-15 22:54:37,227 - INFO - joeynmt.training - Epoch   1, Step:   232400, Batch Loss:     2.754273, Batch Acc: 0.005046, Tokens per Sec:     5020, Lr: 0.000035
2022-09-15 22:55:00,372 - INFO - joeynmt.training - Epoch   1, Step:   232500, Batch Loss:     2.666021, Batch Acc: 0.004323, Tokens per Sec:     4898, Lr: 0.000035
2022-09-15 22:55:23,382 - INFO - joeynmt.training - Epoch   1, Step:   232600, Batch Loss:     2.578382, Batch Acc: 0.005299, Tokens per Sec:     5052, Lr: 0.000035
2022-09-15 22:55:46,225 - INFO - joeynmt.training - Epoch   1, Step:   232700, Batch Loss:     2.709274, Batch Acc: 0.004549, Tokens per Sec:     5033, Lr: 0.000035
2022-09-15 22:56:08,991 - INFO - joeynmt.training - Epoch   1, Step:   232800, Batch Loss:     2.398188, Batch Acc: 0.005286, Tokens per Sec:     4911, Lr: 0.000035
2022-09-15 22:56:31,924 - INFO - joeynmt.training - Epoch   1, Step:   232900, Batch Loss:     2.603895, Batch Acc: 0.004857, Tokens per Sec:     4947, Lr: 0.000035
2022-09-15 22:56:54,706 - INFO - joeynmt.training - Epoch   1, Step:   233000, Batch Loss:     2.484893, Batch Acc: 0.004610, Tokens per Sec:     5008, Lr: 0.000035
2022-09-15 22:56:54,706 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 23:12:32,151 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 23:12:32,153 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.82, loss:   2.54, ppl:  12.67, acc:   0.51, generation: 930.1174[sec], evaluation: 6.9846[sec]
2022-09-15 23:12:34,234 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/230000.ckpt
2022-09-15 23:12:34,276 - INFO - joeynmt.training - Example #0
2022-09-15 23:12:34,288 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 23:12:34,288 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 23:12:34,288 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 23:12:34,288 - INFO - joeynmt.training - Example #1
2022-09-15 23:12:34,299 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 23:12:34,299 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 23:12:34,299 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 23:12:34,299 - INFO - joeynmt.training - Example #2
2022-09-15 23:12:34,309 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 23:12:34,309 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 23:12:34,309 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 23:12:34,309 - INFO - joeynmt.training - Example #3
2022-09-15 23:12:34,320 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 23:12:34,320 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 23:12:34,320 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 23:12:57,220 - INFO - joeynmt.training - Epoch   1, Step:   233100, Batch Loss:     2.522119, Batch Acc: 0.004790, Tokens per Sec:     4606, Lr: 0.000035
2022-09-15 23:13:19,886 - INFO - joeynmt.training - Epoch   1, Step:   233200, Batch Loss:     2.415876, Batch Acc: 0.005597, Tokens per Sec:     4998, Lr: 0.000035
2022-09-15 23:13:42,635 - INFO - joeynmt.training - Epoch   1, Step:   233300, Batch Loss:     2.479902, Batch Acc: 0.005075, Tokens per Sec:     5180, Lr: 0.000035
2022-09-15 23:14:05,379 - INFO - joeynmt.training - Epoch   1, Step:   233400, Batch Loss:     2.724277, Batch Acc: 0.004038, Tokens per Sec:     4966, Lr: 0.000035
2022-09-15 23:14:28,285 - INFO - joeynmt.training - Epoch   1, Step:   233500, Batch Loss:     2.694449, Batch Acc: 0.004042, Tokens per Sec:     5087, Lr: 0.000035
2022-09-15 23:14:51,239 - INFO - joeynmt.training - Epoch   1, Step:   233600, Batch Loss:     2.785095, Batch Acc: 0.004194, Tokens per Sec:     5028, Lr: 0.000035
2022-09-15 23:15:14,059 - INFO - joeynmt.training - Epoch   1, Step:   233700, Batch Loss:     2.906899, Batch Acc: 0.004231, Tokens per Sec:     5085, Lr: 0.000035
2022-09-15 23:15:36,751 - INFO - joeynmt.training - Epoch   1, Step:   233800, Batch Loss:     2.567750, Batch Acc: 0.005457, Tokens per Sec:     4934, Lr: 0.000035
2022-09-15 23:15:59,454 - INFO - joeynmt.training - Epoch   1, Step:   233900, Batch Loss:     2.411706, Batch Acc: 0.004509, Tokens per Sec:     4982, Lr: 0.000035
2022-09-15 23:16:22,192 - INFO - joeynmt.training - Epoch   1, Step:   234000, Batch Loss:     2.698953, Batch Acc: 0.003420, Tokens per Sec:     5131, Lr: 0.000035
2022-09-15 23:16:22,192 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 23:31:58,066 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 23:31:58,068 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.84, loss:   2.54, ppl:  12.62, acc:   0.51, generation: 928.4889[sec], evaluation: 7.0408[sec]
2022-09-15 23:31:58,071 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 23:31:59,592 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/233000.ckpt
2022-09-15 23:31:59,633 - INFO - joeynmt.training - Example #0
2022-09-15 23:31:59,646 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 23:31:59,646 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 23:31:59,646 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 23:31:59,646 - INFO - joeynmt.training - Example #1
2022-09-15 23:31:59,656 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 23:31:59,656 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 23:31:59,656 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 23:31:59,656 - INFO - joeynmt.training - Example #2
2022-09-15 23:31:59,667 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 23:31:59,667 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 23:31:59,667 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 23:31:59,667 - INFO - joeynmt.training - Example #3
2022-09-15 23:31:59,677 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 23:31:59,678 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 23:31:59,678 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 23:32:22,615 - INFO - joeynmt.training - Epoch   1, Step:   234100, Batch Loss:     2.714349, Batch Acc: 0.004704, Tokens per Sec:     4686, Lr: 0.000035
2022-09-15 23:32:45,436 - INFO - joeynmt.training - Epoch   1, Step:   234200, Batch Loss:     2.633061, Batch Acc: 0.003898, Tokens per Sec:     4946, Lr: 0.000035
2022-09-15 23:33:08,336 - INFO - joeynmt.training - Epoch   1, Step:   234300, Batch Loss:     2.581135, Batch Acc: 0.003975, Tokens per Sec:     5032, Lr: 0.000035
2022-09-15 23:33:31,136 - INFO - joeynmt.training - Epoch   1, Step:   234400, Batch Loss:     2.905066, Batch Acc: 0.004093, Tokens per Sec:     5037, Lr: 0.000035
2022-09-15 23:33:53,862 - INFO - joeynmt.training - Epoch   1, Step:   234500, Batch Loss:     2.768661, Batch Acc: 0.003877, Tokens per Sec:     4971, Lr: 0.000035
2022-09-15 23:34:16,690 - INFO - joeynmt.training - Epoch   1, Step:   234600, Batch Loss:     2.718337, Batch Acc: 0.004664, Tokens per Sec:     5006, Lr: 0.000035
2022-09-15 23:34:39,416 - INFO - joeynmt.training - Epoch   1, Step:   234700, Batch Loss:     2.554857, Batch Acc: 0.004306, Tokens per Sec:     5058, Lr: 0.000035
2022-09-15 23:35:02,087 - INFO - joeynmt.training - Epoch   1, Step:   234800, Batch Loss:     2.672624, Batch Acc: 0.004335, Tokens per Sec:     4986, Lr: 0.000035
2022-09-15 23:35:24,888 - INFO - joeynmt.training - Epoch   1, Step:   234900, Batch Loss:     2.504347, Batch Acc: 0.004207, Tokens per Sec:     5035, Lr: 0.000035
2022-09-15 23:35:47,597 - INFO - joeynmt.training - Epoch   1, Step:   235000, Batch Loss:     2.780532, Batch Acc: 0.004310, Tokens per Sec:     5026, Lr: 0.000035
2022-09-15 23:35:47,598 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 23:51:49,043 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 23:51:49,045 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.88, loss:   2.53, ppl:  12.58, acc:   0.51, generation: 954.0835[sec], evaluation: 7.0183[sec]
2022-09-15 23:51:49,048 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 23:51:50,985 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232061.ckpt
2022-09-15 23:51:51,030 - INFO - joeynmt.training - Example #0
2022-09-15 23:51:51,041 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 23:51:51,041 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 23:51:51,041 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 23:51:51,041 - INFO - joeynmt.training - Example #1
2022-09-15 23:51:51,052 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 23:51:51,052 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 23:51:51,052 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 23:51:51,052 - INFO - joeynmt.training - Example #2
2022-09-15 23:51:51,062 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 23:51:51,062 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 23:51:51,062 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 23:51:51,062 - INFO - joeynmt.training - Example #3
2022-09-15 23:51:51,072 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 23:51:51,072 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 23:51:51,073 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 23:52:13,883 - INFO - joeynmt.training - Epoch   1, Step:   235100, Batch Loss:     2.549529, Batch Acc: 0.004665, Tokens per Sec:     4618, Lr: 0.000035
2022-09-15 23:52:36,631 - INFO - joeynmt.training - Epoch   1, Step:   235200, Batch Loss:     2.608458, Batch Acc: 0.005374, Tokens per Sec:     4998, Lr: 0.000035
2022-09-15 23:52:59,388 - INFO - joeynmt.training - Epoch   1, Step:   235300, Batch Loss:     2.585564, Batch Acc: 0.003854, Tokens per Sec:     5074, Lr: 0.000035
2022-09-15 23:53:22,194 - INFO - joeynmt.training - Epoch   1, Step:   235400, Batch Loss:     2.535718, Batch Acc: 0.005182, Tokens per Sec:     4942, Lr: 0.000035
2022-09-15 23:53:45,025 - INFO - joeynmt.training - Epoch   1, Step:   235500, Batch Loss:     2.705285, Batch Acc: 0.004632, Tokens per Sec:     5031, Lr: 0.000035
2022-09-15 23:54:07,823 - INFO - joeynmt.training - Epoch   1, Step:   235600, Batch Loss:     2.533643, Batch Acc: 0.006180, Tokens per Sec:     4968, Lr: 0.000035
2022-09-15 23:54:30,576 - INFO - joeynmt.training - Epoch   1, Step:   235700, Batch Loss:     2.614454, Batch Acc: 0.004895, Tokens per Sec:     5055, Lr: 0.000035
2022-09-15 23:54:53,344 - INFO - joeynmt.training - Epoch   1, Step:   235800, Batch Loss:     2.610902, Batch Acc: 0.004625, Tokens per Sec:     4985, Lr: 0.000035
2022-09-15 23:55:16,011 - INFO - joeynmt.training - Epoch   1, Step:   235900, Batch Loss:     2.592427, Batch Acc: 0.003900, Tokens per Sec:     5056, Lr: 0.000035
2022-09-15 23:55:38,685 - INFO - joeynmt.training - Epoch   1, Step:   236000, Batch Loss:     2.956146, Batch Acc: 0.004348, Tokens per Sec:     5021, Lr: 0.000035
2022-09-15 23:55:38,685 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 00:11:04,668 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 00:11:04,669 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.91, loss:   2.54, ppl:  12.62, acc:   0.51, generation: 919.0776[sec], evaluation: 6.5668[sec]
2022-09-16 00:11:06,315 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/232000.ckpt
2022-09-16 00:11:06,359 - INFO - joeynmt.training - Example #0
2022-09-16 00:11:06,371 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 00:11:06,371 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 00:11:06,371 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 00:11:06,371 - INFO - joeynmt.training - Example #1
2022-09-16 00:11:06,382 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 00:11:06,382 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 00:11:06,382 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन प्ले करें
2022-09-16 00:11:06,382 - INFO - joeynmt.training - Example #2
2022-09-16 00:11:06,392 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 00:11:06,392 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 00:11:06,392 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 00:11:06,393 - INFO - joeynmt.training - Example #3
2022-09-16 00:11:06,403 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 00:11:06,403 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 00:11:06,403 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 00:11:29,382 - INFO - joeynmt.training - Epoch   1, Step:   236100, Batch Loss:     2.483837, Batch Acc: 0.003587, Tokens per Sec:     4638, Lr: 0.000035
2022-09-16 00:11:51,944 - INFO - joeynmt.training - Epoch   1, Step:   236200, Batch Loss:     2.606816, Batch Acc: 0.004518, Tokens per Sec:     5033, Lr: 0.000035
2022-09-16 00:12:14,640 - INFO - joeynmt.training - Epoch   1, Step:   236300, Batch Loss:     2.714186, Batch Acc: 0.004468, Tokens per Sec:     4980, Lr: 0.000035
2022-09-16 00:12:37,412 - INFO - joeynmt.training - Epoch   1, Step:   236400, Batch Loss:     2.895572, Batch Acc: 0.004592, Tokens per Sec:     5049, Lr: 0.000035
2022-09-16 00:13:00,185 - INFO - joeynmt.training - Epoch   1, Step:   236500, Batch Loss:     2.681655, Batch Acc: 0.004120, Tokens per Sec:     5020, Lr: 0.000035
2022-09-16 00:13:22,867 - INFO - joeynmt.training - Epoch   1, Step:   236600, Batch Loss:     2.381360, Batch Acc: 0.004735, Tokens per Sec:     5001, Lr: 0.000035
2022-09-16 00:13:45,629 - INFO - joeynmt.training - Epoch   1, Step:   236700, Batch Loss:     2.648744, Batch Acc: 0.005048, Tokens per Sec:     5039, Lr: 0.000035
2022-09-16 00:14:08,417 - INFO - joeynmt.training - Epoch   1, Step:   236800, Batch Loss:     2.663543, Batch Acc: 0.004248, Tokens per Sec:     5000, Lr: 0.000035
2022-09-16 00:14:31,075 - INFO - joeynmt.training - Epoch   1, Step:   236900, Batch Loss:     2.651706, Batch Acc: 0.003745, Tokens per Sec:     4985, Lr: 0.000035
2022-09-16 00:14:53,925 - INFO - joeynmt.training - Epoch   1, Step:   237000, Batch Loss:     2.535236, Batch Acc: 0.004401, Tokens per Sec:     5052, Lr: 0.000035
2022-09-16 00:14:53,926 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 00:31:46,820 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 00:31:46,821 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.00, loss:   2.53, ppl:  12.52, acc:   0.51, generation: 1005.5668[sec], evaluation: 6.6034[sec]
2022-09-16 00:31:46,825 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 00:31:48,399 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/234000.ckpt
2022-09-16 00:31:48,441 - INFO - joeynmt.training - Example #0
2022-09-16 00:31:48,453 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 00:31:48,453 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 00:31:48,453 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 00:31:48,453 - INFO - joeynmt.training - Example #1
2022-09-16 00:31:48,463 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 00:31:48,463 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 00:31:48,463 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 00:31:48,464 - INFO - joeynmt.training - Example #2
2022-09-16 00:31:48,474 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 00:31:48,474 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 00:31:48,474 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 00:31:48,474 - INFO - joeynmt.training - Example #3
2022-09-16 00:31:48,484 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 00:31:48,484 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 00:31:48,484 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 00:32:11,307 - INFO - joeynmt.training - Epoch   1, Step:   237100, Batch Loss:     2.576090, Batch Acc: 0.004669, Tokens per Sec:     4680, Lr: 0.000035
2022-09-16 00:32:34,004 - INFO - joeynmt.training - Epoch   1, Step:   237200, Batch Loss:     2.658794, Batch Acc: 0.004009, Tokens per Sec:     5055, Lr: 0.000035
2022-09-16 00:32:56,790 - INFO - joeynmt.training - Epoch   1, Step:   237300, Batch Loss:     2.802258, Batch Acc: 0.003846, Tokens per Sec:     4987, Lr: 0.000035
2022-09-16 00:33:19,496 - INFO - joeynmt.training - Epoch   1, Step:   237400, Batch Loss:     2.561522, Batch Acc: 0.005193, Tokens per Sec:     5004, Lr: 0.000035
2022-09-16 00:33:42,203 - INFO - joeynmt.training - Epoch   1, Step:   237500, Batch Loss:     2.309943, Batch Acc: 0.004444, Tokens per Sec:     5074, Lr: 0.000035
2022-09-16 00:34:04,853 - INFO - joeynmt.training - Epoch   1, Step:   237600, Batch Loss:     2.657561, Batch Acc: 0.003979, Tokens per Sec:     5071, Lr: 0.000035
2022-09-16 00:34:27,552 - INFO - joeynmt.training - Epoch   1, Step:   237700, Batch Loss:     2.703329, Batch Acc: 0.004502, Tokens per Sec:     5030, Lr: 0.000035
2022-09-16 00:34:50,354 - INFO - joeynmt.training - Epoch   1, Step:   237800, Batch Loss:     2.712592, Batch Acc: 0.004352, Tokens per Sec:     4978, Lr: 0.000035
2022-09-16 00:35:13,045 - INFO - joeynmt.training - Epoch   1, Step:   237900, Batch Loss:     2.859306, Batch Acc: 0.004225, Tokens per Sec:     5028, Lr: 0.000035
2022-09-16 00:35:35,758 - INFO - joeynmt.training - Epoch   1, Step:   238000, Batch Loss:     2.511887, Batch Acc: 0.004702, Tokens per Sec:     5057, Lr: 0.000035
2022-09-16 00:35:35,758 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 00:51:57,685 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 00:51:57,686 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.91, loss:   2.53, ppl:  12.54, acc:   0.51, generation: 974.4769[sec], evaluation: 7.1068[sec]
2022-09-16 00:51:59,163 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/236000.ckpt
2022-09-16 00:51:59,205 - INFO - joeynmt.training - Example #0
2022-09-16 00:51:59,217 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 00:51:59,217 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 00:51:59,217 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 00:51:59,217 - INFO - joeynmt.training - Example #1
2022-09-16 00:51:59,228 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 00:51:59,228 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 00:51:59,228 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 00:51:59,228 - INFO - joeynmt.training - Example #2
2022-09-16 00:51:59,239 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 00:51:59,239 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 00:51:59,239 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 00:51:59,239 - INFO - joeynmt.training - Example #3
2022-09-16 00:51:59,249 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 00:51:59,249 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 00:51:59,249 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 00:52:22,054 - INFO - joeynmt.training - Epoch   1, Step:   238100, Batch Loss:     2.542808, Batch Acc: 0.004307, Tokens per Sec:     4660, Lr: 0.000035
2022-09-16 00:52:44,769 - INFO - joeynmt.training - Epoch   1, Step:   238200, Batch Loss:     2.600665, Batch Acc: 0.005214, Tokens per Sec:     5057, Lr: 0.000035
2022-09-16 00:53:07,380 - INFO - joeynmt.training - Epoch   1, Step:   238300, Batch Loss:     2.567084, Batch Acc: 0.004798, Tokens per Sec:     5106, Lr: 0.000035
2022-09-16 00:53:30,173 - INFO - joeynmt.training - Epoch   1, Step:   238400, Batch Loss:     2.631370, Batch Acc: 0.004625, Tokens per Sec:     5009, Lr: 0.000035
2022-09-16 00:53:52,797 - INFO - joeynmt.training - Epoch   1, Step:   238500, Batch Loss:     2.958762, Batch Acc: 0.003712, Tokens per Sec:     4989, Lr: 0.000035
2022-09-16 00:54:15,530 - INFO - joeynmt.training - Epoch   1, Step:   238600, Batch Loss:     2.542800, Batch Acc: 0.005105, Tokens per Sec:     5015, Lr: 0.000035
2022-09-16 00:54:38,222 - INFO - joeynmt.training - Epoch   1, Step:   238700, Batch Loss:     2.644223, Batch Acc: 0.004266, Tokens per Sec:     5000, Lr: 0.000035
2022-09-16 00:55:01,053 - INFO - joeynmt.training - Epoch   1, Step:   238800, Batch Loss:     2.757710, Batch Acc: 0.003230, Tokens per Sec:     5058, Lr: 0.000035
2022-09-16 00:55:23,786 - INFO - joeynmt.training - Epoch   1, Step:   238900, Batch Loss:     2.775115, Batch Acc: 0.004717, Tokens per Sec:     4999, Lr: 0.000035
2022-09-16 00:55:46,490 - INFO - joeynmt.training - Epoch   1, Step:   239000, Batch Loss:     2.628381, Batch Acc: 0.004800, Tokens per Sec:     5010, Lr: 0.000035
2022-09-16 00:55:46,490 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:11:57,369 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 01:11:57,371 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.99, loss:   2.52, ppl:  12.46, acc:   0.51, generation: 963.9057[sec], evaluation: 6.6320[sec]
2022-09-16 01:11:57,374 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 01:11:59,327 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/235000.ckpt
2022-09-16 01:11:59,371 - INFO - joeynmt.training - Example #0
2022-09-16 01:11:59,382 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 01:11:59,382 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 01:11:59,382 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 01:11:59,383 - INFO - joeynmt.training - Example #1
2022-09-16 01:11:59,393 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 01:11:59,393 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 01:11:59,394 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 01:11:59,394 - INFO - joeynmt.training - Example #2
2022-09-16 01:11:59,404 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 01:11:59,404 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 01:11:59,404 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 01:11:59,404 - INFO - joeynmt.training - Example #3
2022-09-16 01:11:59,415 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 01:11:59,415 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 01:11:59,415 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 01:12:22,274 - INFO - joeynmt.training - Epoch   1, Step:   239100, Batch Loss:     2.710363, Batch Acc: 0.004696, Tokens per Sec:     4687, Lr: 0.000035
2022-09-16 01:12:44,996 - INFO - joeynmt.training - Epoch   1, Step:   239200, Batch Loss:     2.546679, Batch Acc: 0.005080, Tokens per Sec:     5033, Lr: 0.000035
2022-09-16 01:13:07,621 - INFO - joeynmt.training - Epoch   1, Step:   239300, Batch Loss:     2.540458, Batch Acc: 0.004659, Tokens per Sec:     5113, Lr: 0.000035
2022-09-16 01:13:30,270 - INFO - joeynmt.training - Epoch   1, Step:   239400, Batch Loss:     2.882983, Batch Acc: 0.003781, Tokens per Sec:     5138, Lr: 0.000035
2022-09-16 01:13:52,855 - INFO - joeynmt.training - Epoch   1, Step:   239500, Batch Loss:     2.635468, Batch Acc: 0.004271, Tokens per Sec:     5080, Lr: 0.000035
2022-09-16 01:14:15,634 - INFO - joeynmt.training - Epoch   1, Step:   239600, Batch Loss:     2.430687, Batch Acc: 0.004051, Tokens per Sec:     5029, Lr: 0.000035
2022-09-16 01:14:38,378 - INFO - joeynmt.training - Epoch   1, Step:   239700, Batch Loss:     2.556675, Batch Acc: 0.004723, Tokens per Sec:     5092, Lr: 0.000035
2022-09-16 01:15:01,284 - INFO - joeynmt.training - Epoch   1, Step:   239800, Batch Loss:     2.794376, Batch Acc: 0.004575, Tokens per Sec:     4972, Lr: 0.000035
2022-09-16 01:15:24,114 - INFO - joeynmt.training - Epoch   1, Step:   239900, Batch Loss:     2.563660, Batch Acc: 0.004693, Tokens per Sec:     4975, Lr: 0.000035
2022-09-16 01:15:46,777 - INFO - joeynmt.training - Epoch   1, Step:   240000, Batch Loss:     2.538842, Batch Acc: 0.005564, Tokens per Sec:     5139, Lr: 0.000035
2022-09-16 01:15:46,777 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:31:05,746 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 01:31:05,747 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.96, loss:   2.52, ppl:  12.42, acc:   0.51, generation: 912.0365[sec], evaluation: 6.5890[sec]
2022-09-16 01:31:05,751 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 01:31:07,742 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/238000.ckpt
2022-09-16 01:31:07,784 - INFO - joeynmt.training - Example #0
2022-09-16 01:31:07,796 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 01:31:07,796 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 01:31:07,797 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 01:31:07,797 - INFO - joeynmt.training - Example #1
2022-09-16 01:31:07,807 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 01:31:07,807 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 01:31:07,807 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 01:31:07,807 - INFO - joeynmt.training - Example #2
2022-09-16 01:31:07,818 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 01:31:07,818 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 01:31:07,818 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 01:31:07,818 - INFO - joeynmt.training - Example #3
2022-09-16 01:31:07,828 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 01:31:07,829 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 01:31:07,829 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 01:31:30,663 - INFO - joeynmt.training - Epoch   1, Step:   240100, Batch Loss:     2.702304, Batch Acc: 0.004264, Tokens per Sec:     4631, Lr: 0.000035
2022-09-16 01:31:53,337 - INFO - joeynmt.training - Epoch   1, Step:   240200, Batch Loss:     2.622560, Batch Acc: 0.004369, Tokens per Sec:     5047, Lr: 0.000035
2022-09-16 01:32:16,037 - INFO - joeynmt.training - Epoch   1, Step:   240300, Batch Loss:     2.692842, Batch Acc: 0.004497, Tokens per Sec:     4996, Lr: 0.000035
2022-09-16 01:32:38,722 - INFO - joeynmt.training - Epoch   1, Step:   240400, Batch Loss:     2.625448, Batch Acc: 0.004199, Tokens per Sec:     5082, Lr: 0.000035
2022-09-16 01:33:01,487 - INFO - joeynmt.training - Epoch   1, Step:   240500, Batch Loss:     2.799929, Batch Acc: 0.004465, Tokens per Sec:     5007, Lr: 0.000035
2022-09-16 01:33:24,169 - INFO - joeynmt.training - Epoch   1, Step:   240600, Batch Loss:     2.435328, Batch Acc: 0.004823, Tokens per Sec:     5046, Lr: 0.000035
2022-09-16 01:33:47,015 - INFO - joeynmt.training - Epoch   1, Step:   240700, Batch Loss:     2.257837, Batch Acc: 0.004953, Tokens per Sec:     5047, Lr: 0.000035
2022-09-16 01:34:09,667 - INFO - joeynmt.training - Epoch   1, Step:   240800, Batch Loss:     2.667175, Batch Acc: 0.003925, Tokens per Sec:     5039, Lr: 0.000035
2022-09-16 01:34:32,366 - INFO - joeynmt.training - Epoch   1, Step:   240900, Batch Loss:     2.680660, Batch Acc: 0.003646, Tokens per Sec:     5002, Lr: 0.000035
2022-09-16 01:34:55,112 - INFO - joeynmt.training - Epoch   1, Step:   241000, Batch Loss:     2.634621, Batch Acc: 0.004925, Tokens per Sec:     5034, Lr: 0.000035
2022-09-16 01:34:55,112 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:50:51,790 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 01:50:51,791 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.07, loss:   2.52, ppl:  12.37, acc:   0.51, generation: 949.6076[sec], evaluation: 6.7254[sec]
2022-09-16 01:50:51,795 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 01:50:53,286 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/237000.ckpt
2022-09-16 01:50:53,328 - INFO - joeynmt.training - Example #0
2022-09-16 01:50:53,340 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 01:50:53,340 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 01:50:53,340 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 01:50:53,340 - INFO - joeynmt.training - Example #1
2022-09-16 01:50:53,351 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 01:50:53,351 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 01:50:53,351 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 01:50:53,351 - INFO - joeynmt.training - Example #2
2022-09-16 01:50:53,362 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 01:50:53,362 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 01:50:53,362 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 01:50:53,362 - INFO - joeynmt.training - Example #3
2022-09-16 01:50:53,372 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 01:50:53,373 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 01:50:53,373 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 01:51:16,069 - INFO - joeynmt.training - Epoch   1, Step:   241100, Batch Loss:     2.736036, Batch Acc: 0.004832, Tokens per Sec:     4664, Lr: 0.000035
2022-09-16 01:51:38,790 - INFO - joeynmt.training - Epoch   1, Step:   241200, Batch Loss:     2.669944, Batch Acc: 0.003955, Tokens per Sec:     5030, Lr: 0.000035
2022-09-16 01:52:01,474 - INFO - joeynmt.training - Epoch   1, Step:   241300, Batch Loss:     2.486598, Batch Acc: 0.004631, Tokens per Sec:     4988, Lr: 0.000035
2022-09-16 01:52:24,086 - INFO - joeynmt.training - Epoch   1, Step:   241400, Batch Loss:     2.837461, Batch Acc: 0.003680, Tokens per Sec:     5048, Lr: 0.000035
2022-09-16 01:52:46,746 - INFO - joeynmt.training - Epoch   1, Step:   241500, Batch Loss:     2.570309, Batch Acc: 0.004875, Tokens per Sec:     4979, Lr: 0.000035
2022-09-16 01:53:09,561 - INFO - joeynmt.training - Epoch   1, Step:   241600, Batch Loss:     2.632543, Batch Acc: 0.004736, Tokens per Sec:     4933, Lr: 0.000035
2022-09-16 01:53:32,395 - INFO - joeynmt.training - Epoch   1, Step:   241700, Batch Loss:     2.831003, Batch Acc: 0.004127, Tokens per Sec:     5104, Lr: 0.000035
2022-09-16 01:53:55,065 - INFO - joeynmt.training - Epoch   1, Step:   241800, Batch Loss:     2.533970, Batch Acc: 0.004332, Tokens per Sec:     5031, Lr: 0.000035
2022-09-16 01:54:17,870 - INFO - joeynmt.training - Epoch   1, Step:   241900, Batch Loss:     2.598417, Batch Acc: 0.004705, Tokens per Sec:     5098, Lr: 0.000035
2022-09-16 01:54:40,551 - INFO - joeynmt.training - Epoch   1, Step:   242000, Batch Loss:     2.851004, Batch Acc: 0.004725, Tokens per Sec:     5030, Lr: 0.000035
2022-09-16 01:54:40,551 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 02:10:44,788 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 02:10:44,790 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.17, loss:   2.52, ppl:  12.43, acc:   0.51, generation: 957.2218[sec], evaluation: 6.6703[sec]
2022-09-16 02:10:47,332 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/239000.ckpt
2022-09-16 02:10:47,374 - INFO - joeynmt.training - Example #0
2022-09-16 02:10:47,387 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 02:10:47,387 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 02:10:47,387 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 02:10:47,387 - INFO - joeynmt.training - Example #1
2022-09-16 02:10:47,397 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 02:10:47,397 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 02:10:47,397 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 02:10:47,397 - INFO - joeynmt.training - Example #2
2022-09-16 02:10:47,408 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 02:10:47,408 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 02:10:47,408 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 02:10:47,408 - INFO - joeynmt.training - Example #3
2022-09-16 02:10:47,419 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 02:10:47,419 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 02:10:47,419 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 02:11:10,255 - INFO - joeynmt.training - Epoch   1, Step:   242100, Batch Loss:     2.554745, Batch Acc: 0.004142, Tokens per Sec:     4542, Lr: 0.000035
2022-09-16 02:11:32,959 - INFO - joeynmt.training - Epoch   1, Step:   242200, Batch Loss:     2.732279, Batch Acc: 0.004529, Tokens per Sec:     5048, Lr: 0.000035
2022-09-16 02:11:55,749 - INFO - joeynmt.training - Epoch   1, Step:   242300, Batch Loss:     2.505693, Batch Acc: 0.004501, Tokens per Sec:     5010, Lr: 0.000035
2022-09-16 02:12:18,495 - INFO - joeynmt.training - Epoch   1, Step:   242400, Batch Loss:     2.698363, Batch Acc: 0.004368, Tokens per Sec:     5103, Lr: 0.000035
2022-09-16 02:12:41,340 - INFO - joeynmt.training - Epoch   1, Step:   242500, Batch Loss:     2.551568, Batch Acc: 0.005069, Tokens per Sec:     5139, Lr: 0.000035
2022-09-16 02:13:04,220 - INFO - joeynmt.training - Epoch   1, Step:   242600, Batch Loss:     2.605996, Batch Acc: 0.004888, Tokens per Sec:     5017, Lr: 0.000035
2022-09-16 02:13:26,861 - INFO - joeynmt.training - Epoch   1, Step:   242700, Batch Loss:     2.571330, Batch Acc: 0.004796, Tokens per Sec:     5000, Lr: 0.000035
2022-09-16 02:13:49,498 - INFO - joeynmt.training - Epoch   1, Step:   242800, Batch Loss:     2.779245, Batch Acc: 0.004211, Tokens per Sec:     5057, Lr: 0.000035
2022-09-16 02:14:12,295 - INFO - joeynmt.training - Epoch   1, Step:   242900, Batch Loss:     2.563565, Batch Acc: 0.004531, Tokens per Sec:     5083, Lr: 0.000035
2022-09-16 02:14:34,880 - INFO - joeynmt.training - Epoch   1, Step:   243000, Batch Loss:     2.608133, Batch Acc: 0.004036, Tokens per Sec:     5068, Lr: 0.000035
2022-09-16 02:14:34,881 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 02:30:12,591 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 02:30:12,592 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.04, loss:   2.51, ppl:  12.34, acc:   0.51, generation: 930.2350[sec], evaluation: 6.6842[sec]
2022-09-16 02:30:12,596 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 02:30:14,076 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/242000.ckpt
2022-09-16 02:30:14,118 - INFO - joeynmt.training - Example #0
2022-09-16 02:30:14,131 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 02:30:14,131 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 02:30:14,131 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 02:30:14,131 - INFO - joeynmt.training - Example #1
2022-09-16 02:30:14,142 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 02:30:14,142 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 02:30:14,142 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 02:30:14,142 - INFO - joeynmt.training - Example #2
2022-09-16 02:30:14,153 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 02:30:14,153 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 02:30:14,153 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 02:30:14,153 - INFO - joeynmt.training - Example #3
2022-09-16 02:30:14,164 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 02:30:14,164 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 02:30:14,164 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 02:30:37,019 - INFO - joeynmt.training - Epoch   1, Step:   243100, Batch Loss:     2.605855, Batch Acc: 0.004284, Tokens per Sec:     4683, Lr: 0.000035
2022-09-16 02:30:59,599 - INFO - joeynmt.training - Epoch   1, Step:   243200, Batch Loss:     2.483249, Batch Acc: 0.005502, Tokens per Sec:     5031, Lr: 0.000035
2022-09-16 02:31:22,237 - INFO - joeynmt.training - Epoch   1, Step:   243300, Batch Loss:     2.615170, Batch Acc: 0.005788, Tokens per Sec:     4999, Lr: 0.000035
2022-09-16 02:31:44,957 - INFO - joeynmt.training - Epoch   1, Step:   243400, Batch Loss:     2.644083, Batch Acc: 0.003883, Tokens per Sec:     5021, Lr: 0.000035
2022-09-16 02:32:07,702 - INFO - joeynmt.training - Epoch   1, Step:   243500, Batch Loss:     2.947976, Batch Acc: 0.004514, Tokens per Sec:     5065, Lr: 0.000035
2022-09-16 02:32:30,459 - INFO - joeynmt.training - Epoch   1, Step:   243600, Batch Loss:     2.881059, Batch Acc: 0.004871, Tokens per Sec:     5061, Lr: 0.000035
2022-09-16 02:32:53,144 - INFO - joeynmt.training - Epoch   1, Step:   243700, Batch Loss:     2.493260, Batch Acc: 0.004801, Tokens per Sec:     5005, Lr: 0.000035
2022-09-16 02:33:15,893 - INFO - joeynmt.training - Epoch   1, Step:   243800, Batch Loss:     2.573376, Batch Acc: 0.004213, Tokens per Sec:     5040, Lr: 0.000035
2022-09-16 02:33:38,522 - INFO - joeynmt.training - Epoch   1, Step:   243900, Batch Loss:     2.643811, Batch Acc: 0.004693, Tokens per Sec:     5028, Lr: 0.000035
2022-09-16 02:34:01,195 - INFO - joeynmt.training - Epoch   1, Step:   244000, Batch Loss:     2.601630, Batch Acc: 0.004585, Tokens per Sec:     5002, Lr: 0.000035
2022-09-16 02:34:01,196 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 02:49:21,927 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 02:49:21,928 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.16, loss:   2.51, ppl:  12.28, acc:   0.51, generation: 913.6624[sec], evaluation: 6.7259[sec]
2022-09-16 02:49:21,932 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 02:49:23,415 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/240000.ckpt
2022-09-16 02:49:23,457 - INFO - joeynmt.training - Example #0
2022-09-16 02:49:23,469 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 02:49:23,470 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 02:49:23,470 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 02:49:23,470 - INFO - joeynmt.training - Example #1
2022-09-16 02:49:23,480 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 02:49:23,480 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 02:49:23,480 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 02:49:23,480 - INFO - joeynmt.training - Example #2
2022-09-16 02:49:23,491 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 02:49:23,491 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 02:49:23,491 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 02:49:23,491 - INFO - joeynmt.training - Example #3
2022-09-16 02:49:23,502 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 02:49:23,502 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 02:49:23,502 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 02:49:46,405 - INFO - joeynmt.training - Epoch   1, Step:   244100, Batch Loss:     2.622981, Batch Acc: 0.004298, Tokens per Sec:     4754, Lr: 0.000035
2022-09-16 02:50:09,092 - INFO - joeynmt.training - Epoch   1, Step:   244200, Batch Loss:     2.671213, Batch Acc: 0.004912, Tokens per Sec:     5124, Lr: 0.000035
2022-09-16 02:50:31,647 - INFO - joeynmt.training - Epoch   1, Step:   244300, Batch Loss:     2.729662, Batch Acc: 0.004396, Tokens per Sec:     5063, Lr: 0.000035
2022-09-16 02:50:54,345 - INFO - joeynmt.training - Epoch   1, Step:   244400, Batch Loss:     2.663724, Batch Acc: 0.004723, Tokens per Sec:     5028, Lr: 0.000035
2022-09-16 02:51:17,031 - INFO - joeynmt.training - Epoch   1, Step:   244500, Batch Loss:     2.727729, Batch Acc: 0.004580, Tokens per Sec:     5053, Lr: 0.000035
2022-09-16 02:51:39,767 - INFO - joeynmt.training - Epoch   1, Step:   244600, Batch Loss:     2.671950, Batch Acc: 0.005214, Tokens per Sec:     5129, Lr: 0.000035
2022-09-16 02:52:02,550 - INFO - joeynmt.training - Epoch   1, Step:   244700, Batch Loss:     2.641871, Batch Acc: 0.004169, Tokens per Sec:     5012, Lr: 0.000035
2022-09-16 02:52:25,239 - INFO - joeynmt.training - Epoch   1, Step:   244800, Batch Loss:     2.635784, Batch Acc: 0.005653, Tokens per Sec:     5021, Lr: 0.000035
2022-09-16 02:52:47,984 - INFO - joeynmt.training - Epoch   1, Step:   244900, Batch Loss:     2.655001, Batch Acc: 0.004803, Tokens per Sec:     5035, Lr: 0.000035
2022-09-16 02:53:10,732 - INFO - joeynmt.training - Epoch   1, Step:   245000, Batch Loss:     2.684512, Batch Acc: 0.004905, Tokens per Sec:     5028, Lr: 0.000035
2022-09-16 02:53:10,732 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 03:08:56,027 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 03:08:56,029 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.22, loss:   2.51, ppl:  12.26, acc:   0.52, generation: 938.3670[sec], evaluation: 6.5850[sec]
2022-09-16 03:08:56,032 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 03:08:57,627 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/241000.ckpt
2022-09-16 03:08:57,671 - INFO - joeynmt.training - Example #0
2022-09-16 03:08:57,683 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 03:08:57,683 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 03:08:57,683 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 03:08:57,683 - INFO - joeynmt.training - Example #1
2022-09-16 03:08:57,693 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 03:08:57,693 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 03:08:57,693 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन प्ले करें
2022-09-16 03:08:57,693 - INFO - joeynmt.training - Example #2
2022-09-16 03:08:57,704 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 03:08:57,704 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 03:08:57,704 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 03:08:57,704 - INFO - joeynmt.training - Example #3
2022-09-16 03:08:57,715 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 03:08:57,715 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 03:08:57,715 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 03:09:20,561 - INFO - joeynmt.training - Epoch   1, Step:   245100, Batch Loss:     2.641984, Batch Acc: 0.004362, Tokens per Sec:     4626, Lr: 0.000035
2022-09-16 03:09:43,184 - INFO - joeynmt.training - Epoch   1, Step:   245200, Batch Loss:     2.669905, Batch Acc: 0.004064, Tokens per Sec:     5003, Lr: 0.000035
2022-09-16 03:10:05,961 - INFO - joeynmt.training - Epoch   1, Step:   245300, Batch Loss:     2.598062, Batch Acc: 0.004518, Tokens per Sec:     5034, Lr: 0.000035
2022-09-16 03:10:28,663 - INFO - joeynmt.training - Epoch   1, Step:   245400, Batch Loss:     2.739339, Batch Acc: 0.004068, Tokens per Sec:     5132, Lr: 0.000035
2022-09-16 03:10:51,326 - INFO - joeynmt.training - Epoch   1, Step:   245500, Batch Loss:     2.659563, Batch Acc: 0.004258, Tokens per Sec:     5036, Lr: 0.000035
2022-09-16 03:11:13,926 - INFO - joeynmt.training - Epoch   1, Step:   245600, Batch Loss:     2.519011, Batch Acc: 0.003822, Tokens per Sec:     5048, Lr: 0.000035
2022-09-16 03:11:36,687 - INFO - joeynmt.training - Epoch   1, Step:   245700, Batch Loss:     2.532774, Batch Acc: 0.005338, Tokens per Sec:     5086, Lr: 0.000035
2022-09-16 03:11:59,388 - INFO - joeynmt.training - Epoch   1, Step:   245800, Batch Loss:     2.518682, Batch Acc: 0.005180, Tokens per Sec:     5085, Lr: 0.000035
2022-09-16 03:12:22,070 - INFO - joeynmt.training - Epoch   1, Step:   245900, Batch Loss:     2.676766, Batch Acc: 0.004934, Tokens per Sec:     5067, Lr: 0.000035
2022-09-16 03:12:44,846 - INFO - joeynmt.training - Epoch   1, Step:   246000, Batch Loss:     2.708326, Batch Acc: 0.004517, Tokens per Sec:     4967, Lr: 0.000035
2022-09-16 03:12:44,846 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 03:28:11,552 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 03:28:13,048 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/243000.ckpt
2022-09-16 03:28:13,092 - INFO - joeynmt.training - Example #0
2022-09-16 03:28:13,104 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 03:28:13,104 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 03:28:13,104 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 03:28:13,104 - INFO - joeynmt.training - Example #1
2022-09-16 03:28:13,115 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 03:28:13,115 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 03:28:13,115 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन प्ले करें
2022-09-16 03:28:13,115 - INFO - joeynmt.training - Example #2
2022-09-16 03:28:13,125 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 03:28:13,125 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 03:28:13,125 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 03:28:13,125 - INFO - joeynmt.training - Example #3
2022-09-16 03:28:13,136 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 03:28:13,136 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 03:28:13,136 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 03:28:35,996 - INFO - joeynmt.training - Epoch   1, Step:   246100, Batch Loss:     2.241699, Batch Acc: 0.004614, Tokens per Sec:     4683, Lr: 0.000035
2022-09-16 03:28:58,648 - INFO - joeynmt.training - Epoch   1, Step:   246200, Batch Loss:     2.771186, Batch Acc: 0.004780, Tokens per Sec:     5089, Lr: 0.000035
2022-09-16 03:29:21,383 - INFO - joeynmt.training - Epoch   1, Step:   246300, Batch Loss:     2.747429, Batch Acc: 0.004471, Tokens per Sec:     5028, Lr: 0.000035
2022-09-16 03:29:44,110 - INFO - joeynmt.training - Epoch   1, Step:   246400, Batch Loss:     2.796806, Batch Acc: 0.004566, Tokens per Sec:     5059, Lr: 0.000035
2022-09-16 03:30:06,796 - INFO - joeynmt.training - Epoch   1, Step:   246500, Batch Loss:     2.683833, Batch Acc: 0.004847, Tokens per Sec:     5002, Lr: 0.000035
2022-09-16 03:30:29,442 - INFO - joeynmt.training - Epoch   1, Step:   246600, Batch Loss:     2.733978, Batch Acc: 0.004342, Tokens per Sec:     5003, Lr: 0.000035
2022-09-16 03:30:52,037 - INFO - joeynmt.training - Epoch   1, Step:   246700, Batch Loss:     2.475032, Batch Acc: 0.004466, Tokens per Sec:     4945, Lr: 0.000035
2022-09-16 03:31:12,650 - INFO - joeynmt.training - Epoch   1: total training loss 38891.97
2022-09-16 03:31:12,650 - INFO - joeynmt.training - Training ended after   1 epochs.
2022-09-16 03:31:12,651 - INFO - joeynmt.training - Best validation result (greedy) at step   245000:  12.26 ppl.
2022-09-16 03:31:12,659 - INFO - joeynmt.training - Loading from ckpt file: /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/245000.ckpt
2022-09-16 03:31:12,674 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 03:31:12,674 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 03:31:12,887 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 03:31:12,890 - INFO - joeynmt.model - Total params: 19302144
2022-09-16 03:31:13,022 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/245000.ckpt.
2022-09-16 03:31:13,064 - INFO - joeynmt.prediction - Decoding on dev set...
2022-09-16 03:31:13,064 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 03:51:22,748 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 03:51:22,750 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  23.61, generation: 1202.6186[sec], evaluation: 6.7272[sec]
2022-09-16 03:51:22,845 - INFO - joeynmt.prediction - Translations saved to: /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/00245000.hyps.dev.
2022-09-16 03:51:22,846 - INFO - joeynmt.prediction - Decoding on test set...
2022-09-16 03:51:22,846 - INFO - joeynmt.prediction - Predicting 40858 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 04:11:01,362 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 04:11:01,364 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  22.93, generation: 1171.0092[sec], evaluation: 7.1734[sec]
2022-09-16 04:11:01,484 - INFO - joeynmt.prediction - Translations saved to: /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/00245000.hyps.test.
2022-09-16 04:11:01,492 - INFO - joeynmt.training - ACTIVE LEARNING MODEL END - MARGIN 2
2022-09-16 04:11:01,492 - INFO - joeynmt.training - ACTIVE LEARNING MODEL START - MARGIN 3
2022-09-16 04:11:01,493 - INFO - joeynmt.training - Random Indices picked: [121958 146867 131932 365838 259178 119879 110268 207892  54886 137337] length: 25553
2022-09-16 04:11:01,493 - INFO - joeynmt.training - Processing Predictions on Batch 0/100
2022-09-16 04:11:03,941 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 04:11:03,941 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 04:11:04,155 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 04:11:04,294 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/245000.ckpt.
2022-09-16 04:11:04,385 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 04:11:04,385 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 04:11:04,386 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 04:11:04,804 - INFO - joeynmt.prediction - Generation took 0.4176[sec]. (No references given)
2022-09-16 04:11:04,812 - INFO - joeynmt.training - Processing Predictions on Batch 1/100
2022-09-16 04:11:07,256 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 04:11:07,256 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 04:11:07,470 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 04:11:07,602 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/245000.ckpt.
2022-09-16 04:11:07,688 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 04:11:07,688 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:04:05,242 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).
2022-09-16 06:04:05,242 - INFO - joeynmt.helpers -                           cfg.name : transformer_100_enhi_bpe
2022-09-16 06:04:05,242 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2022-09-16 06:04:05,242 - INFO - joeynmt.helpers -                     cfg.data.train : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/train_tok
2022-09-16 06:04:05,242 - INFO - joeynmt.helpers -                       cfg.data.dev : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/val_tok
2022-09-16 06:04:05,242 - INFO - joeynmt.helpers -                      cfg.data.test : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/test_tok
2022-09-16 06:04:05,242 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain_ac
2022-09-16 06:04:05,242 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en
2022-09-16 06:04:05,242 - INFO - joeynmt.helpers -            cfg.data.src.max_length : 60
2022-09-16 06:04:05,242 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/vocab.en
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/en.bpe.codes
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 16000
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : moses
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : hi
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -            cfg.data.trg.max_length : 60
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/vocab.hi
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/hi.bpe.codes
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 16000
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -                 cfg.testing.n_best : 1
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -             cfg.testing.batch_size : 1024
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -             cfg.testing.batch_type : token
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -      cfg.testing.max_output_length : 130
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -           cfg.testing.eval_metrics : ['bleu']
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -            cfg.testing.return_prob : none
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -       cfg.testing.return_attention : False
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.tokenize : 13a
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.lowercase : False
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -     cfg.active_learning.query_size : 10000
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -    cfg.active_learning.interactive : False
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -      cfg.active_learning.pool_size : 6
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -     cfg.active_learning.batch_size : 256
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -    cfg.active_learning.num_workers : 4
2022-09-16 06:04:05,243 - INFO - joeynmt.helpers -    cfg.active_learning.num_queries : 5
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -     cfg.active_learning.al_percent : 30
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers - cfg.active_learning.query_strategy : margin
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -          cfg.active_learning.epoch : 1
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers - cfg.active_learning.validation_freq : 1000
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -            cfg.training.load_model : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/best.ckpt
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -       cfg.training.reset_best_ckpt : False
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -       cfg.training.reset_scheduler : False
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -       cfg.training.reset_optimizer : False
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -      cfg.training.reset_iter_state : False
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers - cfg.training.learning_rate_decay_length : 2500
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -    cfg.training.learning_rate_peak : 0.005
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -  cfg.training.learning_rate_warmup : 1000
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -  cfg.training.learning_rate_factor : 0.5
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -              cfg.training.patience : 5
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 1e-08
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.1
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -            cfg.training.batch_size : 4096
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 1
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -                cfg.training.epochs : 1
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 1000
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2022-09-16 06:04:05,244 - INFO - joeynmt.helpers -             cfg.training.model_dir : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3]
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 3
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : False
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 1024
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.3
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -       cfg.model.encoder.layer_norm : pre
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 4
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 1024
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.3
2022-09-16 06:04:05,245 - INFO - joeynmt.helpers -       cfg.model.decoder.layer_norm : pre
2022-09-16 06:04:06,330 - INFO - joeynmt.data - Building tokenizer...
2022-09-16 06:04:06,478 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:04:06,479 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:04:06,479 - INFO - joeynmt.data - Loading train set...
2022-09-16 06:07:05,803 - INFO - joeynmt.data - Building vocabulary...
2022-09-16 06:07:07,924 - INFO - joeynmt.data - Loading dev set...
2022-09-16 06:07:13,375 - INFO - joeynmt.data - Loading test set...
2022-09-16 06:07:18,743 - INFO - joeynmt.data - Data loaded.
2022-09-16 06:07:18,743 - INFO - joeynmt.helpers - Train dataset: PlaintextDatasetAC(split=train, len=1552563, src_lang=en, trg_lang=hi, has_trg=True, random_subset=-1)
2022-09-16 06:07:18,743 - INFO - joeynmt.helpers - Valid dataset: PlaintextDatasetAC(split=dev, len=40856, src_lang=en, trg_lang=hi, has_trg=True, random_subset=-1)
2022-09-16 06:07:18,743 - INFO - joeynmt.helpers -  Test dataset: PlaintextDatasetAC(split=test, len=40858, src_lang=en, trg_lang=hi, has_trg=True, random_subset=-1)
2022-09-16 06:07:18,744 - INFO - joeynmt.helpers - First training example:
	[SRC] give your application an accessibility work@@ out
	[TRG] अपने अनुप्रयोग को पहुंच@@ नीयता व्यायाम का लाभ दें
2022-09-16 06:07:18,744 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) , (6) . (7) of (8) and (9) to
2022-09-16 06:07:18,744 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) के (5) है (6) । (7) , (8) और (9) में
2022-09-16 06:07:18,744 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 16021
2022-09-16 06:07:18,744 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 16174
2022-09-16 06:07:18,834 - INFO - joeynmt.training - BASELINE MODEL START
2022-09-16 06:07:18,834 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:07:18,834 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:07:19,156 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:07:22,463 - INFO - joeynmt.model - Total params: 19302144
2022-09-16 06:07:22,464 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm="pre"),
	decoder=TransformerDecoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm="pre"),
	src_embed=Embeddings(embedding_dim=256, vocab_size=16021),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=16174),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.1))
2022-09-16 06:07:24,577 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=[0.9, 0.999])
2022-09-16 06:07:24,577 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=5)
2022-09-16 06:07:24,577 - INFO - joeynmt.training - Loading model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/best.ckpt
2022-09-16 06:07:24,819 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:07:24,874 - INFO - joeynmt.training - BASELINE MODEL END
2022-09-16 06:07:24,875 - INFO - joeynmt.training - Random Indices picked: [121958 146867 131932 365838 259178 119879 110268 207892  54886 137337] length: 27946
2022-09-16 06:07:24,875 - INFO - joeynmt.training - Executing Random Strategy
2022-09-16 06:07:24,876 - INFO - joeynmt.training - Final Query Indices picked: [121958, 146867, 131932, 365838, 259178, 119879, 110268, 207892, 54886, 137337] length: 10000
2022-09-16 06:07:24,876 - INFO - joeynmt.training - Query the samples, one at a time (interactive/file)
2022-09-16 06:07:26,482 - INFO - joeynmt.training - ACTIVE LEARNING MODEL END - RANDOM
2022-09-16 06:07:26,483 - INFO - joeynmt.training - ACTIVE LEARNING MODEL START - MARGIN 0
2022-09-16 06:07:26,484 - INFO - joeynmt.training - Random Indices picked: [ 21058 388370 228845 308840 240035  38872  54971 344227  23306 421426] length: 27353
2022-09-16 06:07:26,484 - INFO - joeynmt.training - Processing Predictions on Batch 0/107
2022-09-16 06:07:28,730 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:07:28,730 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:07:29,020 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:07:29,157 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:07:29,297 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:07:29,297 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:07:29,326 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:07:35,098 - INFO - joeynmt.prediction - Generation took 5.7637[sec]. (No references given)
2022-09-16 06:07:35,105 - INFO - joeynmt.training - Processing Predictions on Batch 1/107
2022-09-16 06:07:37,304 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:07:37,304 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:07:37,581 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:07:37,713 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:07:37,797 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:07:37,797 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:07:37,827 - INFO - joeynmt.prediction - Predicting 248 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:07:43,748 - INFO - joeynmt.prediction - Generation took 5.9132[sec]. (No references given)
2022-09-16 06:07:43,757 - INFO - joeynmt.training - Processing Predictions on Batch 2/107
2022-09-16 06:07:45,977 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:07:45,977 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:07:46,250 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:07:46,385 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:07:46,471 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:07:46,471 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:07:46,500 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:07:52,495 - INFO - joeynmt.prediction - Generation took 5.9871[sec]. (No references given)
2022-09-16 06:07:52,504 - INFO - joeynmt.training - Processing Predictions on Batch 3/107
2022-09-16 06:07:54,730 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:07:54,730 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:07:55,003 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:07:55,138 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:07:55,223 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:07:55,223 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:07:55,253 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:08:00,979 - INFO - joeynmt.prediction - Generation took 5.7178[sec]. (No references given)
2022-09-16 06:08:00,988 - INFO - joeynmt.training - Processing Predictions on Batch 4/107
2022-09-16 06:08:03,223 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:08:03,223 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:08:03,496 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:08:03,628 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:08:03,714 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:08:03,714 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:08:03,742 - INFO - joeynmt.prediction - Predicting 251 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:08:09,300 - INFO - joeynmt.prediction - Generation took 5.5503[sec]. (No references given)
2022-09-16 06:08:09,309 - INFO - joeynmt.training - Processing Predictions on Batch 5/107
2022-09-16 06:08:11,562 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:08:11,562 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:08:11,835 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:08:12,025 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:08:12,110 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:08:12,110 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:08:12,137 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:08:19,278 - INFO - joeynmt.prediction - Generation took 7.1333[sec]. (No references given)
2022-09-16 06:08:19,287 - INFO - joeynmt.training - Processing Predictions on Batch 6/107
2022-09-16 06:08:21,558 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:08:21,558 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:08:21,832 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:08:21,965 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:08:22,051 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:08:22,051 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:08:22,084 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:08:28,171 - INFO - joeynmt.prediction - Generation took 6.0786[sec]. (No references given)
2022-09-16 06:08:28,180 - INFO - joeynmt.training - Processing Predictions on Batch 7/107
2022-09-16 06:08:30,451 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:08:30,451 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:08:30,726 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:08:30,861 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:08:30,948 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:08:30,948 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:08:30,975 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:08:37,744 - INFO - joeynmt.prediction - Generation took 6.7603[sec]. (No references given)
2022-09-16 06:08:37,753 - INFO - joeynmt.training - Processing Predictions on Batch 8/107
2022-09-16 06:08:40,022 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:08:40,022 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:08:40,296 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:08:40,429 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:08:40,514 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:08:40,514 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:08:40,544 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:08:46,870 - INFO - joeynmt.prediction - Generation took 6.3172[sec]. (No references given)
2022-09-16 06:08:46,878 - INFO - joeynmt.training - Processing Predictions on Batch 9/107
2022-09-16 06:08:49,148 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:08:49,148 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:08:49,422 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:08:49,555 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:08:49,642 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:08:49,642 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:08:49,674 - INFO - joeynmt.prediction - Predicting 250 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:08:56,040 - INFO - joeynmt.prediction - Generation took 6.3576[sec]. (No references given)
2022-09-16 06:08:56,049 - INFO - joeynmt.training - Processing Predictions on Batch 10/107
2022-09-16 06:08:58,312 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:08:58,312 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:08:58,585 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:08:58,720 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:08:58,860 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:08:58,861 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:08:58,890 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:09:05,267 - INFO - joeynmt.prediction - Generation took 6.3684[sec]. (No references given)
2022-09-16 06:09:05,276 - INFO - joeynmt.training - Processing Predictions on Batch 11/107
2022-09-16 06:09:07,512 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:09:07,512 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:09:07,786 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:09:07,920 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:09:08,004 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:09:08,005 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:09:08,036 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:09:15,479 - INFO - joeynmt.prediction - Generation took 7.4340[sec]. (No references given)
2022-09-16 06:09:15,488 - INFO - joeynmt.training - Processing Predictions on Batch 12/107
2022-09-16 06:09:17,716 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:09:17,716 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:09:17,989 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:09:18,123 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:09:18,208 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:09:18,208 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:09:18,240 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:09:24,702 - INFO - joeynmt.prediction - Generation took 6.4533[sec]. (No references given)
2022-09-16 06:09:24,711 - INFO - joeynmt.training - Processing Predictions on Batch 13/107
2022-09-16 06:09:26,951 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:09:26,951 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:09:27,227 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:09:27,419 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:09:27,504 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:09:27,504 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:09:27,533 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:09:33,855 - INFO - joeynmt.prediction - Generation took 6.3137[sec]. (No references given)
2022-09-16 06:09:33,863 - INFO - joeynmt.training - Processing Predictions on Batch 14/107
2022-09-16 06:09:36,112 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:09:36,112 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:09:36,387 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:09:36,522 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:09:36,608 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:09:36,609 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:09:36,640 - INFO - joeynmt.prediction - Predicting 248 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:09:43,766 - INFO - joeynmt.prediction - Generation took 7.1170[sec]. (No references given)
2022-09-16 06:09:43,774 - INFO - joeynmt.training - Processing Predictions on Batch 15/107
2022-09-16 06:09:46,038 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:09:46,038 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:09:46,312 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:09:46,447 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:09:46,532 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:09:46,532 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:09:46,561 - INFO - joeynmt.prediction - Predicting 251 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:09:54,327 - INFO - joeynmt.prediction - Generation took 7.7574[sec]. (No references given)
2022-09-16 06:09:54,336 - INFO - joeynmt.training - Processing Predictions on Batch 16/107
2022-09-16 06:09:56,618 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:09:56,619 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:09:56,892 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:09:57,027 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:09:57,111 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:09:57,111 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:09:57,140 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:10:04,538 - INFO - joeynmt.prediction - Generation took 7.3888[sec]. (No references given)
2022-09-16 06:10:04,546 - INFO - joeynmt.training - Processing Predictions on Batch 17/107
2022-09-16 06:10:06,828 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:10:06,828 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:10:07,101 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:10:07,236 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:10:07,320 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:10:07,321 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:10:07,350 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:10:15,773 - INFO - joeynmt.prediction - Generation took 8.4154[sec]. (No references given)
2022-09-16 06:10:15,782 - INFO - joeynmt.training - Processing Predictions on Batch 18/107
2022-09-16 06:10:18,121 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:10:18,121 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:10:18,451 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:10:18,586 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:10:18,670 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:10:18,670 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:10:18,703 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:10:27,241 - INFO - joeynmt.prediction - Generation took 8.5289[sec]. (No references given)
2022-09-16 06:10:27,250 - INFO - joeynmt.training - Processing Predictions on Batch 19/107
2022-09-16 06:10:29,568 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:10:29,568 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:10:29,843 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:10:29,975 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:10:30,061 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:10:30,061 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:10:30,092 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:10:36,511 - INFO - joeynmt.prediction - Generation took 6.4112[sec]. (No references given)
2022-09-16 06:10:36,520 - INFO - joeynmt.training - Processing Predictions on Batch 20/107
2022-09-16 06:10:38,853 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:10:38,853 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:10:39,128 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:10:39,261 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:10:39,349 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:10:39,349 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:10:39,378 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:10:45,761 - INFO - joeynmt.prediction - Generation took 6.3744[sec]. (No references given)
2022-09-16 06:10:45,769 - INFO - joeynmt.training - Processing Predictions on Batch 21/107
2022-09-16 06:10:48,083 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:10:48,083 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:10:48,358 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:10:48,495 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:10:48,579 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:10:48,579 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:10:48,608 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:10:55,799 - INFO - joeynmt.prediction - Generation took 7.1820[sec]. (No references given)
2022-09-16 06:10:55,808 - INFO - joeynmt.training - Processing Predictions on Batch 22/107
2022-09-16 06:10:58,132 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:10:58,132 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:10:58,407 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:10:58,541 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:10:58,627 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:10:58,627 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:10:58,656 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:11:04,558 - INFO - joeynmt.prediction - Generation took 5.8947[sec]. (No references given)
2022-09-16 06:11:04,567 - INFO - joeynmt.training - Processing Predictions on Batch 23/107
2022-09-16 06:11:06,894 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:11:06,894 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:11:07,167 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:11:07,301 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:11:07,387 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:11:07,387 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:11:07,415 - INFO - joeynmt.prediction - Predicting 248 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:11:14,130 - INFO - joeynmt.prediction - Generation took 6.7074[sec]. (No references given)
2022-09-16 06:11:14,139 - INFO - joeynmt.training - Processing Predictions on Batch 24/107
2022-09-16 06:11:16,454 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:11:16,454 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:11:16,728 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:11:16,864 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:11:16,949 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:11:16,949 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:11:16,979 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:11:24,092 - INFO - joeynmt.prediction - Generation took 7.1053[sec]. (No references given)
2022-09-16 06:11:24,101 - INFO - joeynmt.training - Processing Predictions on Batch 25/107
2022-09-16 06:11:26,417 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:11:26,418 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:11:26,692 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:11:26,826 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:11:26,911 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:11:26,911 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:11:26,940 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:11:33,848 - INFO - joeynmt.prediction - Generation took 6.9003[sec]. (No references given)
2022-09-16 06:11:33,856 - INFO - joeynmt.training - Processing Predictions on Batch 26/107
2022-09-16 06:11:36,189 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:11:36,190 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:11:36,464 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:11:36,599 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:11:36,684 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:11:36,684 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:11:36,713 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:11:42,860 - INFO - joeynmt.prediction - Generation took 6.1380[sec]. (No references given)
2022-09-16 06:11:42,868 - INFO - joeynmt.training - Processing Predictions on Batch 27/107
2022-09-16 06:11:45,241 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:11:45,241 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:11:45,515 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:11:45,649 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:11:45,735 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:11:45,735 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:11:45,764 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:11:52,346 - INFO - joeynmt.prediction - Generation took 6.5740[sec]. (No references given)
2022-09-16 06:11:52,355 - INFO - joeynmt.training - Processing Predictions on Batch 28/107
2022-09-16 06:11:54,755 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:11:54,755 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:11:55,029 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:11:55,162 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:11:55,248 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:11:55,248 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:11:55,276 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:12:01,469 - INFO - joeynmt.prediction - Generation took 6.1854[sec]. (No references given)
2022-09-16 06:12:01,478 - INFO - joeynmt.training - Processing Predictions on Batch 29/107
2022-09-16 06:12:03,802 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:12:03,802 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:12:04,077 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:12:04,213 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:12:04,296 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:12:04,296 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:12:04,327 - INFO - joeynmt.prediction - Predicting 251 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:12:11,807 - INFO - joeynmt.prediction - Generation took 7.4713[sec]. (No references given)
2022-09-16 06:12:11,815 - INFO - joeynmt.training - Processing Predictions on Batch 30/107
2022-09-16 06:12:14,144 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:12:14,144 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:12:14,420 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:12:14,553 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:12:14,638 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:12:14,638 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:12:14,669 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:12:21,768 - INFO - joeynmt.prediction - Generation took 7.0898[sec]. (No references given)
2022-09-16 06:12:21,776 - INFO - joeynmt.training - Processing Predictions on Batch 31/107
2022-09-16 06:12:24,103 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:12:24,103 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:12:24,377 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:12:24,511 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:12:24,596 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:12:24,596 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:12:24,625 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:12:31,567 - INFO - joeynmt.prediction - Generation took 6.9338[sec]. (No references given)
2022-09-16 06:12:31,576 - INFO - joeynmt.training - Processing Predictions on Batch 32/107
2022-09-16 06:12:33,872 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:12:33,872 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:12:34,148 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:12:34,283 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:12:34,366 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:12:34,366 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:12:34,397 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:12:42,289 - INFO - joeynmt.prediction - Generation took 7.8843[sec]. (No references given)
2022-09-16 06:12:42,298 - INFO - joeynmt.training - Processing Predictions on Batch 33/107
2022-09-16 06:12:44,614 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:12:44,615 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:12:44,890 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:12:45,025 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:12:45,109 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:12:45,109 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:12:45,140 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:12:51,312 - INFO - joeynmt.prediction - Generation took 6.1644[sec]. (No references given)
2022-09-16 06:12:51,321 - INFO - joeynmt.training - Processing Predictions on Batch 34/107
2022-09-16 06:12:53,639 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:12:53,639 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:12:53,915 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:12:54,049 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:12:54,136 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:12:54,136 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:12:54,167 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:13:01,461 - INFO - joeynmt.prediction - Generation took 7.2857[sec]. (No references given)
2022-09-16 06:13:01,470 - INFO - joeynmt.training - Processing Predictions on Batch 35/107
2022-09-16 06:13:03,796 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:13:03,796 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:13:04,069 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:13:04,204 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:13:04,289 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:13:04,289 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:13:04,319 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:13:11,194 - INFO - joeynmt.prediction - Generation took 6.8671[sec]. (No references given)
2022-09-16 06:13:11,203 - INFO - joeynmt.training - Processing Predictions on Batch 36/107
2022-09-16 06:13:13,535 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:13:13,535 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:13:13,809 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:13:13,945 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:13:14,029 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:13:14,029 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:13:14,058 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:13:22,026 - INFO - joeynmt.prediction - Generation took 7.9591[sec]. (No references given)
2022-09-16 06:13:22,035 - INFO - joeynmt.training - Processing Predictions on Batch 37/107
2022-09-16 06:13:24,352 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:13:24,352 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:13:24,626 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:13:24,761 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:13:24,846 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:13:24,846 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:13:24,878 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:13:31,912 - INFO - joeynmt.prediction - Generation took 7.0253[sec]. (No references given)
2022-09-16 06:13:31,920 - INFO - joeynmt.training - Processing Predictions on Batch 38/107
2022-09-16 06:13:34,229 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:13:34,229 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:13:34,502 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:13:34,636 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:13:34,720 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:13:34,721 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:13:34,750 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:13:39,541 - INFO - joeynmt.prediction - Generation took 4.7835[sec]. (No references given)
2022-09-16 06:13:39,550 - INFO - joeynmt.training - Processing Predictions on Batch 39/107
2022-09-16 06:13:41,863 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:13:41,863 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:13:42,136 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:13:42,336 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:13:42,420 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:13:42,420 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:13:42,453 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:13:52,071 - INFO - joeynmt.prediction - Generation took 9.6086[sec]. (No references given)
2022-09-16 06:13:52,080 - INFO - joeynmt.training - Processing Predictions on Batch 40/107
2022-09-16 06:13:54,393 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:13:54,393 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:13:54,667 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:13:54,801 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:13:54,888 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:13:54,888 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:13:54,919 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:14:03,055 - INFO - joeynmt.prediction - Generation took 8.1271[sec]. (No references given)
2022-09-16 06:14:03,064 - INFO - joeynmt.training - Processing Predictions on Batch 41/107
2022-09-16 06:14:05,390 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:14:05,390 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:14:05,665 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:14:05,800 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:14:05,884 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:14:05,884 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:14:05,916 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:14:13,074 - INFO - joeynmt.prediction - Generation took 7.1504[sec]. (No references given)
2022-09-16 06:14:13,083 - INFO - joeynmt.training - Processing Predictions on Batch 42/107
2022-09-16 06:14:15,402 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:14:15,402 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:14:15,675 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:14:15,875 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:14:15,959 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:14:15,959 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:14:15,987 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:14:22,152 - INFO - joeynmt.prediction - Generation took 6.1569[sec]. (No references given)
2022-09-16 06:14:22,161 - INFO - joeynmt.training - Processing Predictions on Batch 43/107
2022-09-16 06:14:24,487 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:14:24,487 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:14:24,761 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:14:24,895 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:14:24,979 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:14:24,980 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:14:25,013 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:14:32,290 - INFO - joeynmt.prediction - Generation took 7.2688[sec]. (No references given)
2022-09-16 06:14:32,299 - INFO - joeynmt.training - Processing Predictions on Batch 44/107
2022-09-16 06:14:34,600 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:14:34,600 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:14:34,873 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:14:35,007 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:14:35,092 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:14:35,092 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:14:35,123 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:14:42,373 - INFO - joeynmt.prediction - Generation took 7.2416[sec]. (No references given)
2022-09-16 06:14:42,382 - INFO - joeynmt.training - Processing Predictions on Batch 45/107
2022-09-16 06:14:44,679 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:14:44,679 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:14:44,956 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:14:45,091 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:14:45,239 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:14:45,239 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:14:45,267 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:14:51,206 - INFO - joeynmt.prediction - Generation took 5.9317[sec]. (No references given)
2022-09-16 06:14:51,215 - INFO - joeynmt.training - Processing Predictions on Batch 46/107
2022-09-16 06:14:53,506 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:14:53,506 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:14:53,780 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:14:53,914 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:14:53,998 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:14:53,998 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:14:54,028 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:15:00,682 - INFO - joeynmt.prediction - Generation took 6.6461[sec]. (No references given)
2022-09-16 06:15:00,691 - INFO - joeynmt.training - Processing Predictions on Batch 47/107
2022-09-16 06:15:02,999 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:15:02,999 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:15:03,272 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:15:03,406 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:15:03,493 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:15:03,493 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:15:03,520 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:15:08,686 - INFO - joeynmt.prediction - Generation took 5.1578[sec]. (No references given)
2022-09-16 06:15:08,694 - INFO - joeynmt.training - Processing Predictions on Batch 48/107
2022-09-16 06:15:11,004 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:15:11,004 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:15:11,278 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:15:11,414 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:15:11,562 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:15:11,562 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:15:11,592 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:15:17,883 - INFO - joeynmt.prediction - Generation took 6.2836[sec]. (No references given)
2022-09-16 06:15:17,892 - INFO - joeynmt.training - Processing Predictions on Batch 49/107
2022-09-16 06:15:20,201 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:15:20,201 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:15:20,474 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:15:20,608 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:15:20,693 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:15:20,693 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:15:20,725 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:15:27,016 - INFO - joeynmt.prediction - Generation took 6.2829[sec]. (No references given)
2022-09-16 06:15:27,024 - INFO - joeynmt.training - Processing Predictions on Batch 50/107
2022-09-16 06:15:29,336 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:15:29,336 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:15:29,611 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:15:29,745 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:15:29,830 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:15:29,830 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:15:29,862 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:15:39,252 - INFO - joeynmt.prediction - Generation took 9.3820[sec]. (No references given)
2022-09-16 06:15:39,261 - INFO - joeynmt.training - Processing Predictions on Batch 51/107
2022-09-16 06:15:41,573 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:15:41,573 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:15:41,847 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:15:41,981 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:15:42,131 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:15:42,131 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:15:42,157 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:15:47,874 - INFO - joeynmt.prediction - Generation took 5.7100[sec]. (No references given)
2022-09-16 06:15:47,883 - INFO - joeynmt.training - Processing Predictions on Batch 52/107
2022-09-16 06:15:50,198 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:15:50,198 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:15:50,472 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:15:50,607 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:15:50,691 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:15:50,691 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:15:50,722 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:15:57,359 - INFO - joeynmt.prediction - Generation took 6.6281[sec]. (No references given)
2022-09-16 06:15:57,367 - INFO - joeynmt.training - Processing Predictions on Batch 53/107
2022-09-16 06:15:59,677 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:15:59,677 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:15:59,949 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:16:00,084 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:16:00,167 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:16:00,167 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:16:00,198 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:16:07,763 - INFO - joeynmt.prediction - Generation took 7.5568[sec]. (No references given)
2022-09-16 06:16:07,772 - INFO - joeynmt.training - Processing Predictions on Batch 54/107
2022-09-16 06:16:10,083 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:16:10,083 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:16:10,357 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:16:10,559 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:16:10,645 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:16:10,645 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:16:10,677 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:16:19,802 - INFO - joeynmt.prediction - Generation took 9.1161[sec]. (No references given)
2022-09-16 06:16:19,811 - INFO - joeynmt.training - Processing Predictions on Batch 55/107
2022-09-16 06:16:22,133 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:16:22,133 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:16:22,407 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:16:22,540 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:16:22,625 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:16:22,625 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:16:22,654 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:16:29,232 - INFO - joeynmt.prediction - Generation took 6.5702[sec]. (No references given)
2022-09-16 06:16:29,240 - INFO - joeynmt.training - Processing Predictions on Batch 56/107
2022-09-16 06:16:31,569 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:16:31,569 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:16:31,842 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:16:31,976 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:16:32,062 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:16:32,062 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:16:32,092 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:16:37,137 - INFO - joeynmt.prediction - Generation took 5.0371[sec]. (No references given)
2022-09-16 06:16:37,145 - INFO - joeynmt.training - Processing Predictions on Batch 57/107
2022-09-16 06:16:39,453 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:16:39,453 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:16:39,727 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:16:39,863 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:16:40,013 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:16:40,013 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:16:40,043 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:16:49,159 - INFO - joeynmt.prediction - Generation took 9.1069[sec]. (No references given)
2022-09-16 06:16:49,167 - INFO - joeynmt.training - Processing Predictions on Batch 58/107
2022-09-16 06:16:51,468 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:16:51,468 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:16:51,742 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:16:51,878 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:16:51,962 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:16:51,962 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:16:51,993 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:16:57,985 - INFO - joeynmt.prediction - Generation took 5.9837[sec]. (No references given)
2022-09-16 06:16:57,993 - INFO - joeynmt.training - Processing Predictions on Batch 59/107
2022-09-16 06:17:00,305 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:17:00,305 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:17:00,580 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:17:00,713 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:17:00,798 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:17:00,798 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:17:00,825 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:17:08,670 - INFO - joeynmt.prediction - Generation took 7.8366[sec]. (No references given)
2022-09-16 06:17:08,678 - INFO - joeynmt.training - Processing Predictions on Batch 60/107
2022-09-16 06:17:10,976 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:17:10,976 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:17:11,256 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:17:11,389 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:17:11,475 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:17:11,475 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:17:11,506 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:17:19,640 - INFO - joeynmt.prediction - Generation took 8.1257[sec]. (No references given)
2022-09-16 06:17:19,649 - INFO - joeynmt.training - Processing Predictions on Batch 61/107
2022-09-16 06:17:21,949 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:17:21,949 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:17:22,222 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:17:22,357 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:17:22,440 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:17:22,441 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:17:22,473 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:17:31,172 - INFO - joeynmt.prediction - Generation took 8.6901[sec]. (No references given)
2022-09-16 06:17:31,180 - INFO - joeynmt.training - Processing Predictions on Batch 62/107
2022-09-16 06:17:33,473 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:17:33,473 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:17:33,746 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:17:33,882 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:17:33,966 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:17:33,966 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:17:33,994 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:17:42,173 - INFO - joeynmt.prediction - Generation took 8.1707[sec]. (No references given)
2022-09-16 06:17:42,181 - INFO - joeynmt.training - Processing Predictions on Batch 63/107
2022-09-16 06:17:44,476 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:17:44,476 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:17:44,749 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:17:44,885 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:17:44,968 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:17:44,968 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:17:44,999 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:17:52,302 - INFO - joeynmt.prediction - Generation took 7.2937[sec]. (No references given)
2022-09-16 06:17:52,310 - INFO - joeynmt.training - Processing Predictions on Batch 64/107
2022-09-16 06:17:54,613 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:17:54,614 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:17:54,888 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:17:55,022 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:17:55,107 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:17:55,107 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:17:55,135 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:18:02,834 - INFO - joeynmt.prediction - Generation took 7.6904[sec]. (No references given)
2022-09-16 06:18:02,843 - INFO - joeynmt.training - Processing Predictions on Batch 65/107
2022-09-16 06:18:05,156 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:18:05,156 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:18:05,429 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:18:05,562 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:18:05,646 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:18:05,647 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:18:05,675 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:18:12,660 - INFO - joeynmt.prediction - Generation took 6.9768[sec]. (No references given)
2022-09-16 06:18:12,669 - INFO - joeynmt.training - Processing Predictions on Batch 66/107
2022-09-16 06:18:14,974 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:18:14,975 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:18:15,248 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:18:15,382 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:18:15,469 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:18:15,470 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:18:15,499 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:18:23,489 - INFO - joeynmt.prediction - Generation took 7.9821[sec]. (No references given)
2022-09-16 06:18:23,498 - INFO - joeynmt.training - Processing Predictions on Batch 67/107
2022-09-16 06:18:25,814 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:18:25,814 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:18:26,086 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:18:26,221 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:18:26,306 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:18:26,306 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:18:26,335 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:18:33,445 - INFO - joeynmt.prediction - Generation took 7.1022[sec]. (No references given)
2022-09-16 06:18:33,454 - INFO - joeynmt.training - Processing Predictions on Batch 68/107
2022-09-16 06:18:35,763 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:18:35,763 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:18:36,036 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:18:36,172 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:18:36,256 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:18:36,256 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:18:36,283 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:18:42,205 - INFO - joeynmt.prediction - Generation took 5.9140[sec]. (No references given)
2022-09-16 06:18:42,213 - INFO - joeynmt.training - Processing Predictions on Batch 69/107
2022-09-16 06:18:44,525 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:18:44,525 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:18:44,798 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:18:44,934 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:18:45,018 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:18:45,018 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:18:45,047 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:18:50,333 - INFO - joeynmt.prediction - Generation took 5.2783[sec]. (No references given)
2022-09-16 06:18:50,342 - INFO - joeynmt.training - Processing Predictions on Batch 70/107
2022-09-16 06:18:52,654 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:18:52,654 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:18:52,929 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:18:53,065 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:18:53,149 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:18:53,149 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:18:53,178 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:19:00,160 - INFO - joeynmt.prediction - Generation took 6.9741[sec]. (No references given)
2022-09-16 06:19:00,169 - INFO - joeynmt.training - Processing Predictions on Batch 71/107
2022-09-16 06:19:02,487 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:19:02,487 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:19:02,760 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:19:02,897 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:19:02,981 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:19:02,981 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:19:03,007 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:19:09,482 - INFO - joeynmt.prediction - Generation took 6.4674[sec]. (No references given)
2022-09-16 06:19:09,491 - INFO - joeynmt.training - Processing Predictions on Batch 72/107
2022-09-16 06:19:11,814 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:19:11,814 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:19:12,089 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:19:12,222 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:19:12,306 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:19:12,306 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:19:12,335 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:19:18,265 - INFO - joeynmt.prediction - Generation took 5.9223[sec]. (No references given)
2022-09-16 06:19:18,273 - INFO - joeynmt.training - Processing Predictions on Batch 73/107
2022-09-16 06:19:20,592 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:19:20,592 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:19:20,866 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:19:21,000 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:19:21,086 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:19:21,086 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:19:21,117 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:19:27,265 - INFO - joeynmt.prediction - Generation took 6.1395[sec]. (No references given)
2022-09-16 06:19:27,274 - INFO - joeynmt.training - Processing Predictions on Batch 74/107
2022-09-16 06:19:29,597 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:19:29,598 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:19:29,872 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:19:30,006 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:19:30,160 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:19:30,160 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:19:30,190 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:19:36,743 - INFO - joeynmt.prediction - Generation took 6.5454[sec]. (No references given)
2022-09-16 06:19:36,752 - INFO - joeynmt.training - Processing Predictions on Batch 75/107
2022-09-16 06:19:39,063 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:19:39,063 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:19:39,337 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:19:39,472 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:19:39,555 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:19:39,555 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:19:39,585 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:19:46,092 - INFO - joeynmt.prediction - Generation took 6.4986[sec]. (No references given)
2022-09-16 06:19:46,101 - INFO - joeynmt.training - Processing Predictions on Batch 76/107
2022-09-16 06:19:48,415 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:19:48,415 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:19:48,688 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:19:48,822 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:19:48,906 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:19:48,906 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:19:48,935 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:19:56,852 - INFO - joeynmt.prediction - Generation took 7.9097[sec]. (No references given)
2022-09-16 06:19:56,861 - INFO - joeynmt.training - Processing Predictions on Batch 77/107
2022-09-16 06:19:59,188 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:19:59,188 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:19:59,462 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:19:59,598 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:19:59,682 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:19:59,682 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:19:59,710 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:20:07,405 - INFO - joeynmt.prediction - Generation took 7.6873[sec]. (No references given)
2022-09-16 06:20:07,414 - INFO - joeynmt.training - Processing Predictions on Batch 78/107
2022-09-16 06:20:09,716 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:20:09,716 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:20:09,992 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:20:10,126 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:20:10,209 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:20:10,209 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:20:10,239 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:20:16,518 - INFO - joeynmt.prediction - Generation took 6.2709[sec]. (No references given)
2022-09-16 06:20:16,526 - INFO - joeynmt.training - Processing Predictions on Batch 79/107
2022-09-16 06:20:18,835 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:20:18,835 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:20:19,108 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:20:19,240 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:20:19,325 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:20:19,325 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:20:19,352 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:20:25,034 - INFO - joeynmt.prediction - Generation took 5.6693[sec]. (No references given)
2022-09-16 06:20:25,042 - INFO - joeynmt.training - Processing Predictions on Batch 80/107
2022-09-16 06:20:27,356 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:20:27,356 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:20:27,630 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:20:27,762 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:20:27,849 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:20:27,849 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:20:27,877 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:20:34,623 - INFO - joeynmt.prediction - Generation took 6.7371[sec]. (No references given)
2022-09-16 06:20:34,632 - INFO - joeynmt.training - Processing Predictions on Batch 81/107
2022-09-16 06:20:36,947 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:20:36,947 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:20:37,221 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:20:37,354 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:20:37,439 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:20:37,439 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:20:37,469 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:20:43,820 - INFO - joeynmt.prediction - Generation took 6.3433[sec]. (No references given)
2022-09-16 06:20:43,829 - INFO - joeynmt.training - Processing Predictions on Batch 82/107
2022-09-16 06:20:46,150 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:20:46,150 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:20:46,424 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:20:46,558 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:20:46,641 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:20:46,641 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:20:46,673 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:20:53,356 - INFO - joeynmt.prediction - Generation took 6.6745[sec]. (No references given)
2022-09-16 06:20:53,365 - INFO - joeynmt.training - Processing Predictions on Batch 83/107
2022-09-16 06:20:55,688 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:20:55,688 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:20:55,962 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:20:56,097 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:20:56,181 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:20:56,181 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:20:56,212 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:21:02,802 - INFO - joeynmt.prediction - Generation took 6.5813[sec]. (No references given)
2022-09-16 06:21:02,810 - INFO - joeynmt.training - Processing Predictions on Batch 84/107
2022-09-16 06:21:05,144 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:21:05,144 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:21:05,419 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:21:05,552 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:21:05,636 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:21:05,636 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:21:05,667 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:21:12,670 - INFO - joeynmt.prediction - Generation took 6.9946[sec]. (No references given)
2022-09-16 06:21:12,679 - INFO - joeynmt.training - Processing Predictions on Batch 85/107
2022-09-16 06:21:15,002 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:21:15,002 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:21:15,276 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:21:15,411 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:21:15,567 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:21:15,567 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:21:15,594 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:21:23,558 - INFO - joeynmt.prediction - Generation took 7.9566[sec]. (No references given)
2022-09-16 06:21:23,567 - INFO - joeynmt.training - Processing Predictions on Batch 86/107
2022-09-16 06:21:25,857 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:21:25,857 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:21:26,130 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:21:26,264 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:21:26,347 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:21:26,347 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:21:26,375 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:21:31,540 - INFO - joeynmt.prediction - Generation took 5.1572[sec]. (No references given)
2022-09-16 06:21:31,549 - INFO - joeynmt.training - Processing Predictions on Batch 87/107
2022-09-16 06:21:33,854 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:21:33,854 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:21:34,127 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:21:34,261 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:21:34,346 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:21:34,346 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:21:34,374 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:21:42,021 - INFO - joeynmt.prediction - Generation took 7.6388[sec]. (No references given)
2022-09-16 06:21:42,030 - INFO - joeynmt.training - Processing Predictions on Batch 88/107
2022-09-16 06:21:44,332 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:21:44,332 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:21:44,605 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:21:44,740 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:21:44,823 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:21:44,823 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:21:44,849 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:21:50,414 - INFO - joeynmt.prediction - Generation took 5.5568[sec]. (No references given)
2022-09-16 06:21:50,422 - INFO - joeynmt.training - Processing Predictions on Batch 89/107
2022-09-16 06:21:52,748 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:21:52,748 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:21:53,022 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:21:53,157 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:21:53,240 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:21:53,240 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:21:53,268 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:21:59,578 - INFO - joeynmt.prediction - Generation took 6.3012[sec]. (No references given)
2022-09-16 06:21:59,586 - INFO - joeynmt.training - Processing Predictions on Batch 90/107
2022-09-16 06:22:01,880 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:22:01,880 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:22:02,155 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:22:02,290 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:22:02,373 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:22:02,374 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:22:02,403 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:22:09,190 - INFO - joeynmt.prediction - Generation took 6.7786[sec]. (No references given)
2022-09-16 06:22:09,199 - INFO - joeynmt.training - Processing Predictions on Batch 91/107
2022-09-16 06:22:11,516 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:22:11,516 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:22:11,791 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:22:11,923 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:22:12,006 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:22:12,006 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:22:12,032 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:22:17,511 - INFO - joeynmt.prediction - Generation took 5.4710[sec]. (No references given)
2022-09-16 06:22:17,519 - INFO - joeynmt.training - Processing Predictions on Batch 92/107
2022-09-16 06:22:19,852 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:22:19,852 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:22:20,127 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:22:20,259 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:22:20,343 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:22:20,343 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:22:20,370 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:22:26,744 - INFO - joeynmt.prediction - Generation took 6.3656[sec]. (No references given)
2022-09-16 06:22:26,752 - INFO - joeynmt.training - Processing Predictions on Batch 93/107
2022-09-16 06:22:29,067 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:22:29,067 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:22:29,342 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:22:29,474 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:22:29,558 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:22:29,558 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:22:29,591 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:22:37,171 - INFO - joeynmt.prediction - Generation took 7.5713[sec]. (No references given)
2022-09-16 06:22:37,180 - INFO - joeynmt.training - Processing Predictions on Batch 94/107
2022-09-16 06:22:39,509 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:22:39,509 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:22:39,782 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:22:39,915 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:22:40,000 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:22:40,000 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:22:40,031 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:22:48,249 - INFO - joeynmt.prediction - Generation took 8.2091[sec]. (No references given)
2022-09-16 06:22:48,257 - INFO - joeynmt.training - Processing Predictions on Batch 95/107
2022-09-16 06:22:50,561 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:22:50,561 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:22:50,834 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:22:50,967 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:22:51,050 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:22:51,051 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:22:51,079 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:22:58,273 - INFO - joeynmt.prediction - Generation took 7.1850[sec]. (No references given)
2022-09-16 06:22:58,282 - INFO - joeynmt.training - Processing Predictions on Batch 96/107
2022-09-16 06:23:00,596 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:23:00,596 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:23:00,869 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:23:01,077 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:23:01,160 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:23:01,160 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:23:01,186 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:23:07,205 - INFO - joeynmt.prediction - Generation took 6.0118[sec]. (No references given)
2022-09-16 06:23:07,213 - INFO - joeynmt.training - Processing Predictions on Batch 97/107
2022-09-16 06:23:09,525 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:23:09,525 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:23:09,799 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:23:09,930 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:23:10,015 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:23:10,015 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:23:10,044 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:23:17,974 - INFO - joeynmt.prediction - Generation took 7.9217[sec]. (No references given)
2022-09-16 06:23:17,982 - INFO - joeynmt.training - Processing Predictions on Batch 98/107
2022-09-16 06:23:20,301 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:23:20,301 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:23:20,574 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:23:20,707 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:23:20,790 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:23:20,791 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:23:20,819 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:23:26,772 - INFO - joeynmt.prediction - Generation took 5.9449[sec]. (No references given)
2022-09-16 06:23:26,780 - INFO - joeynmt.training - Processing Predictions on Batch 99/107
2022-09-16 06:23:29,075 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:23:29,076 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:23:29,351 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:23:29,483 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:23:29,567 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:23:29,567 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:23:29,594 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:23:35,202 - INFO - joeynmt.prediction - Generation took 5.6005[sec]. (No references given)
2022-09-16 06:23:35,210 - INFO - joeynmt.training - Processing Predictions on Batch 100/107
2022-09-16 06:23:37,499 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:23:37,499 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:23:37,773 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:23:37,905 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:23:38,062 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:23:38,063 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:23:38,089 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:23:46,678 - INFO - joeynmt.prediction - Generation took 8.5801[sec]. (No references given)
2022-09-16 06:23:46,686 - INFO - joeynmt.training - Processing Predictions on Batch 101/107
2022-09-16 06:23:48,989 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:23:48,989 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:23:49,262 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:23:49,397 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:23:49,482 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:23:49,482 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:23:49,512 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:23:57,365 - INFO - joeynmt.prediction - Generation took 7.8444[sec]. (No references given)
2022-09-16 06:23:57,373 - INFO - joeynmt.training - Processing Predictions on Batch 102/107
2022-09-16 06:23:59,673 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:23:59,674 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:23:59,947 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:24:00,081 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:24:00,164 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:24:00,164 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:24:00,194 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:24:06,927 - INFO - joeynmt.prediction - Generation took 6.7242[sec]. (No references given)
2022-09-16 06:24:06,936 - INFO - joeynmt.training - Processing Predictions on Batch 103/107
2022-09-16 06:24:09,253 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:24:09,253 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:24:09,526 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:24:09,658 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:24:09,742 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:24:09,742 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:24:09,772 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:24:16,122 - INFO - joeynmt.prediction - Generation took 6.3414[sec]. (No references given)
2022-09-16 06:24:16,130 - INFO - joeynmt.training - Processing Predictions on Batch 104/107
2022-09-16 06:24:18,439 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:24:18,439 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:24:18,711 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:24:18,847 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:24:18,930 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:24:18,930 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:24:18,960 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:24:25,624 - INFO - joeynmt.prediction - Generation took 6.6563[sec]. (No references given)
2022-09-16 06:24:25,633 - INFO - joeynmt.training - Processing Predictions on Batch 105/107
2022-09-16 06:24:27,950 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:24:27,950 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:24:28,223 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:24:28,357 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:24:28,441 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:24:28,441 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:24:28,470 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:24:36,145 - INFO - joeynmt.prediction - Generation took 7.6678[sec]. (No references given)
2022-09-16 06:24:36,154 - INFO - joeynmt.training - Processing Predictions on Batch 106/107
2022-09-16 06:24:38,466 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 06:24:38,466 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 06:24:38,739 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 06:24:38,873 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin_2_done/245000.ckpt.
2022-09-16 06:24:38,956 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:24:38,956 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 06:24:38,982 - INFO - joeynmt.prediction - Predicting 205 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:24:44,345 - INFO - joeynmt.prediction - Generation took 5.3572[sec]. (No references given)
2022-09-16 06:24:44,515 - INFO - joeynmt.training - Final Query Indices picked: [367297, 219888, 26499, 411344, 251290, 420603, 442704, 437047, 384260, 122857] length: 10000
2022-09-16 06:24:44,515 - INFO - joeynmt.training - Query the samples, one at a time (interactive/file)
2022-09-16 06:24:46,687 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 4096
	effective batch size (w. parallel & accumulation): 4096
2022-09-16 06:24:46,687 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:40:34,015 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 06:40:34,017 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.22, loss:   2.51, ppl:  12.26, acc:   0.52, generation: 940.3491[sec], evaluation: 6.6387[sec]
2022-09-16 06:40:35,580 - INFO - joeynmt.training - Example #0
2022-09-16 06:40:35,594 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 06:40:35,594 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 06:40:35,594 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 06:40:35,594 - INFO - joeynmt.training - Example #1
2022-09-16 06:40:35,605 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 06:40:35,605 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 06:40:35,605 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन प्ले करें
2022-09-16 06:40:35,605 - INFO - joeynmt.training - Example #2
2022-09-16 06:40:35,616 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 06:40:35,617 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 06:40:35,617 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 06:40:35,617 - INFO - joeynmt.training - Example #3
2022-09-16 06:40:35,628 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 06:40:35,628 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 06:40:35,628 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 06:40:35,720 - INFO - joeynmt.training - EPOCH 1
2022-09-16 06:40:59,366 - INFO - joeynmt.training - Epoch   1, Step:   245100, Batch Loss:     2.562285, Batch Acc: 0.005249, Tokens per Sec:     4858, Lr: 0.000035
2022-09-16 06:41:22,430 - INFO - joeynmt.training - Epoch   1, Step:   245200, Batch Loss:     2.676063, Batch Acc: 0.004294, Tokens per Sec:     4958, Lr: 0.000035
2022-09-16 06:41:45,524 - INFO - joeynmt.training - Epoch   1, Step:   245300, Batch Loss:     2.626508, Batch Acc: 0.004931, Tokens per Sec:     4953, Lr: 0.000035
2022-09-16 06:42:08,636 - INFO - joeynmt.training - Epoch   1, Step:   245400, Batch Loss:     2.591212, Batch Acc: 0.004462, Tokens per Sec:     5013, Lr: 0.000035
2022-09-16 06:42:31,617 - INFO - joeynmt.training - Epoch   1, Step:   245500, Batch Loss:     2.650737, Batch Acc: 0.004317, Tokens per Sec:     4980, Lr: 0.000035
2022-09-16 06:42:54,529 - INFO - joeynmt.training - Epoch   1, Step:   245600, Batch Loss:     2.587448, Batch Acc: 0.004141, Tokens per Sec:     4922, Lr: 0.000035
2022-09-16 06:43:17,478 - INFO - joeynmt.training - Epoch   1, Step:   245700, Batch Loss:     2.698207, Batch Acc: 0.004771, Tokens per Sec:     4932, Lr: 0.000035
2022-09-16 06:43:40,437 - INFO - joeynmt.training - Epoch   1, Step:   245800, Batch Loss:     2.634722, Batch Acc: 0.004424, Tokens per Sec:     5120, Lr: 0.000035
2022-09-16 06:44:03,496 - INFO - joeynmt.training - Epoch   1, Step:   245900, Batch Loss:     2.751201, Batch Acc: 0.004154, Tokens per Sec:     4949, Lr: 0.000035
2022-09-16 06:44:26,487 - INFO - joeynmt.training - Epoch   1, Step:   246000, Batch Loss:     2.507719, Batch Acc: 0.004049, Tokens per Sec:     5017, Lr: 0.000035
2022-09-16 06:44:26,487 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 07:00:20,729 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 07:00:20,730 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.17, loss:   2.51, ppl:  12.33, acc:   0.52, generation: 946.7514[sec], evaluation: 6.8955[sec]
2022-09-16 07:00:20,742 - INFO - joeynmt.training - Example #0
2022-09-16 07:00:20,754 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 07:00:20,754 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 07:00:20,754 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को शामिल किया गया है
2022-09-16 07:00:20,754 - INFO - joeynmt.training - Example #1
2022-09-16 07:00:20,766 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 07:00:20,766 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 07:00:20,766 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 07:00:20,766 - INFO - joeynmt.training - Example #2
2022-09-16 07:00:20,777 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 07:00:20,777 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 07:00:20,777 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 07:00:20,778 - INFO - joeynmt.training - Example #3
2022-09-16 07:00:20,789 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 07:00:20,789 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 07:00:20,789 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 07:00:43,953 - INFO - joeynmt.training - Epoch   1, Step:   246100, Batch Loss:     2.476719, Batch Acc: 0.004680, Tokens per Sec:     4960, Lr: 0.000035
2022-09-16 07:01:06,854 - INFO - joeynmt.training - Epoch   1, Step:   246200, Batch Loss:     2.568018, Batch Acc: 0.004132, Tokens per Sec:     4924, Lr: 0.000035
2022-09-16 07:01:29,864 - INFO - joeynmt.training - Epoch   1, Step:   246300, Batch Loss:     2.750480, Batch Acc: 0.005510, Tokens per Sec:     4946, Lr: 0.000035
2022-09-16 07:01:52,834 - INFO - joeynmt.training - Epoch   1, Step:   246400, Batch Loss:     2.558341, Batch Acc: 0.005002, Tokens per Sec:     4882, Lr: 0.000035
2022-09-16 07:02:15,752 - INFO - joeynmt.training - Epoch   1, Step:   246500, Batch Loss:     2.636674, Batch Acc: 0.004782, Tokens per Sec:     4964, Lr: 0.000035
2022-09-16 07:02:38,577 - INFO - joeynmt.training - Epoch   1, Step:   246600, Batch Loss:     2.482432, Batch Acc: 0.004935, Tokens per Sec:     4963, Lr: 0.000035
2022-09-16 07:03:01,483 - INFO - joeynmt.training - Epoch   1, Step:   246700, Batch Loss:     2.519272, Batch Acc: 0.005442, Tokens per Sec:     4950, Lr: 0.000035
2022-09-16 07:03:24,415 - INFO - joeynmt.training - Epoch   1, Step:   246800, Batch Loss:     2.744927, Batch Acc: 0.004257, Tokens per Sec:     5029, Lr: 0.000035
2022-09-16 07:03:47,479 - INFO - joeynmt.training - Epoch   1, Step:   246900, Batch Loss:     2.708784, Batch Acc: 0.003961, Tokens per Sec:     4992, Lr: 0.000035
2022-09-16 07:04:10,339 - INFO - joeynmt.training - Epoch   1, Step:   247000, Batch Loss:     2.817325, Batch Acc: 0.004956, Tokens per Sec:     4970, Lr: 0.000035
2022-09-16 07:04:10,339 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 07:19:47,542 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 07:19:47,544 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.17, loss:   2.51, ppl:  12.28, acc:   0.52, generation: 929.9183[sec], evaluation: 6.9416[sec]
2022-09-16 07:19:47,556 - INFO - joeynmt.training - Example #0
2022-09-16 07:19:47,569 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 07:19:47,569 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 07:19:47,569 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 07:19:47,569 - INFO - joeynmt.training - Example #1
2022-09-16 07:19:47,580 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 07:19:47,580 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 07:19:47,580 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 07:19:47,580 - INFO - joeynmt.training - Example #2
2022-09-16 07:19:47,591 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 07:19:47,591 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 07:19:47,591 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 07:19:47,591 - INFO - joeynmt.training - Example #3
2022-09-16 07:19:47,602 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 07:19:47,602 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 07:19:47,602 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 07:20:10,601 - INFO - joeynmt.training - Epoch   1, Step:   247100, Batch Loss:     2.619147, Batch Acc: 0.003993, Tokens per Sec:     5019, Lr: 0.000035
2022-09-16 07:20:33,579 - INFO - joeynmt.training - Epoch   1, Step:   247200, Batch Loss:     2.497373, Batch Acc: 0.005000, Tokens per Sec:     5031, Lr: 0.000035
2022-09-16 07:20:56,394 - INFO - joeynmt.training - Epoch   1, Step:   247300, Batch Loss:     2.544774, Batch Acc: 0.004506, Tokens per Sec:     4990, Lr: 0.000035
2022-09-16 07:21:19,307 - INFO - joeynmt.training - Epoch   1, Step:   247400, Batch Loss:     2.628230, Batch Acc: 0.004464, Tokens per Sec:     4956, Lr: 0.000035
2022-09-16 07:21:42,122 - INFO - joeynmt.training - Epoch   1, Step:   247500, Batch Loss:     2.413354, Batch Acc: 0.005063, Tokens per Sec:     5021, Lr: 0.000035
2022-09-16 07:22:05,017 - INFO - joeynmt.training - Epoch   1, Step:   247600, Batch Loss:     2.927389, Batch Acc: 0.003724, Tokens per Sec:     5020, Lr: 0.000035
2022-09-16 07:22:27,936 - INFO - joeynmt.training - Epoch   1, Step:   247700, Batch Loss:     2.637993, Batch Acc: 0.004387, Tokens per Sec:     4993, Lr: 0.000035
2022-09-16 07:22:50,739 - INFO - joeynmt.training - Epoch   1, Step:   247800, Batch Loss:     2.479141, Batch Acc: 0.005384, Tokens per Sec:     4977, Lr: 0.000035
2022-09-16 07:23:13,662 - INFO - joeynmt.training - Epoch   1, Step:   247900, Batch Loss:     2.497609, Batch Acc: 0.004553, Tokens per Sec:     5021, Lr: 0.000035
2022-09-16 07:23:36,453 - INFO - joeynmt.training - Epoch   1, Step:   248000, Batch Loss:     2.626754, Batch Acc: 0.003517, Tokens per Sec:     4965, Lr: 0.000035
2022-09-16 07:23:36,453 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 07:39:18,837 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 07:39:18,839 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.18, loss:   2.51, ppl:  12.26, acc:   0.52, generation: 935.0767[sec], evaluation: 6.9669[sec]
2022-09-16 07:39:18,851 - INFO - joeynmt.training - Example #0
2022-09-16 07:39:18,863 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 07:39:18,863 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 07:39:18,863 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 07:39:18,863 - INFO - joeynmt.training - Example #1
2022-09-16 07:39:18,874 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 07:39:18,874 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 07:39:18,874 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 07:39:18,874 - INFO - joeynmt.training - Example #2
2022-09-16 07:39:18,885 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 07:39:18,885 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 07:39:18,885 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 07:39:18,885 - INFO - joeynmt.training - Example #3
2022-09-16 07:39:18,896 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 07:39:18,896 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 07:39:18,896 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 07:39:41,814 - INFO - joeynmt.training - Epoch   1, Step:   248100, Batch Loss:     2.512830, Batch Acc: 0.004832, Tokens per Sec:     4928, Lr: 0.000035
2022-09-16 07:40:04,656 - INFO - joeynmt.training - Epoch   1, Step:   248200, Batch Loss:     2.566149, Batch Acc: 0.005139, Tokens per Sec:     5061, Lr: 0.000035
2022-09-16 07:40:27,607 - INFO - joeynmt.training - Epoch   1, Step:   248300, Batch Loss:     2.684736, Batch Acc: 0.004260, Tokens per Sec:     5073, Lr: 0.000035
2022-09-16 07:40:50,488 - INFO - joeynmt.training - Epoch   1, Step:   248400, Batch Loss:     2.412861, Batch Acc: 0.005324, Tokens per Sec:     5057, Lr: 0.000035
2022-09-16 07:41:13,427 - INFO - joeynmt.training - Epoch   1, Step:   248500, Batch Loss:     2.669275, Batch Acc: 0.004791, Tokens per Sec:     4941, Lr: 0.000035
2022-09-16 07:41:36,177 - INFO - joeynmt.training - Epoch   1, Step:   248600, Batch Loss:     2.666144, Batch Acc: 0.004386, Tokens per Sec:     5051, Lr: 0.000035
2022-09-16 07:41:58,944 - INFO - joeynmt.training - Epoch   1, Step:   248700, Batch Loss:     2.450567, Batch Acc: 0.005429, Tokens per Sec:     4984, Lr: 0.000035
2022-09-16 07:42:21,988 - INFO - joeynmt.training - Epoch   1, Step:   248800, Batch Loss:     2.277677, Batch Acc: 0.004729, Tokens per Sec:     4901, Lr: 0.000035
2022-09-16 07:42:44,805 - INFO - joeynmt.training - Epoch   1, Step:   248900, Batch Loss:     2.604866, Batch Acc: 0.003969, Tokens per Sec:     4992, Lr: 0.000035
2022-09-16 07:43:07,623 - INFO - joeynmt.training - Epoch   1, Step:   249000, Batch Loss:     2.894761, Batch Acc: 0.004557, Tokens per Sec:     5049, Lr: 0.000035
2022-09-16 07:43:07,623 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 07:58:23,241 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 07:58:23,242 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.10, loss:   2.51, ppl:  12.25, acc:   0.52, generation: 908.7473[sec], evaluation: 6.5306[sec]
2022-09-16 07:58:23,246 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 07:58:24,802 - INFO - joeynmt.training - Example #0
2022-09-16 07:58:24,815 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 07:58:24,815 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 07:58:24,815 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 07:58:24,815 - INFO - joeynmt.training - Example #1
2022-09-16 07:58:24,825 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 07:58:24,825 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 07:58:24,825 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 07:58:24,826 - INFO - joeynmt.training - Example #2
2022-09-16 07:58:24,836 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 07:58:24,836 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 07:58:24,836 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 07:58:24,836 - INFO - joeynmt.training - Example #3
2022-09-16 07:58:24,847 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 07:58:24,847 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 07:58:24,847 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 07:58:47,648 - INFO - joeynmt.training - Epoch   1, Step:   249100, Batch Loss:     2.697497, Batch Acc: 0.004175, Tokens per Sec:     4653, Lr: 0.000035
2022-09-16 07:59:10,513 - INFO - joeynmt.training - Epoch   1, Step:   249200, Batch Loss:     2.705676, Batch Acc: 0.004417, Tokens per Sec:     4970, Lr: 0.000035
2022-09-16 07:59:33,389 - INFO - joeynmt.training - Epoch   1, Step:   249300, Batch Loss:     2.722455, Batch Acc: 0.004933, Tokens per Sec:     5069, Lr: 0.000035
2022-09-16 07:59:56,336 - INFO - joeynmt.training - Epoch   1, Step:   249400, Batch Loss:     2.758542, Batch Acc: 0.004815, Tokens per Sec:     5095, Lr: 0.000035
2022-09-16 08:00:19,229 - INFO - joeynmt.training - Epoch   1, Step:   249500, Batch Loss:     2.637282, Batch Acc: 0.004076, Tokens per Sec:     4973, Lr: 0.000035
2022-09-16 08:00:42,126 - INFO - joeynmt.training - Epoch   1, Step:   249600, Batch Loss:     2.585555, Batch Acc: 0.004633, Tokens per Sec:     5034, Lr: 0.000035
2022-09-16 08:01:05,313 - INFO - joeynmt.training - Epoch   1, Step:   249700, Batch Loss:     2.596226, Batch Acc: 0.004864, Tokens per Sec:     4948, Lr: 0.000035
2022-09-16 08:01:28,148 - INFO - joeynmt.training - Epoch   1, Step:   249800, Batch Loss:     2.647476, Batch Acc: 0.005113, Tokens per Sec:     4976, Lr: 0.000035
2022-09-16 08:01:50,929 - INFO - joeynmt.training - Epoch   1, Step:   249900, Batch Loss:     2.510637, Batch Acc: 0.004433, Tokens per Sec:     5050, Lr: 0.000035
2022-09-16 08:02:13,853 - INFO - joeynmt.training - Epoch   1, Step:   250000, Batch Loss:     2.827589, Batch Acc: 0.005050, Tokens per Sec:     5123, Lr: 0.000035
2022-09-16 08:02:13,854 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 08:17:06,418 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 08:17:06,419 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.32, loss:   2.50, ppl:  12.21, acc:   0.52, generation: 885.6348[sec], evaluation: 6.5888[sec]
2022-09-16 08:17:06,422 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 08:17:07,961 - INFO - joeynmt.training - Example #0
2022-09-16 08:17:07,973 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 08:17:07,973 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 08:17:07,973 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 08:17:07,973 - INFO - joeynmt.training - Example #1
2022-09-16 08:17:07,984 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 08:17:07,984 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 08:17:07,984 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 08:17:07,984 - INFO - joeynmt.training - Example #2
2022-09-16 08:17:07,995 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 08:17:07,995 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 08:17:07,995 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 08:17:07,995 - INFO - joeynmt.training - Example #3
2022-09-16 08:17:08,005 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 08:17:08,005 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 08:17:08,005 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 08:17:30,987 - INFO - joeynmt.training - Epoch   1, Step:   250100, Batch Loss:     2.715578, Batch Acc: 0.004630, Tokens per Sec:     4660, Lr: 0.000035
2022-09-16 08:17:53,761 - INFO - joeynmt.training - Epoch   1, Step:   250200, Batch Loss:     2.611095, Batch Acc: 0.004201, Tokens per Sec:     4944, Lr: 0.000035
2022-09-16 08:18:16,579 - INFO - joeynmt.training - Epoch   1, Step:   250300, Batch Loss:     2.786304, Batch Acc: 0.005163, Tokens per Sec:     5068, Lr: 0.000035
2022-09-16 08:18:39,253 - INFO - joeynmt.training - Epoch   1, Step:   250400, Batch Loss:     2.732193, Batch Acc: 0.005035, Tokens per Sec:     4975, Lr: 0.000035
2022-09-16 08:19:02,044 - INFO - joeynmt.training - Epoch   1, Step:   250500, Batch Loss:     2.668383, Batch Acc: 0.004058, Tokens per Sec:     5114, Lr: 0.000035
2022-09-16 08:19:24,751 - INFO - joeynmt.training - Epoch   1, Step:   250600, Batch Loss:     2.917849, Batch Acc: 0.003784, Tokens per Sec:     4946, Lr: 0.000035
2022-09-16 08:19:47,610 - INFO - joeynmt.training - Epoch   1, Step:   250700, Batch Loss:     2.804484, Batch Acc: 0.004057, Tokens per Sec:     5036, Lr: 0.000035
2022-09-16 08:20:10,565 - INFO - joeynmt.training - Epoch   1, Step:   250800, Batch Loss:     2.799154, Batch Acc: 0.004717, Tokens per Sec:     4941, Lr: 0.000035
2022-09-16 08:20:33,315 - INFO - joeynmt.training - Epoch   1, Step:   250900, Batch Loss:     2.310070, Batch Acc: 0.004753, Tokens per Sec:     5040, Lr: 0.000035
2022-09-16 08:20:56,126 - INFO - joeynmt.training - Epoch   1, Step:   251000, Batch Loss:     2.520390, Batch Acc: 0.004755, Tokens per Sec:     5062, Lr: 0.000035
2022-09-16 08:20:56,127 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 08:35:37,224 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 08:35:37,229 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 08:35:39,414 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/245000.ckpt
2022-09-16 08:35:39,457 - INFO - joeynmt.training - Example #0
2022-09-16 08:35:39,469 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 08:35:39,469 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 08:35:39,469 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 08:35:39,469 - INFO - joeynmt.training - Example #1
2022-09-16 08:35:39,480 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 08:35:39,480 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 08:35:39,480 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 08:35:39,480 - INFO - joeynmt.training - Example #2
2022-09-16 08:35:39,491 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 08:35:39,491 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 08:35:39,491 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 08:35:39,491 - INFO - joeynmt.training - Example #3
2022-09-16 08:35:39,502 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 08:35:39,502 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 08:35:39,502 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 08:36:02,500 - INFO - joeynmt.training - Epoch   1, Step:   251100, Batch Loss:     2.614004, Batch Acc: 0.004711, Tokens per Sec:     4545, Lr: 0.000035
2022-09-16 08:36:25,282 - INFO - joeynmt.training - Epoch   1, Step:   251200, Batch Loss:     2.641248, Batch Acc: 0.004987, Tokens per Sec:     5052, Lr: 0.000035
2022-09-16 08:36:48,100 - INFO - joeynmt.training - Epoch   1, Step:   251300, Batch Loss:     2.397372, Batch Acc: 0.005406, Tokens per Sec:     5050, Lr: 0.000035
2022-09-16 08:37:11,006 - INFO - joeynmt.training - Epoch   1, Step:   251400, Batch Loss:     2.667206, Batch Acc: 0.004661, Tokens per Sec:     5002, Lr: 0.000035
2022-09-16 08:37:33,949 - INFO - joeynmt.training - Epoch   1, Step:   251500, Batch Loss:     2.427425, Batch Acc: 0.004281, Tokens per Sec:     4978, Lr: 0.000035
2022-09-16 08:37:56,842 - INFO - joeynmt.training - Epoch   1, Step:   251600, Batch Loss:     2.434954, Batch Acc: 0.004919, Tokens per Sec:     5079, Lr: 0.000035
2022-09-16 08:38:19,656 - INFO - joeynmt.training - Epoch   1, Step:   251700, Batch Loss:     2.491837, Batch Acc: 0.005360, Tokens per Sec:     5022, Lr: 0.000035
2022-09-16 08:38:42,593 - INFO - joeynmt.training - Epoch   1, Step:   251800, Batch Loss:     2.733796, Batch Acc: 0.004993, Tokens per Sec:     5030, Lr: 0.000035
2022-09-16 08:39:05,327 - INFO - joeynmt.training - Epoch   1, Step:   251900, Batch Loss:     2.591837, Batch Acc: 0.005215, Tokens per Sec:     5027, Lr: 0.000035
2022-09-16 08:39:28,207 - INFO - joeynmt.training - Epoch   1, Step:   252000, Batch Loss:     2.587410, Batch Acc: 0.004749, Tokens per Sec:     5062, Lr: 0.000035
2022-09-16 08:39:28,207 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 08:54:15,277 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 08:54:15,279 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.23, loss:   2.50, ppl:  12.14, acc:   0.52, generation: 879.7010[sec], evaluation: 7.0294[sec]
2022-09-16 08:54:15,282 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 08:54:16,748 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/249000.ckpt
2022-09-16 08:54:16,792 - INFO - joeynmt.training - Example #0
2022-09-16 08:54:16,804 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 08:54:16,804 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 08:54:16,804 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 08:54:16,804 - INFO - joeynmt.training - Example #1
2022-09-16 08:54:16,815 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 08:54:16,815 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 08:54:16,815 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 08:54:16,815 - INFO - joeynmt.training - Example #2
2022-09-16 08:54:16,825 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 08:54:16,825 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 08:54:16,825 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 08:54:16,826 - INFO - joeynmt.training - Example #3
2022-09-16 08:54:16,836 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 08:54:16,836 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 08:54:16,836 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 08:54:39,891 - INFO - joeynmt.training - Epoch   1, Step:   252100, Batch Loss:     2.695660, Batch Acc: 0.004087, Tokens per Sec:     4663, Lr: 0.000035
2022-09-16 08:55:02,660 - INFO - joeynmt.training - Epoch   1, Step:   252200, Batch Loss:     2.588495, Batch Acc: 0.004926, Tokens per Sec:     5082, Lr: 0.000035
2022-09-16 08:55:25,587 - INFO - joeynmt.training - Epoch   1, Step:   252300, Batch Loss:     2.612062, Batch Acc: 0.004287, Tokens per Sec:     4955, Lr: 0.000035
2022-09-16 08:55:48,504 - INFO - joeynmt.training - Epoch   1, Step:   252400, Batch Loss:     2.832309, Batch Acc: 0.004467, Tokens per Sec:     5001, Lr: 0.000035
2022-09-16 08:56:11,331 - INFO - joeynmt.training - Epoch   1, Step:   252500, Batch Loss:     2.614229, Batch Acc: 0.005150, Tokens per Sec:     5036, Lr: 0.000035
2022-09-16 08:56:34,114 - INFO - joeynmt.training - Epoch   1, Step:   252600, Batch Loss:     2.762203, Batch Acc: 0.003532, Tokens per Sec:     4934, Lr: 0.000035
2022-09-16 08:56:56,844 - INFO - joeynmt.training - Epoch   1, Step:   252700, Batch Loss:     2.520044, Batch Acc: 0.004322, Tokens per Sec:     5100, Lr: 0.000035
2022-09-16 08:57:19,704 - INFO - joeynmt.training - Epoch   1, Step:   252800, Batch Loss:     2.593191, Batch Acc: 0.005276, Tokens per Sec:     4975, Lr: 0.000035
2022-09-16 08:57:42,613 - INFO - joeynmt.training - Epoch   1, Step:   252900, Batch Loss:     2.734904, Batch Acc: 0.004225, Tokens per Sec:     5031, Lr: 0.000035
2022-09-16 08:58:05,399 - INFO - joeynmt.training - Epoch   1, Step:   253000, Batch Loss:     2.561095, Batch Acc: 0.004943, Tokens per Sec:     5034, Lr: 0.000035
2022-09-16 08:58:05,400 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:13:10,903 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 09:13:10,905 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.39, loss:   2.50, ppl:  12.14, acc:   0.52, generation: 898.5059[sec], evaluation: 6.6557[sec]
2022-09-16 09:13:10,908 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 09:13:12,385 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/250000.ckpt
2022-09-16 09:13:12,440 - INFO - joeynmt.training - Example #0
2022-09-16 09:13:12,453 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 09:13:12,453 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 09:13:12,453 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 09:13:12,453 - INFO - joeynmt.training - Example #1
2022-09-16 09:13:12,464 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 09:13:12,464 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 09:13:12,464 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 09:13:12,464 - INFO - joeynmt.training - Example #2
2022-09-16 09:13:12,475 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 09:13:12,475 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 09:13:12,475 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 09:13:12,475 - INFO - joeynmt.training - Example #3
2022-09-16 09:13:12,486 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 09:13:12,486 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 09:13:12,486 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 09:13:35,403 - INFO - joeynmt.training - Epoch   1, Step:   253100, Batch Loss:     2.470870, Batch Acc: 0.005278, Tokens per Sec:     4595, Lr: 0.000035
2022-09-16 09:13:58,327 - INFO - joeynmt.training - Epoch   1, Step:   253200, Batch Loss:     2.571251, Batch Acc: 0.005306, Tokens per Sec:     4933, Lr: 0.000035
2022-09-16 09:14:21,098 - INFO - joeynmt.training - Epoch   1, Step:   253300, Batch Loss:     2.586583, Batch Acc: 0.004178, Tokens per Sec:     4951, Lr: 0.000035
2022-09-16 09:14:43,829 - INFO - joeynmt.training - Epoch   1, Step:   253400, Batch Loss:     2.477041, Batch Acc: 0.004929, Tokens per Sec:     5079, Lr: 0.000035
2022-09-16 09:15:06,495 - INFO - joeynmt.training - Epoch   1, Step:   253500, Batch Loss:     2.446890, Batch Acc: 0.004708, Tokens per Sec:     5070, Lr: 0.000035
2022-09-16 09:15:29,258 - INFO - joeynmt.training - Epoch   1, Step:   253600, Batch Loss:     2.638480, Batch Acc: 0.004876, Tokens per Sec:     5037, Lr: 0.000035
2022-09-16 09:15:52,043 - INFO - joeynmt.training - Epoch   1, Step:   253700, Batch Loss:     2.552725, Batch Acc: 0.005618, Tokens per Sec:     5031, Lr: 0.000035
2022-09-16 09:16:14,963 - INFO - joeynmt.training - Epoch   1, Step:   253800, Batch Loss:     2.420588, Batch Acc: 0.005022, Tokens per Sec:     5039, Lr: 0.000035
2022-09-16 09:16:37,878 - INFO - joeynmt.training - Epoch   1, Step:   253900, Batch Loss:     2.754835, Batch Acc: 0.004481, Tokens per Sec:     4996, Lr: 0.000035
2022-09-16 09:17:00,833 - INFO - joeynmt.training - Epoch   1, Step:   254000, Batch Loss:     2.750941, Batch Acc: 0.004551, Tokens per Sec:     4997, Lr: 0.000035
2022-09-16 09:17:00,833 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:32:20,810 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 09:32:20,811 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.29, loss:   2.49, ppl:  12.09, acc:   0.52, generation: 912.5656[sec], evaluation: 6.6831[sec]
2022-09-16 09:32:20,815 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 09:32:23,041 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/251000.ckpt
2022-09-16 09:32:23,084 - INFO - joeynmt.training - Example #0
2022-09-16 09:32:23,096 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 09:32:23,096 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 09:32:23,096 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 09:32:23,096 - INFO - joeynmt.training - Example #1
2022-09-16 09:32:23,107 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 09:32:23,107 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 09:32:23,107 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 09:32:23,107 - INFO - joeynmt.training - Example #2
2022-09-16 09:32:23,117 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 09:32:23,117 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 09:32:23,117 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 09:32:23,117 - INFO - joeynmt.training - Example #3
2022-09-16 09:32:23,128 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 09:32:23,128 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 09:32:23,128 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 09:32:46,231 - INFO - joeynmt.training - Epoch   1, Step:   254100, Batch Loss:     2.642209, Batch Acc: 0.003894, Tokens per Sec:     4547, Lr: 0.000035
2022-09-16 09:33:08,865 - INFO - joeynmt.training - Epoch   1, Step:   254200, Batch Loss:     2.346305, Batch Acc: 0.005349, Tokens per Sec:     5104, Lr: 0.000035
2022-09-16 09:33:31,724 - INFO - joeynmt.training - Epoch   1, Step:   254300, Batch Loss:     2.518652, Batch Acc: 0.004510, Tokens per Sec:     5025, Lr: 0.000035
2022-09-16 09:33:54,490 - INFO - joeynmt.training - Epoch   1, Step:   254400, Batch Loss:     2.716169, Batch Acc: 0.004031, Tokens per Sec:     5001, Lr: 0.000035
2022-09-16 09:34:17,322 - INFO - joeynmt.training - Epoch   1, Step:   254500, Batch Loss:     2.695270, Batch Acc: 0.004850, Tokens per Sec:     5021, Lr: 0.000035
2022-09-16 09:34:40,074 - INFO - joeynmt.training - Epoch   1, Step:   254600, Batch Loss:     2.721024, Batch Acc: 0.004698, Tokens per Sec:     5042, Lr: 0.000035
2022-09-16 09:35:02,930 - INFO - joeynmt.training - Epoch   1, Step:   254700, Batch Loss:     2.651752, Batch Acc: 0.004261, Tokens per Sec:     5011, Lr: 0.000035
2022-09-16 09:35:25,693 - INFO - joeynmt.training - Epoch   1, Step:   254800, Batch Loss:     2.780361, Batch Acc: 0.004442, Tokens per Sec:     5083, Lr: 0.000035
2022-09-16 09:35:48,520 - INFO - joeynmt.training - Epoch   1, Step:   254900, Batch Loss:     2.701201, Batch Acc: 0.004535, Tokens per Sec:     5004, Lr: 0.000035
2022-09-16 09:36:11,463 - INFO - joeynmt.training - Epoch   1, Step:   255000, Batch Loss:     2.518533, Batch Acc: 0.005258, Tokens per Sec:     5065, Lr: 0.000035
2022-09-16 09:36:11,463 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:51:45,293 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 09:51:45,295 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.43, loss:   2.49, ppl:  12.08, acc:   0.52, generation: 926.8462[sec], evaluation: 6.6397[sec]
2022-09-16 09:51:45,298 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 09:51:46,778 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/252000.ckpt
2022-09-16 09:51:46,821 - INFO - joeynmt.training - Example #0
2022-09-16 09:51:46,833 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 09:51:46,833 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 09:51:46,833 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 09:51:46,833 - INFO - joeynmt.training - Example #1
2022-09-16 09:51:46,843 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 09:51:46,843 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 09:51:46,844 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 09:51:46,844 - INFO - joeynmt.training - Example #2
2022-09-16 09:51:46,854 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 09:51:46,854 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 09:51:46,854 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 09:51:46,854 - INFO - joeynmt.training - Example #3
2022-09-16 09:51:46,865 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 09:51:46,865 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 09:51:46,865 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 09:52:09,781 - INFO - joeynmt.training - Epoch   1, Step:   255100, Batch Loss:     2.555676, Batch Acc: 0.003993, Tokens per Sec:     4767, Lr: 0.000035
2022-09-16 09:52:32,576 - INFO - joeynmt.training - Epoch   1, Step:   255200, Batch Loss:     2.468408, Batch Acc: 0.005423, Tokens per Sec:     4959, Lr: 0.000035
2022-09-16 09:52:55,394 - INFO - joeynmt.training - Epoch   1, Step:   255300, Batch Loss:     2.588086, Batch Acc: 0.004992, Tokens per Sec:     5048, Lr: 0.000035
2022-09-16 09:53:18,272 - INFO - joeynmt.training - Epoch   1, Step:   255400, Batch Loss:     2.439252, Batch Acc: 0.004484, Tokens per Sec:     5079, Lr: 0.000035
2022-09-16 09:53:41,461 - INFO - joeynmt.training - Epoch   1, Step:   255500, Batch Loss:     2.380168, Batch Acc: 0.004436, Tokens per Sec:     4919, Lr: 0.000035
2022-09-16 09:54:04,180 - INFO - joeynmt.training - Epoch   1, Step:   255600, Batch Loss:     2.579283, Batch Acc: 0.003964, Tokens per Sec:     4964, Lr: 0.000035
2022-09-16 09:54:26,858 - INFO - joeynmt.training - Epoch   1, Step:   255700, Batch Loss:     2.574782, Batch Acc: 0.004093, Tokens per Sec:     4945, Lr: 0.000035
2022-09-16 09:54:49,686 - INFO - joeynmt.training - Epoch   1, Step:   255800, Batch Loss:     2.703518, Batch Acc: 0.004273, Tokens per Sec:     4993, Lr: 0.000035
2022-09-16 09:55:12,486 - INFO - joeynmt.training - Epoch   1, Step:   255900, Batch Loss:     2.456392, Batch Acc: 0.004102, Tokens per Sec:     5101, Lr: 0.000035
2022-09-16 09:55:35,285 - INFO - joeynmt.training - Epoch   1, Step:   256000, Batch Loss:     2.745093, Batch Acc: 0.004690, Tokens per Sec:     5116, Lr: 0.000035
2022-09-16 09:55:35,285 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 10:10:48,020 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 10:10:48,021 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.43, loss:   2.49, ppl:  12.02, acc:   0.52, generation: 905.6576[sec], evaluation: 6.7331[sec]
2022-09-16 10:10:48,025 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 10:10:49,516 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/253000.ckpt
2022-09-16 10:10:49,559 - INFO - joeynmt.training - Example #0
2022-09-16 10:10:49,571 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 10:10:49,571 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 10:10:49,571 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 10:10:49,571 - INFO - joeynmt.training - Example #1
2022-09-16 10:10:49,582 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 10:10:49,582 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 10:10:49,582 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 10:10:49,583 - INFO - joeynmt.training - Example #2
2022-09-16 10:10:49,593 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 10:10:49,593 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 10:10:49,593 - INFO - joeynmt.training - 	Hypothesis: अभिनय , गति और इम
2022-09-16 10:10:49,593 - INFO - joeynmt.training - Example #3
2022-09-16 10:10:49,604 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 10:10:49,604 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 10:10:49,604 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 10:11:12,504 - INFO - joeynmt.training - Epoch   1, Step:   256100, Batch Loss:     2.397372, Batch Acc: 0.005226, Tokens per Sec:     4721, Lr: 0.000035
2022-09-16 10:11:35,327 - INFO - joeynmt.training - Epoch   1, Step:   256200, Batch Loss:     2.735417, Batch Acc: 0.004367, Tokens per Sec:     5077, Lr: 0.000035
2022-09-16 10:11:58,270 - INFO - joeynmt.training - Epoch   1, Step:   256300, Batch Loss:     2.466780, Batch Acc: 0.004711, Tokens per Sec:     5005, Lr: 0.000035
2022-09-16 10:12:21,244 - INFO - joeynmt.training - Epoch   1, Step:   256400, Batch Loss:     2.371364, Batch Acc: 0.004883, Tokens per Sec:     5099, Lr: 0.000035
2022-09-16 10:12:44,136 - INFO - joeynmt.training - Epoch   1, Step:   256500, Batch Loss:     2.866112, Batch Acc: 0.004911, Tokens per Sec:     5079, Lr: 0.000035
2022-09-16 10:13:06,901 - INFO - joeynmt.training - Epoch   1, Step:   256600, Batch Loss:     2.550507, Batch Acc: 0.004832, Tokens per Sec:     5090, Lr: 0.000035
2022-09-16 10:13:29,620 - INFO - joeynmt.training - Epoch   1, Step:   256700, Batch Loss:     2.602752, Batch Acc: 0.004833, Tokens per Sec:     5027, Lr: 0.000035
2022-09-16 10:13:52,474 - INFO - joeynmt.training - Epoch   1, Step:   256800, Batch Loss:     2.509982, Batch Acc: 0.004561, Tokens per Sec:     5027, Lr: 0.000035
2022-09-16 10:14:15,209 - INFO - joeynmt.training - Epoch   1, Step:   256900, Batch Loss:     2.709509, Batch Acc: 0.004988, Tokens per Sec:     4938, Lr: 0.000035
2022-09-16 10:14:38,090 - INFO - joeynmt.training - Epoch   1, Step:   257000, Batch Loss:     2.477518, Batch Acc: 0.004606, Tokens per Sec:     5001, Lr: 0.000035
2022-09-16 10:14:38,090 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 10:29:45,417 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 10:29:45,419 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.45, loss:   2.49, ppl:  12.02, acc:   0.52, generation: 900.3452[sec], evaluation: 6.6380[sec]
2022-09-16 10:29:45,422 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 10:29:46,977 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/254000.ckpt
2022-09-16 10:29:47,019 - INFO - joeynmt.training - Example #0
2022-09-16 10:29:47,031 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 10:29:47,031 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 10:29:47,031 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 10:29:47,031 - INFO - joeynmt.training - Example #1
2022-09-16 10:29:47,042 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 10:29:47,042 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 10:29:47,042 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 10:29:47,042 - INFO - joeynmt.training - Example #2
2022-09-16 10:29:47,053 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 10:29:47,053 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 10:29:47,053 - INFO - joeynmt.training - 	Hypothesis: अभिनय , गति और इम
2022-09-16 10:29:47,053 - INFO - joeynmt.training - Example #3
2022-09-16 10:29:47,064 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 10:29:47,064 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 10:29:47,064 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 10:30:10,444 - INFO - joeynmt.training - Epoch   1, Step:   257100, Batch Loss:     2.361573, Batch Acc: 0.005234, Tokens per Sec:     4627, Lr: 0.000035
2022-09-16 10:30:33,121 - INFO - joeynmt.training - Epoch   1, Step:   257200, Batch Loss:     2.523098, Batch Acc: 0.005284, Tokens per Sec:     4991, Lr: 0.000035
2022-09-16 10:30:55,862 - INFO - joeynmt.training - Epoch   1, Step:   257300, Batch Loss:     2.551761, Batch Acc: 0.004903, Tokens per Sec:     5050, Lr: 0.000035
2022-09-16 10:31:18,719 - INFO - joeynmt.training - Epoch   1, Step:   257400, Batch Loss:     2.546695, Batch Acc: 0.005575, Tokens per Sec:     5031, Lr: 0.000035
2022-09-16 10:31:41,466 - INFO - joeynmt.training - Epoch   1, Step:   257500, Batch Loss:     2.529449, Batch Acc: 0.004138, Tokens per Sec:     5036, Lr: 0.000035
2022-09-16 10:32:04,183 - INFO - joeynmt.training - Epoch   1, Step:   257600, Batch Loss:     2.688434, Batch Acc: 0.004488, Tokens per Sec:     5012, Lr: 0.000035
2022-09-16 10:32:27,013 - INFO - joeynmt.training - Epoch   1, Step:   257700, Batch Loss:     2.709743, Batch Acc: 0.004663, Tokens per Sec:     4998, Lr: 0.000035
2022-09-16 10:32:49,831 - INFO - joeynmt.training - Epoch   1, Step:   257800, Batch Loss:     2.615900, Batch Acc: 0.003884, Tokens per Sec:     5010, Lr: 0.000035
2022-09-16 10:33:12,608 - INFO - joeynmt.training - Epoch   1, Step:   257900, Batch Loss:     2.399989, Batch Acc: 0.004448, Tokens per Sec:     5084, Lr: 0.000035
2022-09-16 10:33:35,477 - INFO - joeynmt.training - Epoch   1, Step:   258000, Batch Loss:     2.511733, Batch Acc: 0.004919, Tokens per Sec:     4987, Lr: 0.000035
2022-09-16 10:33:35,478 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 10:48:34,087 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 10:48:34,088 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.47, loss:   2.49, ppl:  12.04, acc:   0.52, generation: 891.6488[sec], evaluation: 6.6199[sec]
2022-09-16 10:48:35,582 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/255000.ckpt
2022-09-16 10:48:35,624 - INFO - joeynmt.training - Example #0
2022-09-16 10:48:35,637 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 10:48:35,637 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 10:48:35,637 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 10:48:35,637 - INFO - joeynmt.training - Example #1
2022-09-16 10:48:35,648 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 10:48:35,648 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 10:48:35,648 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 10:48:35,648 - INFO - joeynmt.training - Example #2
2022-09-16 10:48:35,658 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 10:48:35,658 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 10:48:35,658 - INFO - joeynmt.training - 	Hypothesis: अभिनय , गति और इम
2022-09-16 10:48:35,658 - INFO - joeynmt.training - Example #3
2022-09-16 10:48:35,669 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 10:48:35,669 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 10:48:35,669 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 10:48:58,646 - INFO - joeynmt.training - Epoch   1, Step:   258100, Batch Loss:     2.436952, Batch Acc: 0.004482, Tokens per Sec:     4661, Lr: 0.000035
2022-09-16 10:49:21,497 - INFO - joeynmt.training - Epoch   1, Step:   258200, Batch Loss:     2.350900, Batch Acc: 0.005204, Tokens per Sec:     4987, Lr: 0.000035
2022-09-16 10:49:44,197 - INFO - joeynmt.training - Epoch   1, Step:   258300, Batch Loss:     2.553724, Batch Acc: 0.005859, Tokens per Sec:     5038, Lr: 0.000035
2022-09-16 10:50:06,903 - INFO - joeynmt.training - Epoch   1, Step:   258400, Batch Loss:     2.366664, Batch Acc: 0.004672, Tokens per Sec:     4959, Lr: 0.000035
2022-09-16 10:50:29,662 - INFO - joeynmt.training - Epoch   1, Step:   258500, Batch Loss:     2.506012, Batch Acc: 0.004540, Tokens per Sec:     5023, Lr: 0.000035
2022-09-16 10:50:52,358 - INFO - joeynmt.training - Epoch   1, Step:   258600, Batch Loss:     2.538113, Batch Acc: 0.004712, Tokens per Sec:     5021, Lr: 0.000035
2022-09-16 10:51:15,049 - INFO - joeynmt.training - Epoch   1, Step:   258700, Batch Loss:     2.909085, Batch Acc: 0.004432, Tokens per Sec:     5061, Lr: 0.000035
2022-09-16 10:51:37,847 - INFO - joeynmt.training - Epoch   1, Step:   258800, Batch Loss:     2.495526, Batch Acc: 0.004530, Tokens per Sec:     4997, Lr: 0.000035
2022-09-16 10:52:00,649 - INFO - joeynmt.training - Epoch   1, Step:   258900, Batch Loss:     2.478202, Batch Acc: 0.004476, Tokens per Sec:     5006, Lr: 0.000035
2022-09-16 10:52:23,377 - INFO - joeynmt.training - Epoch   1, Step:   259000, Batch Loss:     2.813438, Batch Acc: 0.004905, Tokens per Sec:     5068, Lr: 0.000035
2022-09-16 10:52:23,377 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:07:32,163 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 11:07:32,164 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.50, loss:   2.48, ppl:  11.94, acc:   0.52, generation: 901.7827[sec], evaluation: 6.6612[sec]
2022-09-16 11:07:32,168 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 11:07:33,678 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/258000.ckpt
2022-09-16 11:07:33,720 - INFO - joeynmt.training - Example #0
2022-09-16 11:07:33,733 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 11:07:33,733 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 11:07:33,733 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 11:07:33,733 - INFO - joeynmt.training - Example #1
2022-09-16 11:07:33,744 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 11:07:33,744 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 11:07:33,744 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 11:07:33,744 - INFO - joeynmt.training - Example #2
2022-09-16 11:07:33,754 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 11:07:33,754 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 11:07:33,754 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 11:07:33,754 - INFO - joeynmt.training - Example #3
2022-09-16 11:07:33,765 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 11:07:33,765 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 11:07:33,765 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 11:07:56,586 - INFO - joeynmt.training - Epoch   1, Step:   259100, Batch Loss:     2.495065, Batch Acc: 0.004055, Tokens per Sec:     4656, Lr: 0.000035
2022-09-16 11:08:19,323 - INFO - joeynmt.training - Epoch   1, Step:   259200, Batch Loss:     2.840530, Batch Acc: 0.004053, Tokens per Sec:     4959, Lr: 0.000035
2022-09-16 11:08:42,153 - INFO - joeynmt.training - Epoch   1, Step:   259300, Batch Loss:     2.487229, Batch Acc: 0.004415, Tokens per Sec:     4990, Lr: 0.000035
2022-09-16 11:09:04,863 - INFO - joeynmt.training - Epoch   1, Step:   259400, Batch Loss:     2.504154, Batch Acc: 0.004999, Tokens per Sec:     5021, Lr: 0.000035
2022-09-16 11:09:18,797 - INFO - joeynmt.training - Epoch   1: total training loss 37985.17
2022-09-16 11:09:18,797 - INFO - joeynmt.training - Training ended after   1 epochs.
2022-09-16 11:09:18,797 - INFO - joeynmt.training - Best validation result (greedy) at step   259000:  11.94 ppl.
2022-09-16 11:09:18,804 - INFO - joeynmt.training - Loading from ckpt file: /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt
2022-09-16 11:09:18,817 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:09:18,817 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:09:19,035 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:09:19,038 - INFO - joeynmt.model - Total params: 19302144
2022-09-16 11:09:19,170 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:09:19,214 - INFO - joeynmt.prediction - Decoding on dev set...
2022-09-16 11:09:19,215 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:29:21,141 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 11:29:21,143 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  23.84, generation: 1194.5112[sec], evaluation: 6.6479[sec]
2022-09-16 11:29:21,238 - INFO - joeynmt.prediction - Translations saved to: /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/00259000.hyps.dev.
2022-09-16 11:29:21,238 - INFO - joeynmt.prediction - Decoding on test set...
2022-09-16 11:29:21,238 - INFO - joeynmt.prediction - Predicting 40858 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:48:57,868 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 11:48:57,870 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  23.08, generation: 1169.1814[sec], evaluation: 7.1177[sec]
2022-09-16 11:48:57,970 - INFO - joeynmt.prediction - Translations saved to: /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/00259000.hyps.test.
2022-09-16 11:48:57,978 - INFO - joeynmt.training - ACTIVE LEARNING MODEL END - MARGIN 0
2022-09-16 11:48:57,978 - INFO - joeynmt.training - ACTIVE LEARNING MODEL START - MARGIN 1
2022-09-16 11:48:57,979 - INFO - joeynmt.training - Random Indices picked: [121958 146867 131932 365838 259178 119879 110268 207892  54886 137337] length: 26753
2022-09-16 11:48:57,979 - INFO - joeynmt.training - Processing Predictions on Batch 0/105
2022-09-16 11:49:00,344 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:49:00,344 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:49:00,618 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:49:00,758 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:49:00,846 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:49:00,846 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:49:00,876 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:49:08,573 - INFO - joeynmt.prediction - Generation took 7.6891[sec]. (No references given)
2022-09-16 11:49:08,581 - INFO - joeynmt.training - Processing Predictions on Batch 1/105
2022-09-16 11:49:10,939 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:49:10,939 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:49:11,211 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:49:11,345 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:49:11,430 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:49:11,430 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:49:11,457 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:49:17,134 - INFO - joeynmt.prediction - Generation took 5.6697[sec]. (No references given)
2022-09-16 11:49:17,144 - INFO - joeynmt.training - Processing Predictions on Batch 2/105
2022-09-16 11:49:19,464 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:49:19,464 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:49:19,737 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:49:19,871 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:49:19,956 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:49:19,956 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:49:19,987 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:49:27,034 - INFO - joeynmt.prediction - Generation took 7.0380[sec]. (No references given)
2022-09-16 11:49:27,044 - INFO - joeynmt.training - Processing Predictions on Batch 3/105
2022-09-16 11:49:29,372 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:49:29,372 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:49:29,645 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:49:29,778 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:49:29,863 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:49:29,863 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:49:29,888 - INFO - joeynmt.prediction - Predicting 224 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:49:37,508 - INFO - joeynmt.prediction - Generation took 7.6116[sec]. (No references given)
2022-09-16 11:49:37,517 - INFO - joeynmt.training - Processing Predictions on Batch 4/105
2022-09-16 11:49:39,846 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:49:39,846 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:49:40,118 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:49:40,251 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:49:40,337 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:49:40,337 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:49:40,364 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:49:45,703 - INFO - joeynmt.prediction - Generation took 5.3320[sec]. (No references given)
2022-09-16 11:49:45,712 - INFO - joeynmt.training - Processing Predictions on Batch 5/105
2022-09-16 11:49:48,043 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:49:48,043 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:49:48,316 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:49:48,449 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:49:48,535 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:49:48,535 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:49:48,564 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:49:55,171 - INFO - joeynmt.prediction - Generation took 6.5994[sec]. (No references given)
2022-09-16 11:49:55,181 - INFO - joeynmt.training - Processing Predictions on Batch 6/105
2022-09-16 11:49:57,522 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:49:57,522 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:49:57,795 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:49:57,930 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:49:58,016 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:49:58,016 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:49:58,043 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:50:04,694 - INFO - joeynmt.prediction - Generation took 6.6433[sec]. (No references given)
2022-09-16 11:50:04,704 - INFO - joeynmt.training - Processing Predictions on Batch 7/105
2022-09-16 11:50:07,079 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:50:07,080 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:50:07,353 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:50:07,485 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:50:07,572 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:50:07,573 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:50:07,599 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:50:14,736 - INFO - joeynmt.prediction - Generation took 7.1296[sec]. (No references given)
2022-09-16 11:50:14,746 - INFO - joeynmt.training - Processing Predictions on Batch 8/105
2022-09-16 11:50:17,133 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:50:17,133 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:50:17,408 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:50:17,539 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:50:17,625 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:50:17,625 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:50:17,654 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:50:24,273 - INFO - joeynmt.prediction - Generation took 6.6107[sec]. (No references given)
2022-09-16 11:50:24,282 - INFO - joeynmt.training - Processing Predictions on Batch 9/105
2022-09-16 11:50:26,647 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:50:26,647 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:50:26,924 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:50:27,056 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:50:27,142 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:50:27,142 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:50:27,173 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:50:33,813 - INFO - joeynmt.prediction - Generation took 6.6316[sec]. (No references given)
2022-09-16 11:50:33,822 - INFO - joeynmt.training - Processing Predictions on Batch 10/105
2022-09-16 11:50:36,193 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:50:36,193 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:50:36,468 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:50:36,600 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:50:36,686 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:50:36,686 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:50:36,714 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:50:43,734 - INFO - joeynmt.prediction - Generation took 7.0122[sec]. (No references given)
2022-09-16 11:50:43,743 - INFO - joeynmt.training - Processing Predictions on Batch 11/105
2022-09-16 11:50:46,116 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:50:46,116 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:50:46,393 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:50:46,525 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:50:46,613 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:50:46,613 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:50:46,640 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:50:53,550 - INFO - joeynmt.prediction - Generation took 6.9027[sec]. (No references given)
2022-09-16 11:50:53,560 - INFO - joeynmt.training - Processing Predictions on Batch 12/105
2022-09-16 11:50:55,919 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:50:55,919 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:50:56,193 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:50:56,325 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:50:56,412 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:50:56,412 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:50:56,439 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:51:03,780 - INFO - joeynmt.prediction - Generation took 7.3325[sec]. (No references given)
2022-09-16 11:51:03,789 - INFO - joeynmt.training - Processing Predictions on Batch 13/105
2022-09-16 11:51:06,149 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:51:06,149 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:51:06,423 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:51:06,555 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:51:06,641 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:51:06,641 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:51:06,668 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:51:12,334 - INFO - joeynmt.prediction - Generation took 5.6586[sec]. (No references given)
2022-09-16 11:51:12,343 - INFO - joeynmt.training - Processing Predictions on Batch 14/105
2022-09-16 11:51:14,704 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:51:14,704 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:51:14,978 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:51:15,111 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:51:15,198 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:51:15,198 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:51:15,230 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:51:22,806 - INFO - joeynmt.prediction - Generation took 7.5678[sec]. (No references given)
2022-09-16 11:51:22,815 - INFO - joeynmt.training - Processing Predictions on Batch 15/105
2022-09-16 11:51:25,171 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:51:25,171 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:51:25,446 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:51:25,577 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:51:25,663 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:51:25,663 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:51:25,693 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:51:32,141 - INFO - joeynmt.prediction - Generation took 6.4392[sec]. (No references given)
2022-09-16 11:51:32,150 - INFO - joeynmt.training - Processing Predictions on Batch 16/105
2022-09-16 11:51:34,518 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:51:34,518 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:51:34,792 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:51:34,924 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:51:35,009 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:51:35,010 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:51:35,035 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:51:41,605 - INFO - joeynmt.prediction - Generation took 6.5618[sec]. (No references given)
2022-09-16 11:51:41,614 - INFO - joeynmt.training - Processing Predictions on Batch 17/105
2022-09-16 11:51:43,989 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:51:43,989 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:51:44,263 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:51:44,395 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:51:44,480 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:51:44,480 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:51:44,509 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:51:53,090 - INFO - joeynmt.prediction - Generation took 8.5721[sec]. (No references given)
2022-09-16 11:51:53,099 - INFO - joeynmt.training - Processing Predictions on Batch 18/105
2022-09-16 11:51:55,466 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:51:55,466 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:51:55,741 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:51:55,873 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:51:55,958 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:51:55,959 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:51:55,986 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:52:02,514 - INFO - joeynmt.prediction - Generation took 6.5201[sec]. (No references given)
2022-09-16 11:52:02,523 - INFO - joeynmt.training - Processing Predictions on Batch 19/105
2022-09-16 11:52:04,878 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:52:04,878 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:52:05,154 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:52:05,286 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:52:05,373 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:52:05,373 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:52:05,405 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:52:13,251 - INFO - joeynmt.prediction - Generation took 7.8379[sec]. (No references given)
2022-09-16 11:52:13,260 - INFO - joeynmt.training - Processing Predictions on Batch 20/105
2022-09-16 11:52:15,636 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:52:15,636 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:52:15,914 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:52:16,046 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:52:16,134 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:52:16,134 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:52:16,162 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:52:22,644 - INFO - joeynmt.prediction - Generation took 6.4739[sec]. (No references given)
2022-09-16 11:52:22,653 - INFO - joeynmt.training - Processing Predictions on Batch 21/105
2022-09-16 11:52:25,018 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:52:25,018 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:52:25,292 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:52:25,424 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:52:25,509 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:52:25,510 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:52:25,542 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:52:33,286 - INFO - joeynmt.prediction - Generation took 7.7361[sec]. (No references given)
2022-09-16 11:52:33,295 - INFO - joeynmt.training - Processing Predictions on Batch 22/105
2022-09-16 11:52:35,659 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:52:35,659 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:52:35,934 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:52:36,067 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:52:36,153 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:52:36,153 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:52:36,183 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:52:44,560 - INFO - joeynmt.prediction - Generation took 8.3681[sec]. (No references given)
2022-09-16 11:52:44,569 - INFO - joeynmt.training - Processing Predictions on Batch 23/105
2022-09-16 11:52:46,940 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:52:46,941 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:52:47,216 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:52:47,791 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:52:47,877 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:52:47,877 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:52:47,902 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:52:55,211 - INFO - joeynmt.prediction - Generation took 7.3012[sec]. (No references given)
2022-09-16 11:52:55,220 - INFO - joeynmt.training - Processing Predictions on Batch 24/105
2022-09-16 11:52:57,590 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:52:57,590 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:52:57,864 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:52:57,996 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:52:58,082 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:52:58,082 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:52:58,112 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:53:05,298 - INFO - joeynmt.prediction - Generation took 7.1780[sec]. (No references given)
2022-09-16 11:53:05,307 - INFO - joeynmt.training - Processing Predictions on Batch 25/105
2022-09-16 11:53:07,665 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:53:07,665 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:53:07,939 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:53:08,071 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:53:08,157 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:53:08,157 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:53:08,186 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:53:14,492 - INFO - joeynmt.prediction - Generation took 6.2972[sec]. (No references given)
2022-09-16 11:53:14,501 - INFO - joeynmt.training - Processing Predictions on Batch 26/105
2022-09-16 11:53:16,876 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:53:16,876 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:53:17,149 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:53:17,281 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:53:17,369 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:53:17,369 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:53:17,395 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:53:23,109 - INFO - joeynmt.prediction - Generation took 5.7063[sec]. (No references given)
2022-09-16 11:53:23,118 - INFO - joeynmt.training - Processing Predictions on Batch 27/105
2022-09-16 11:53:25,497 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:53:25,497 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:53:25,770 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:53:25,902 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:53:25,988 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:53:25,989 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:53:26,015 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:53:31,057 - INFO - joeynmt.prediction - Generation took 5.0339[sec]. (No references given)
2022-09-16 11:53:31,066 - INFO - joeynmt.training - Processing Predictions on Batch 28/105
2022-09-16 11:53:33,441 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:53:33,441 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:53:33,715 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:53:33,846 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:53:33,933 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:53:33,933 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:53:33,964 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:53:41,727 - INFO - joeynmt.prediction - Generation took 7.7554[sec]. (No references given)
2022-09-16 11:53:41,737 - INFO - joeynmt.training - Processing Predictions on Batch 29/105
2022-09-16 11:53:44,132 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:53:44,132 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:53:44,405 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:53:44,537 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:53:44,624 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:53:44,624 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:53:44,651 - INFO - joeynmt.prediction - Predicting 219 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:53:53,262 - INFO - joeynmt.prediction - Generation took 8.6028[sec]. (No references given)
2022-09-16 11:53:53,271 - INFO - joeynmt.training - Processing Predictions on Batch 30/105
2022-09-16 11:53:55,622 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:53:55,622 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:53:55,894 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:53:56,025 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:53:56,111 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:53:56,111 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:53:56,145 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:54:04,217 - INFO - joeynmt.prediction - Generation took 8.0634[sec]. (No references given)
2022-09-16 11:54:04,226 - INFO - joeynmt.training - Processing Predictions on Batch 31/105
2022-09-16 11:54:06,564 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:54:06,565 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:54:06,837 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:54:06,970 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:54:07,056 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:54:07,056 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:54:07,086 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:54:16,485 - INFO - joeynmt.prediction - Generation took 9.3894[sec]. (No references given)
2022-09-16 11:54:16,494 - INFO - joeynmt.training - Processing Predictions on Batch 32/105
2022-09-16 11:54:18,823 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:54:18,823 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:54:19,095 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:54:19,226 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:54:19,314 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:54:19,315 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:54:19,341 - INFO - joeynmt.prediction - Predicting 223 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:54:27,047 - INFO - joeynmt.prediction - Generation took 7.6984[sec]. (No references given)
2022-09-16 11:54:27,056 - INFO - joeynmt.training - Processing Predictions on Batch 33/105
2022-09-16 11:54:29,422 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:54:29,422 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:54:29,695 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:54:29,826 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:54:29,912 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:54:29,912 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:54:29,942 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:54:38,210 - INFO - joeynmt.prediction - Generation took 8.2602[sec]. (No references given)
2022-09-16 11:54:38,220 - INFO - joeynmt.training - Processing Predictions on Batch 34/105
2022-09-16 11:54:40,568 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:54:40,569 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:54:40,841 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:54:40,972 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:54:41,058 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:54:41,058 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:54:41,090 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:54:49,867 - INFO - joeynmt.prediction - Generation took 8.7684[sec]. (No references given)
2022-09-16 11:54:49,876 - INFO - joeynmt.training - Processing Predictions on Batch 35/105
2022-09-16 11:54:52,212 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:54:52,212 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:54:52,487 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:54:52,620 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:54:52,705 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:54:52,705 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:54:52,732 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:55:00,052 - INFO - joeynmt.prediction - Generation took 7.3128[sec]. (No references given)
2022-09-16 11:55:00,062 - INFO - joeynmt.training - Processing Predictions on Batch 36/105
2022-09-16 11:55:02,418 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:55:02,418 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:55:02,694 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:55:02,827 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:55:02,912 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:55:02,912 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:55:02,941 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:55:09,695 - INFO - joeynmt.prediction - Generation took 6.7454[sec]. (No references given)
2022-09-16 11:55:09,704 - INFO - joeynmt.training - Processing Predictions on Batch 37/105
2022-09-16 11:55:12,070 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:55:12,070 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:55:12,343 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:55:12,476 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:55:12,561 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:55:12,561 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:55:12,588 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:55:21,427 - INFO - joeynmt.prediction - Generation took 8.8304[sec]. (No references given)
2022-09-16 11:55:21,436 - INFO - joeynmt.training - Processing Predictions on Batch 38/105
2022-09-16 11:55:23,793 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:55:23,793 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:55:24,066 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:55:24,199 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:55:24,286 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:55:24,286 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:55:24,314 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:55:29,909 - INFO - joeynmt.prediction - Generation took 5.5876[sec]. (No references given)
2022-09-16 11:55:29,918 - INFO - joeynmt.training - Processing Predictions on Batch 39/105
2022-09-16 11:55:32,278 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:55:32,278 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:55:32,550 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:55:32,685 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:55:32,777 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:55:32,777 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:55:32,807 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:55:42,208 - INFO - joeynmt.prediction - Generation took 9.3926[sec]. (No references given)
2022-09-16 11:55:42,217 - INFO - joeynmt.training - Processing Predictions on Batch 40/105
2022-09-16 11:55:44,553 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:55:44,553 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:55:44,826 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:55:44,959 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:55:45,044 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:55:45,044 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:55:45,071 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:55:50,784 - INFO - joeynmt.prediction - Generation took 5.7054[sec]. (No references given)
2022-09-16 11:55:50,793 - INFO - joeynmt.training - Processing Predictions on Batch 41/105
2022-09-16 11:55:53,141 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:55:53,142 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:55:53,416 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:55:53,549 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:55:53,634 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:55:53,634 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:55:53,663 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:56:01,793 - INFO - joeynmt.prediction - Generation took 7.6744[sec]. (No references given)
2022-09-16 11:56:01,802 - INFO - joeynmt.training - Processing Predictions on Batch 42/105
2022-09-16 11:56:04,143 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:56:04,143 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:56:04,419 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:56:04,552 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:56:04,637 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:56:04,637 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:56:04,667 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:56:11,206 - INFO - joeynmt.prediction - Generation took 6.5312[sec]. (No references given)
2022-09-16 11:56:11,215 - INFO - joeynmt.training - Processing Predictions on Batch 43/105
2022-09-16 11:56:13,573 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:56:13,573 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:56:13,844 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:56:13,978 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:56:14,063 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:56:14,063 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:56:14,092 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:56:20,708 - INFO - joeynmt.prediction - Generation took 6.6085[sec]. (No references given)
2022-09-16 11:56:20,717 - INFO - joeynmt.training - Processing Predictions on Batch 44/105
2022-09-16 11:56:23,066 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:56:23,066 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:56:23,338 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:56:23,472 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:56:23,557 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:56:23,557 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:56:23,584 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:56:30,749 - INFO - joeynmt.prediction - Generation took 7.1576[sec]. (No references given)
2022-09-16 11:56:30,759 - INFO - joeynmt.training - Processing Predictions on Batch 45/105
2022-09-16 11:56:33,100 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:56:33,100 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:56:33,372 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:56:33,505 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:56:33,591 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:56:33,592 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:56:33,620 - INFO - joeynmt.prediction - Predicting 220 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:56:39,924 - INFO - joeynmt.prediction - Generation took 6.2974[sec]. (No references given)
2022-09-16 11:56:39,934 - INFO - joeynmt.training - Processing Predictions on Batch 46/105
2022-09-16 11:56:42,276 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:56:42,276 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:56:42,549 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:56:42,682 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:56:42,767 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:56:42,767 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:56:42,795 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:56:48,705 - INFO - joeynmt.prediction - Generation took 5.9029[sec]. (No references given)
2022-09-16 11:56:48,715 - INFO - joeynmt.training - Processing Predictions on Batch 47/105
2022-09-16 11:56:51,087 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:56:51,087 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:56:51,359 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:56:51,493 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:56:51,578 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:56:51,578 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:56:51,607 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:57:00,503 - INFO - joeynmt.prediction - Generation took 8.8874[sec]. (No references given)
2022-09-16 11:57:00,512 - INFO - joeynmt.training - Processing Predictions on Batch 48/105
2022-09-16 11:57:02,864 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:57:02,864 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:57:03,137 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:57:03,270 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:57:03,355 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:57:03,355 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:57:03,383 - INFO - joeynmt.prediction - Predicting 222 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:57:09,977 - INFO - joeynmt.prediction - Generation took 6.5865[sec]. (No references given)
2022-09-16 11:57:09,986 - INFO - joeynmt.training - Processing Predictions on Batch 49/105
2022-09-16 11:57:12,332 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:57:12,333 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:57:12,607 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:57:12,741 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:57:12,827 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:57:12,827 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:57:12,858 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:57:21,406 - INFO - joeynmt.prediction - Generation took 8.5398[sec]. (No references given)
2022-09-16 11:57:21,415 - INFO - joeynmt.training - Processing Predictions on Batch 50/105
2022-09-16 11:57:23,761 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:57:23,761 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:57:24,034 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:57:24,170 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:57:24,257 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:57:24,257 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:57:24,282 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:57:30,242 - INFO - joeynmt.prediction - Generation took 5.9524[sec]. (No references given)
2022-09-16 11:57:30,252 - INFO - joeynmt.training - Processing Predictions on Batch 51/105
2022-09-16 11:57:32,659 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:57:32,659 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:57:32,940 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:57:33,077 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:57:33,164 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:57:33,164 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:57:33,193 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:57:39,985 - INFO - joeynmt.prediction - Generation took 6.7839[sec]. (No references given)
2022-09-16 11:57:39,994 - INFO - joeynmt.training - Processing Predictions on Batch 52/105
2022-09-16 11:57:42,364 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:57:42,364 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:57:42,642 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:57:42,777 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:57:42,865 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:57:42,865 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:57:42,891 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:57:48,813 - INFO - joeynmt.prediction - Generation took 5.9145[sec]. (No references given)
2022-09-16 11:57:48,822 - INFO - joeynmt.training - Processing Predictions on Batch 53/105
2022-09-16 11:57:51,189 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:57:51,189 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:57:51,465 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:57:51,599 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:57:51,685 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:57:51,685 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:57:51,710 - INFO - joeynmt.prediction - Predicting 216 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:57:58,309 - INFO - joeynmt.prediction - Generation took 6.5920[sec]. (No references given)
2022-09-16 11:57:58,318 - INFO - joeynmt.training - Processing Predictions on Batch 54/105
2022-09-16 11:58:00,699 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:58:00,699 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:58:00,978 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:58:01,113 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:58:01,199 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:58:01,199 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:58:01,227 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:58:07,586 - INFO - joeynmt.prediction - Generation took 6.3512[sec]. (No references given)
2022-09-16 11:58:07,595 - INFO - joeynmt.training - Processing Predictions on Batch 55/105
2022-09-16 11:58:09,980 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:58:09,981 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:58:10,258 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:58:10,394 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:58:10,482 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:58:10,482 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:58:10,512 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:58:18,473 - INFO - joeynmt.prediction - Generation took 7.9523[sec]. (No references given)
2022-09-16 11:58:18,482 - INFO - joeynmt.training - Processing Predictions on Batch 56/105
2022-09-16 11:58:20,852 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:58:20,852 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:58:21,130 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:58:21,265 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:58:21,807 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:58:21,807 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:58:21,835 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:58:29,385 - INFO - joeynmt.prediction - Generation took 7.5414[sec]. (No references given)
2022-09-16 11:58:29,395 - INFO - joeynmt.training - Processing Predictions on Batch 57/105
2022-09-16 11:58:31,722 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:58:31,722 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:58:31,995 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:58:32,129 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:58:32,215 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:58:32,215 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:58:32,241 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:58:37,932 - INFO - joeynmt.prediction - Generation took 5.6827[sec]. (No references given)
2022-09-16 11:58:37,942 - INFO - joeynmt.training - Processing Predictions on Batch 58/105
2022-09-16 11:58:40,353 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:58:40,353 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:58:40,629 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:58:40,763 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:58:40,852 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:58:40,852 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:58:40,879 - INFO - joeynmt.prediction - Predicting 219 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:58:48,167 - INFO - joeynmt.prediction - Generation took 7.2793[sec]. (No references given)
2022-09-16 11:58:48,176 - INFO - joeynmt.training - Processing Predictions on Batch 59/105
2022-09-16 11:58:50,565 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:58:50,565 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:58:50,842 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:58:50,976 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:58:51,065 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:58:51,065 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:58:51,089 - INFO - joeynmt.prediction - Predicting 215 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:58:57,927 - INFO - joeynmt.prediction - Generation took 6.8314[sec]. (No references given)
2022-09-16 11:58:57,937 - INFO - joeynmt.training - Processing Predictions on Batch 60/105
2022-09-16 11:59:00,297 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:59:00,297 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:59:00,571 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:59:00,703 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:59:00,789 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:59:00,790 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:59:00,815 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:59:06,299 - INFO - joeynmt.prediction - Generation took 5.4758[sec]. (No references given)
2022-09-16 11:59:06,308 - INFO - joeynmt.training - Processing Predictions on Batch 61/105
2022-09-16 11:59:08,659 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:59:08,660 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:59:08,934 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:59:09,066 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:59:09,151 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:59:09,151 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:59:09,179 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:59:16,049 - INFO - joeynmt.prediction - Generation took 6.8616[sec]. (No references given)
2022-09-16 11:59:16,058 - INFO - joeynmt.training - Processing Predictions on Batch 62/105
2022-09-16 11:59:18,417 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:59:18,417 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:59:18,691 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:59:18,825 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:59:18,911 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:59:18,911 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:59:18,939 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:59:24,058 - INFO - joeynmt.prediction - Generation took 5.1114[sec]. (No references given)
2022-09-16 11:59:24,067 - INFO - joeynmt.training - Processing Predictions on Batch 63/105
2022-09-16 11:59:26,417 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:59:26,417 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:59:26,691 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:59:26,823 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:59:26,908 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:59:26,909 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:59:26,935 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:59:34,399 - INFO - joeynmt.prediction - Generation took 7.4555[sec]. (No references given)
2022-09-16 11:59:34,408 - INFO - joeynmt.training - Processing Predictions on Batch 64/105
2022-09-16 11:59:36,767 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:59:36,767 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:59:37,042 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:59:37,174 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:59:37,260 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:59:37,260 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:59:37,286 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:59:44,341 - INFO - joeynmt.prediction - Generation took 7.0472[sec]. (No references given)
2022-09-16 11:59:44,350 - INFO - joeynmt.training - Processing Predictions on Batch 65/105
2022-09-16 11:59:46,729 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:59:46,729 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:59:47,007 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:59:47,138 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:59:47,225 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:59:47,225 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:59:47,251 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:59:54,960 - INFO - joeynmt.prediction - Generation took 7.7022[sec]. (No references given)
2022-09-16 11:59:54,970 - INFO - joeynmt.training - Processing Predictions on Batch 66/105
2022-09-16 11:59:57,351 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:59:57,351 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:59:57,628 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:59:57,760 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 11:59:57,847 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:59:57,847 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 11:59:57,877 - INFO - joeynmt.prediction - Predicting 222 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:00:04,920 - INFO - joeynmt.prediction - Generation took 7.0345[sec]. (No references given)
2022-09-16 12:00:04,929 - INFO - joeynmt.training - Processing Predictions on Batch 67/105
2022-09-16 12:00:07,287 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:00:07,287 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:00:07,561 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:00:07,693 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:00:07,778 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:00:07,778 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:00:07,805 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:00:13,551 - INFO - joeynmt.prediction - Generation took 5.7379[sec]. (No references given)
2022-09-16 12:00:13,560 - INFO - joeynmt.training - Processing Predictions on Batch 68/105
2022-09-16 12:00:15,942 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:00:15,942 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:00:16,217 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:00:16,350 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:00:16,436 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:00:16,436 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:00:16,463 - INFO - joeynmt.prediction - Predicting 217 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:00:22,944 - INFO - joeynmt.prediction - Generation took 6.4722[sec]. (No references given)
2022-09-16 12:00:22,953 - INFO - joeynmt.training - Processing Predictions on Batch 69/105
2022-09-16 12:00:25,320 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:00:25,320 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:00:25,597 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:00:25,729 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:00:25,815 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:00:25,815 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:00:25,844 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:00:30,973 - INFO - joeynmt.prediction - Generation took 5.1211[sec]. (No references given)
2022-09-16 12:00:30,982 - INFO - joeynmt.training - Processing Predictions on Batch 70/105
2022-09-16 12:00:33,345 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:00:33,346 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:00:33,623 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:00:33,755 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:00:33,841 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:00:33,841 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:00:33,869 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:00:41,249 - INFO - joeynmt.prediction - Generation took 7.3723[sec]. (No references given)
2022-09-16 12:00:41,258 - INFO - joeynmt.training - Processing Predictions on Batch 71/105
2022-09-16 12:00:43,633 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:00:43,633 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:00:43,913 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:00:44,044 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:00:44,131 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:00:44,131 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:00:44,160 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:00:49,986 - INFO - joeynmt.prediction - Generation took 5.8190[sec]. (No references given)
2022-09-16 12:00:49,996 - INFO - joeynmt.training - Processing Predictions on Batch 72/105
2022-09-16 12:00:52,343 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:00:52,343 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:00:52,618 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:00:52,750 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:00:52,836 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:00:52,836 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:00:52,866 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:00:59,497 - INFO - joeynmt.prediction - Generation took 6.6228[sec]. (No references given)
2022-09-16 12:00:59,506 - INFO - joeynmt.training - Processing Predictions on Batch 73/105
2022-09-16 12:01:01,865 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:01:01,865 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:01:02,143 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:01:02,275 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:01:02,363 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:01:02,363 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:01:02,391 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:01:09,271 - INFO - joeynmt.prediction - Generation took 6.8731[sec]. (No references given)
2022-09-16 12:01:09,281 - INFO - joeynmt.training - Processing Predictions on Batch 74/105
2022-09-16 12:01:11,609 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:01:11,609 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:01:11,884 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:01:12,016 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:01:12,556 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:01:12,557 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:01:12,582 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:01:17,936 - INFO - joeynmt.prediction - Generation took 5.3461[sec]. (No references given)
2022-09-16 12:01:17,945 - INFO - joeynmt.training - Processing Predictions on Batch 75/105
2022-09-16 12:01:20,277 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:01:20,277 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:01:20,554 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:01:20,685 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:01:20,771 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:01:20,771 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:01:20,799 - INFO - joeynmt.prediction - Predicting 217 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:01:27,275 - INFO - joeynmt.prediction - Generation took 6.4678[sec]. (No references given)
2022-09-16 12:01:27,284 - INFO - joeynmt.training - Processing Predictions on Batch 76/105
2022-09-16 12:01:29,615 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:01:29,615 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:01:29,888 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:01:30,019 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:01:30,105 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:01:30,105 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:01:30,133 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:01:36,779 - INFO - joeynmt.prediction - Generation took 6.6388[sec]. (No references given)
2022-09-16 12:01:36,788 - INFO - joeynmt.training - Processing Predictions on Batch 77/105
2022-09-16 12:01:39,119 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:01:39,119 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:01:39,393 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:01:39,524 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:01:39,610 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:01:39,611 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:01:39,638 - INFO - joeynmt.prediction - Predicting 220 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:01:44,722 - INFO - joeynmt.prediction - Generation took 5.0768[sec]. (No references given)
2022-09-16 12:01:44,731 - INFO - joeynmt.training - Processing Predictions on Batch 78/105
2022-09-16 12:01:47,102 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:01:47,102 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:01:47,376 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:01:47,508 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:01:47,594 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:01:47,594 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:01:47,619 - INFO - joeynmt.prediction - Predicting 216 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:01:53,202 - INFO - joeynmt.prediction - Generation took 5.5766[sec]. (No references given)
2022-09-16 12:01:53,212 - INFO - joeynmt.training - Processing Predictions on Batch 79/105
2022-09-16 12:01:55,558 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:01:55,558 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:01:55,830 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:01:55,961 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:01:56,048 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:01:56,048 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:01:56,076 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:02:02,654 - INFO - joeynmt.prediction - Generation took 6.5704[sec]. (No references given)
2022-09-16 12:02:02,663 - INFO - joeynmt.training - Processing Predictions on Batch 80/105
2022-09-16 12:02:05,025 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:02:05,025 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:02:05,298 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:02:05,429 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:02:05,517 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:02:05,517 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:02:05,543 - INFO - joeynmt.prediction - Predicting 214 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:02:10,600 - INFO - joeynmt.prediction - Generation took 5.0500[sec]. (No references given)
2022-09-16 12:02:10,609 - INFO - joeynmt.training - Processing Predictions on Batch 81/105
2022-09-16 12:02:12,945 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:02:12,945 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:02:13,220 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:02:13,351 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:02:13,437 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:02:13,438 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:02:13,466 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:02:21,394 - INFO - joeynmt.prediction - Generation took 7.9195[sec]. (No references given)
2022-09-16 12:02:21,403 - INFO - joeynmt.training - Processing Predictions on Batch 82/105
2022-09-16 12:02:23,766 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:02:23,767 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:02:24,043 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:02:24,175 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:02:24,261 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:02:24,261 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:02:24,290 - INFO - joeynmt.prediction - Predicting 219 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:02:30,737 - INFO - joeynmt.prediction - Generation took 6.4400[sec]. (No references given)
2022-09-16 12:02:30,747 - INFO - joeynmt.training - Processing Predictions on Batch 83/105
2022-09-16 12:02:33,106 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:02:33,107 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:02:33,379 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:02:33,511 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:02:33,597 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:02:33,597 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:02:33,626 - INFO - joeynmt.prediction - Predicting 220 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:02:40,931 - INFO - joeynmt.prediction - Generation took 7.2971[sec]. (No references given)
2022-09-16 12:02:40,940 - INFO - joeynmt.training - Processing Predictions on Batch 84/105
2022-09-16 12:02:43,321 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:02:43,321 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:02:43,596 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:02:43,728 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:02:43,814 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:02:43,815 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:02:43,841 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:02:49,634 - INFO - joeynmt.prediction - Generation took 5.7844[sec]. (No references given)
2022-09-16 12:02:49,643 - INFO - joeynmt.training - Processing Predictions on Batch 85/105
2022-09-16 12:02:52,000 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:02:52,000 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:02:52,273 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:02:52,405 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:02:52,491 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:02:52,491 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:02:52,517 - INFO - joeynmt.prediction - Predicting 218 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:02:58,445 - INFO - joeynmt.prediction - Generation took 5.9213[sec]. (No references given)
2022-09-16 12:02:58,454 - INFO - joeynmt.training - Processing Predictions on Batch 86/105
2022-09-16 12:03:00,823 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:03:00,823 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:03:01,096 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:03:01,228 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:03:01,314 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:03:01,314 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:03:01,341 - INFO - joeynmt.prediction - Predicting 218 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:03:07,279 - INFO - joeynmt.prediction - Generation took 5.9296[sec]. (No references given)
2022-09-16 12:03:07,288 - INFO - joeynmt.training - Processing Predictions on Batch 87/105
2022-09-16 12:03:09,650 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:03:09,650 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:03:09,924 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:03:10,056 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:03:10,143 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:03:10,144 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:03:10,173 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:03:18,280 - INFO - joeynmt.prediction - Generation took 8.0989[sec]. (No references given)
2022-09-16 12:03:18,289 - INFO - joeynmt.training - Processing Predictions on Batch 88/105
2022-09-16 12:03:20,648 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:03:20,648 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:03:20,922 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:03:21,054 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:03:21,140 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:03:21,141 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:03:21,167 - INFO - joeynmt.prediction - Predicting 219 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:03:28,474 - INFO - joeynmt.prediction - Generation took 7.2996[sec]. (No references given)
2022-09-16 12:03:28,483 - INFO - joeynmt.training - Processing Predictions on Batch 89/105
2022-09-16 12:03:30,877 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:03:30,877 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:03:31,150 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:03:31,283 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:03:31,371 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:03:31,371 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:03:31,398 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:03:38,529 - INFO - joeynmt.prediction - Generation took 7.1223[sec]. (No references given)
2022-09-16 12:03:38,538 - INFO - joeynmt.training - Processing Predictions on Batch 90/105
2022-09-16 12:03:40,902 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:03:40,902 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:03:41,176 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:03:41,307 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:03:41,394 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:03:41,394 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:03:41,422 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:03:49,311 - INFO - joeynmt.prediction - Generation took 7.8813[sec]. (No references given)
2022-09-16 12:03:49,321 - INFO - joeynmt.training - Processing Predictions on Batch 91/105
2022-09-16 12:03:51,652 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:03:51,652 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:03:51,925 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:03:52,058 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:03:52,144 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:03:52,145 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:03:52,172 - INFO - joeynmt.prediction - Predicting 220 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:03:59,336 - INFO - joeynmt.prediction - Generation took 7.1558[sec]. (No references given)
2022-09-16 12:03:59,345 - INFO - joeynmt.training - Processing Predictions on Batch 92/105
2022-09-16 12:04:01,670 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:04:01,670 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:04:01,943 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:04:02,075 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:04:02,161 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:04:02,162 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:04:02,185 - INFO - joeynmt.prediction - Predicting 218 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:04:06,906 - INFO - joeynmt.prediction - Generation took 4.7136[sec]. (No references given)
2022-09-16 12:04:06,915 - INFO - joeynmt.training - Processing Predictions on Batch 93/105
2022-09-16 12:04:09,269 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:04:09,269 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:04:09,543 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:04:09,675 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:04:09,761 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:04:09,761 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:04:09,793 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:04:17,879 - INFO - joeynmt.prediction - Generation took 8.0776[sec]. (No references given)
2022-09-16 12:04:17,888 - INFO - joeynmt.training - Processing Predictions on Batch 94/105
2022-09-16 12:04:20,261 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:04:20,261 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:04:20,534 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:04:20,668 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:04:20,754 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:04:20,755 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:04:20,782 - INFO - joeynmt.prediction - Predicting 223 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:04:29,343 - INFO - joeynmt.prediction - Generation took 8.5525[sec]. (No references given)
2022-09-16 12:04:29,352 - INFO - joeynmt.training - Processing Predictions on Batch 95/105
2022-09-16 12:04:31,706 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:04:31,706 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:04:31,980 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:04:32,114 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:04:32,200 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:04:32,200 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:04:32,228 - INFO - joeynmt.prediction - Predicting 219 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:04:39,164 - INFO - joeynmt.prediction - Generation took 6.9292[sec]. (No references given)
2022-09-16 12:04:39,173 - INFO - joeynmt.training - Processing Predictions on Batch 96/105
2022-09-16 12:04:41,543 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:04:41,543 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:04:41,817 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:04:41,951 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:04:42,495 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:04:42,495 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:04:42,524 - INFO - joeynmt.prediction - Predicting 219 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:04:49,215 - INFO - joeynmt.prediction - Generation took 6.6839[sec]. (No references given)
2022-09-16 12:04:49,224 - INFO - joeynmt.training - Processing Predictions on Batch 97/105
2022-09-16 12:04:51,602 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:04:51,602 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:04:51,878 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:04:52,013 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:04:52,099 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:04:52,099 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:04:52,125 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:04:59,153 - INFO - joeynmt.prediction - Generation took 7.0196[sec]. (No references given)
2022-09-16 12:04:59,162 - INFO - joeynmt.training - Processing Predictions on Batch 98/105
2022-09-16 12:05:01,487 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:05:01,487 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:05:01,763 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:05:01,895 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:05:01,981 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:05:01,981 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:05:02,010 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:05:08,625 - INFO - joeynmt.prediction - Generation took 6.6080[sec]. (No references given)
2022-09-16 12:05:08,634 - INFO - joeynmt.training - Processing Predictions on Batch 99/105
2022-09-16 12:05:11,006 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:05:11,006 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:05:11,280 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:05:11,414 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:05:11,499 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:05:11,499 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:05:11,523 - INFO - joeynmt.prediction - Predicting 215 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:05:18,668 - INFO - joeynmt.prediction - Generation took 7.1379[sec]. (No references given)
2022-09-16 12:05:18,677 - INFO - joeynmt.training - Processing Predictions on Batch 100/105
2022-09-16 12:05:21,031 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:05:21,032 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:05:21,307 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:05:21,439 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:05:21,526 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:05:21,526 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:05:21,554 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:05:28,317 - INFO - joeynmt.prediction - Generation took 6.7556[sec]. (No references given)
2022-09-16 12:05:28,327 - INFO - joeynmt.training - Processing Predictions on Batch 101/105
2022-09-16 12:05:30,720 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:05:30,720 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:05:30,993 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:05:31,126 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:05:31,211 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:05:31,212 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:05:31,240 - INFO - joeynmt.prediction - Predicting 222 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:05:38,886 - INFO - joeynmt.prediction - Generation took 7.6379[sec]. (No references given)
2022-09-16 12:05:38,896 - INFO - joeynmt.training - Processing Predictions on Batch 102/105
2022-09-16 12:05:41,270 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:05:41,270 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:05:41,546 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:05:41,678 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:05:41,764 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:05:41,764 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:05:41,794 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:05:47,887 - INFO - joeynmt.prediction - Generation took 6.0852[sec]. (No references given)
2022-09-16 12:05:47,896 - INFO - joeynmt.training - Processing Predictions on Batch 103/105
2022-09-16 12:05:50,268 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:05:50,268 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:05:50,544 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:05:50,677 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:05:50,763 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:05:50,763 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:05:50,787 - INFO - joeynmt.prediction - Predicting 216 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:05:57,201 - INFO - joeynmt.prediction - Generation took 6.4061[sec]. (No references given)
2022-09-16 12:05:57,210 - INFO - joeynmt.training - Processing Predictions on Batch 104/105
2022-09-16 12:05:59,547 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 12:05:59,548 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 12:05:59,820 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 12:05:59,953 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259000.ckpt.
2022-09-16 12:06:00,038 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:06:00,038 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 12:06:00,053 - INFO - joeynmt.prediction - Predicting 108 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:06:03,509 - INFO - joeynmt.prediction - Generation took 3.4518[sec]. (No references given)
2022-09-16 12:06:03,683 - INFO - joeynmt.training - Final Query Indices picked: [232380, 16365, 196375, 325019, 67628, 94776, 140202, 140272, 70949, 45002] length: 10000
2022-09-16 12:06:03,684 - INFO - joeynmt.training - Query the samples, one at a time (interactive/file)
2022-09-16 12:06:05,927 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 4096
	effective batch size (w. parallel & accumulation): 4096
2022-09-16 12:06:05,927 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:21:05,351 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 12:21:05,353 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.57, loss:   2.48, ppl:  12.00, acc:   0.52, generation: 891.7467[sec], evaluation: 7.0026[sec]
2022-09-16 12:21:07,418 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/256000.ckpt
2022-09-16 12:21:07,448 - INFO - joeynmt.training - Example #0
2022-09-16 12:21:07,459 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 12:21:07,459 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 12:21:07,459 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 12:21:07,459 - INFO - joeynmt.training - Example #1
2022-09-16 12:21:07,470 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 12:21:07,470 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 12:21:07,470 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 12:21:07,470 - INFO - joeynmt.training - Example #2
2022-09-16 12:21:07,480 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 12:21:07,480 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 12:21:07,480 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 12:21:07,480 - INFO - joeynmt.training - Example #3
2022-09-16 12:21:07,491 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 12:21:07,491 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 12:21:07,491 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 12:21:07,588 - INFO - joeynmt.training - EPOCH 1
2022-09-16 12:21:16,839 - INFO - joeynmt.training - Epoch   1, Step:   259500, Batch Loss:     2.503163, Batch Acc: 0.010615, Tokens per Sec:     4797, Lr: 0.000035
2022-09-16 12:21:39,949 - INFO - joeynmt.training - Epoch   1, Step:   259600, Batch Loss:     2.414726, Batch Acc: 0.004836, Tokens per Sec:     4939, Lr: 0.000035
2022-09-16 12:22:02,896 - INFO - joeynmt.training - Epoch   1, Step:   259700, Batch Loss:     2.514026, Batch Acc: 0.004467, Tokens per Sec:     4946, Lr: 0.000035
2022-09-16 12:22:25,993 - INFO - joeynmt.training - Epoch   1, Step:   259800, Batch Loss:     2.537317, Batch Acc: 0.005329, Tokens per Sec:     5013, Lr: 0.000035
2022-09-16 12:22:49,026 - INFO - joeynmt.training - Epoch   1, Step:   259900, Batch Loss:     2.491617, Batch Acc: 0.005217, Tokens per Sec:     4969, Lr: 0.000035
2022-09-16 12:23:12,038 - INFO - joeynmt.training - Epoch   1, Step:   260000, Batch Loss:     2.524823, Batch Acc: 0.004032, Tokens per Sec:     4925, Lr: 0.000035
2022-09-16 12:23:12,038 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:38:31,024 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 12:38:31,026 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.54, loss:   2.48, ppl:  11.98, acc:   0.52, generation: 911.7348[sec], evaluation: 6.9111[sec]
2022-09-16 12:38:32,547 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/257000.ckpt
2022-09-16 12:38:32,574 - INFO - joeynmt.training - Example #0
2022-09-16 12:38:32,586 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 12:38:32,586 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 12:38:32,586 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 12:38:32,586 - INFO - joeynmt.training - Example #1
2022-09-16 12:38:32,597 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 12:38:32,597 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 12:38:32,597 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 12:38:32,597 - INFO - joeynmt.training - Example #2
2022-09-16 12:38:32,607 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 12:38:32,607 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 12:38:32,607 - INFO - joeynmt.training - 	Hypothesis: अभिनय , गति और माइम
2022-09-16 12:38:32,607 - INFO - joeynmt.training - Example #3
2022-09-16 12:38:32,618 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 12:38:32,618 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 12:38:32,618 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 12:38:55,767 - INFO - joeynmt.training - Epoch   1, Step:   260100, Batch Loss:     2.677548, Batch Acc: 0.004506, Tokens per Sec:     4620, Lr: 0.000035
2022-09-16 12:39:18,556 - INFO - joeynmt.training - Epoch   1, Step:   260200, Batch Loss:     2.650980, Batch Acc: 0.004249, Tokens per Sec:     4999, Lr: 0.000035
2022-09-16 12:39:41,518 - INFO - joeynmt.training - Epoch   1, Step:   260300, Batch Loss:     2.569566, Batch Acc: 0.004038, Tokens per Sec:     5015, Lr: 0.000035
2022-09-16 12:40:04,304 - INFO - joeynmt.training - Epoch   1, Step:   260400, Batch Loss:     2.541424, Batch Acc: 0.005274, Tokens per Sec:     5027, Lr: 0.000035
2022-09-16 12:40:27,176 - INFO - joeynmt.training - Epoch   1, Step:   260500, Batch Loss:     2.502177, Batch Acc: 0.004928, Tokens per Sec:     5004, Lr: 0.000035
2022-09-16 12:40:50,019 - INFO - joeynmt.training - Epoch   1, Step:   260600, Batch Loss:     2.630968, Batch Acc: 0.004658, Tokens per Sec:     4990, Lr: 0.000035
2022-09-16 12:41:12,853 - INFO - joeynmt.training - Epoch   1, Step:   260700, Batch Loss:     2.597475, Batch Acc: 0.005564, Tokens per Sec:     5029, Lr: 0.000035
2022-09-16 12:41:35,706 - INFO - joeynmt.training - Epoch   1, Step:   260800, Batch Loss:     2.536676, Batch Acc: 0.004385, Tokens per Sec:     5060, Lr: 0.000035
2022-09-16 12:41:58,611 - INFO - joeynmt.training - Epoch   1, Step:   260900, Batch Loss:     2.320221, Batch Acc: 0.004810, Tokens per Sec:     4974, Lr: 0.000035
2022-09-16 12:42:21,503 - INFO - joeynmt.training - Epoch   1, Step:   261000, Batch Loss:     2.681345, Batch Acc: 0.004358, Tokens per Sec:     4932, Lr: 0.000035
2022-09-16 12:42:21,503 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:57:44,875 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 12:57:44,877 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.45, loss:   2.48, ppl:  11.95, acc:   0.52, generation: 916.0915[sec], evaluation: 6.9403[sec]
2022-09-16 12:57:46,378 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/259461.ckpt
2022-09-16 12:57:46,421 - INFO - joeynmt.training - Example #0
2022-09-16 12:57:46,433 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 12:57:46,433 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 12:57:46,433 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 12:57:46,433 - INFO - joeynmt.training - Example #1
2022-09-16 12:57:46,444 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 12:57:46,444 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 12:57:46,444 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 12:57:46,444 - INFO - joeynmt.training - Example #2
2022-09-16 12:57:46,454 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 12:57:46,455 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 12:57:46,455 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और माइम
2022-09-16 12:57:46,455 - INFO - joeynmt.training - Example #3
2022-09-16 12:57:46,465 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 12:57:46,465 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 12:57:46,465 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 12:58:09,446 - INFO - joeynmt.training - Epoch   1, Step:   261100, Batch Loss:     2.560351, Batch Acc: 0.004725, Tokens per Sec:     4618, Lr: 0.000035
2022-09-16 12:58:32,285 - INFO - joeynmt.training - Epoch   1, Step:   261200, Batch Loss:     2.706287, Batch Acc: 0.004392, Tokens per Sec:     5005, Lr: 0.000035
2022-09-16 12:58:55,200 - INFO - joeynmt.training - Epoch   1, Step:   261300, Batch Loss:     2.479721, Batch Acc: 0.004886, Tokens per Sec:     5020, Lr: 0.000035
2022-09-16 12:59:18,049 - INFO - joeynmt.training - Epoch   1, Step:   261400, Batch Loss:     2.736337, Batch Acc: 0.004254, Tokens per Sec:     4969, Lr: 0.000035
2022-09-16 12:59:40,929 - INFO - joeynmt.training - Epoch   1, Step:   261500, Batch Loss:     2.855010, Batch Acc: 0.003865, Tokens per Sec:     4987, Lr: 0.000035
2022-09-16 13:00:03,809 - INFO - joeynmt.training - Epoch   1, Step:   261600, Batch Loss:     2.674901, Batch Acc: 0.004087, Tokens per Sec:     4951, Lr: 0.000035
2022-09-16 13:00:26,590 - INFO - joeynmt.training - Epoch   1, Step:   261700, Batch Loss:     2.568292, Batch Acc: 0.004738, Tokens per Sec:     5003, Lr: 0.000035
2022-09-16 13:00:49,397 - INFO - joeynmt.training - Epoch   1, Step:   261800, Batch Loss:     2.667754, Batch Acc: 0.004247, Tokens per Sec:     4987, Lr: 0.000035
2022-09-16 13:01:12,273 - INFO - joeynmt.training - Epoch   1, Step:   261900, Batch Loss:     2.457977, Batch Acc: 0.005095, Tokens per Sec:     4976, Lr: 0.000035
2022-09-16 13:01:35,128 - INFO - joeynmt.training - Epoch   1, Step:   262000, Batch Loss:     2.598404, Batch Acc: 0.004749, Tokens per Sec:     5049, Lr: 0.000035
2022-09-16 13:01:35,128 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 13:17:13,537 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 13:17:13,538 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.56, loss:   2.48, ppl:  11.93, acc:   0.52, generation: 931.0198[sec], evaluation: 6.6899[sec]
2022-09-16 13:17:13,542 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 13:17:15,159 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/260000.ckpt
2022-09-16 13:17:15,202 - INFO - joeynmt.training - Example #0
2022-09-16 13:17:15,214 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 13:17:15,214 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 13:17:15,214 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 13:17:15,214 - INFO - joeynmt.training - Example #1
2022-09-16 13:17:15,225 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 13:17:15,225 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 13:17:15,225 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन प्ले करें
2022-09-16 13:17:15,225 - INFO - joeynmt.training - Example #2
2022-09-16 13:17:15,236 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 13:17:15,236 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 13:17:15,236 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 13:17:15,236 - INFO - joeynmt.training - Example #3
2022-09-16 13:17:15,247 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 13:17:15,247 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 13:17:15,247 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 13:17:38,273 - INFO - joeynmt.training - Epoch   1, Step:   262100, Batch Loss:     2.487758, Batch Acc: 0.004799, Tokens per Sec:     4584, Lr: 0.000035
2022-09-16 13:18:01,109 - INFO - joeynmt.training - Epoch   1, Step:   262200, Batch Loss:     2.745907, Batch Acc: 0.004021, Tokens per Sec:     5064, Lr: 0.000035
2022-09-16 13:18:23,967 - INFO - joeynmt.training - Epoch   1, Step:   262300, Batch Loss:     2.446647, Batch Acc: 0.004101, Tokens per Sec:     5078, Lr: 0.000035
2022-09-16 13:18:46,767 - INFO - joeynmt.training - Epoch   1, Step:   262400, Batch Loss:     2.522201, Batch Acc: 0.004735, Tokens per Sec:     5039, Lr: 0.000035
2022-09-16 13:19:09,632 - INFO - joeynmt.training - Epoch   1, Step:   262500, Batch Loss:     2.486741, Batch Acc: 0.004884, Tokens per Sec:     5015, Lr: 0.000035
2022-09-16 13:19:32,494 - INFO - joeynmt.training - Epoch   1, Step:   262600, Batch Loss:     2.595436, Batch Acc: 0.004975, Tokens per Sec:     5021, Lr: 0.000035
2022-09-16 13:19:55,352 - INFO - joeynmt.training - Epoch   1, Step:   262700, Batch Loss:     2.423958, Batch Acc: 0.005128, Tokens per Sec:     4999, Lr: 0.000035
2022-09-16 13:20:18,081 - INFO - joeynmt.training - Epoch   1, Step:   262800, Batch Loss:     2.537260, Batch Acc: 0.003903, Tokens per Sec:     5017, Lr: 0.000035
2022-09-16 13:20:40,971 - INFO - joeynmt.training - Epoch   1, Step:   262900, Batch Loss:     2.481249, Batch Acc: 0.004457, Tokens per Sec:     5097, Lr: 0.000035
2022-09-16 13:21:03,786 - INFO - joeynmt.training - Epoch   1, Step:   263000, Batch Loss:     2.702125, Batch Acc: 0.005324, Tokens per Sec:     5014, Lr: 0.000035
2022-09-16 13:21:03,786 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 13:36:23,577 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 13:36:23,579 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.57, loss:   2.48, ppl:  11.96, acc:   0.52, generation: 912.3526[sec], evaluation: 7.0943[sec]
2022-09-16 13:36:23,591 - INFO - joeynmt.training - Example #0
2022-09-16 13:36:23,603 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 13:36:23,603 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 13:36:23,603 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 13:36:23,603 - INFO - joeynmt.training - Example #1
2022-09-16 13:36:23,614 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 13:36:23,614 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 13:36:23,614 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 13:36:23,614 - INFO - joeynmt.training - Example #2
2022-09-16 13:36:23,625 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 13:36:23,625 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 13:36:23,625 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 13:36:23,625 - INFO - joeynmt.training - Example #3
2022-09-16 13:36:23,636 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 13:36:23,636 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 13:36:23,636 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 13:36:46,546 - INFO - joeynmt.training - Epoch   1, Step:   263100, Batch Loss:     2.746699, Batch Acc: 0.004606, Tokens per Sec:     4926, Lr: 0.000035
2022-09-16 13:37:09,411 - INFO - joeynmt.training - Epoch   1, Step:   263200, Batch Loss:     2.652928, Batch Acc: 0.004387, Tokens per Sec:     5025, Lr: 0.000035
2022-09-16 13:37:32,229 - INFO - joeynmt.training - Epoch   1, Step:   263300, Batch Loss:     2.640841, Batch Acc: 0.003993, Tokens per Sec:     4994, Lr: 0.000035
2022-09-16 13:37:54,995 - INFO - joeynmt.training - Epoch   1, Step:   263400, Batch Loss:     2.584430, Batch Acc: 0.004230, Tokens per Sec:     5026, Lr: 0.000035
2022-09-16 13:38:17,770 - INFO - joeynmt.training - Epoch   1, Step:   263500, Batch Loss:     2.465505, Batch Acc: 0.004386, Tokens per Sec:     4956, Lr: 0.000035
2022-09-16 13:38:40,515 - INFO - joeynmt.training - Epoch   1, Step:   263600, Batch Loss:     2.525535, Batch Acc: 0.004673, Tokens per Sec:     4930, Lr: 0.000035
2022-09-16 13:39:03,305 - INFO - joeynmt.training - Epoch   1, Step:   263700, Batch Loss:     2.524276, Batch Acc: 0.004418, Tokens per Sec:     4927, Lr: 0.000035
2022-09-16 13:39:26,122 - INFO - joeynmt.training - Epoch   1, Step:   263800, Batch Loss:     2.541955, Batch Acc: 0.005156, Tokens per Sec:     5041, Lr: 0.000035
2022-09-16 13:39:48,908 - INFO - joeynmt.training - Epoch   1, Step:   263900, Batch Loss:     2.768054, Batch Acc: 0.004753, Tokens per Sec:     5051, Lr: 0.000035
2022-09-16 13:40:11,739 - INFO - joeynmt.training - Epoch   1, Step:   264000, Batch Loss:     2.664146, Batch Acc: 0.004558, Tokens per Sec:     5064, Lr: 0.000035
2022-09-16 13:40:11,739 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 13:55:20,788 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 13:55:20,789 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.60, loss:   2.48, ppl:  11.92, acc:   0.52, generation: 902.0820[sec], evaluation: 6.6245[sec]
2022-09-16 13:55:20,793 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 13:55:22,283 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_margin/261000.ckpt
2022-09-16 13:55:22,327 - INFO - joeynmt.training - Example #0
2022-09-16 13:55:22,339 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 13:55:22,339 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 13:55:22,339 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 13:55:22,339 - INFO - joeynmt.training - Example #1
2022-09-16 13:55:22,350 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 13:55:22,350 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 13:55:22,350 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 13:55:22,350 - INFO - joeynmt.training - Example #2
2022-09-16 13:55:22,361 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 13:55:22,361 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 13:55:22,361 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और माइम
2022-09-16 13:55:22,361 - INFO - joeynmt.training - Example #3
2022-09-16 13:55:22,372 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 13:55:22,372 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 13:55:22,372 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 13:55:45,334 - INFO - joeynmt.training - Epoch   1, Step:   264100, Batch Loss:     2.654667, Batch Acc: 0.004708, Tokens per Sec:     4769, Lr: 0.000035
2022-09-16 13:56:08,121 - INFO - joeynmt.training - Epoch   1, Step:   264200, Batch Loss:     2.647632, Batch Acc: 0.004381, Tokens per Sec:     5079, Lr: 0.000035
2022-09-16 13:56:31,218 - INFO - joeynmt.training - Epoch   1, Step:   264300, Batch Loss:     2.585308, Batch Acc: 0.004668, Tokens per Sec:     4897, Lr: 0.000035
2022-09-16 13:56:54,022 - INFO - joeynmt.training - Epoch   1, Step:   264400, Batch Loss:     2.546415, Batch Acc: 0.004083, Tokens per Sec:     5080, Lr: 0.000035
2022-09-16 13:57:16,810 - INFO - joeynmt.training - Epoch   1, Step:   264500, Batch Loss:     2.468932, Batch Acc: 0.004131, Tokens per Sec:     4972, Lr: 0.000035
2022-09-16 13:57:39,692 - INFO - joeynmt.training - Epoch   1, Step:   264600, Batch Loss:     2.529605, Batch Acc: 0.005020, Tokens per Sec:     5032, Lr: 0.000035
2022-09-16 13:58:02,560 - INFO - joeynmt.training - Epoch   1, Step:   264700, Batch Loss:     2.460216, Batch Acc: 0.004899, Tokens per Sec:     5044, Lr: 0.000035
2022-09-16 13:58:25,450 - INFO - joeynmt.training - Epoch   1, Step:   264800, Batch Loss:     2.463950, Batch Acc: 0.004093, Tokens per Sec:     5070, Lr: 0.000035
2022-09-16 13:58:48,232 - INFO - joeynmt.training - Epoch   1, Step:   264900, Batch Loss:     2.731308, Batch Acc: 0.004545, Tokens per Sec:     5012, Lr: 0.000035
2022-09-16 13:59:10,931 - INFO - joeynmt.training - Epoch   1, Step:   265000, Batch Loss:     2.621557, Batch Acc: 0.004642, Tokens per Sec:     4983, Lr: 0.000035
2022-09-16 11:56:48,196 - INFO - joeynmt.training - Epoch   1: total training loss 38205.03
2022-09-16 11:56:48,196 - INFO - joeynmt.training - Training ended after   1 epochs.
2022-09-16 11:56:48,197 - INFO - joeynmt.training - Best validation result (greedy) at step   273000:  11.71 ppl.
2022-09-16 11:56:48,197 - INFO - joeynmt.training - Loading from ckpt file: /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/273000.ckpt
2022-09-16 11:56:48,215 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:56:48,215 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:56:48,549 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:56:48,554 - INFO - joeynmt.model - Total params: 19302144
2022-09-16 11:56:48,821 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/273000.ckpt.
2022-09-16 11:56:48,887 - INFO - joeynmt.prediction - Decoding on dev set...
2022-09-16 11:56:48,887 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:22:50,694 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 12:22:50,696 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  24.25, generation: 1551.1021[sec], evaluation: 10.1985[sec]
2022-09-16 12:22:50,757 - INFO - joeynmt.prediction - Translations saved to: /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/00273000.hyps.dev.
2022-09-16 12:22:50,757 - INFO - joeynmt.prediction - Decoding on test set...
2022-09-16 12:22:50,757 - INFO - joeynmt.prediction - Predicting 40858 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:47:40,361 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 12:47:40,363 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  23.20, generation: 1478.8113[sec], evaluation: 10.2912[sec]
2022-09-16 12:47:40,434 - INFO - joeynmt.prediction - Translations saved to: /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/00273000.hyps.test.
2022-09-16 12:47:40,447 - INFO - joeynmt.training - ACTIVE LEARNING MODEL END - MARGIN 5
