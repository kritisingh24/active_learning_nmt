2022-09-14 22:36:00,214 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                           cfg.name : transformer_100_enhi_bpe
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                     cfg.data.train : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/train_tok
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                       cfg.data.dev : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/val_tok
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                      cfg.data.test : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/test_tok
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain_ac
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -            cfg.data.src.max_length : 60
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/vocab.en
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/en.bpe.codes
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 16000
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : moses
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : hi
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -            cfg.data.trg.max_length : 60
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/vocab.hi
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/data/datasets_enhi_100/hi.bpe.codes
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 16000
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -                 cfg.testing.n_best : 1
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -             cfg.testing.batch_size : 1024
2022-09-14 22:36:00,215 - INFO - joeynmt.helpers -             cfg.testing.batch_type : token
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -      cfg.testing.max_output_length : 130
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -           cfg.testing.eval_metrics : ['bleu']
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -            cfg.testing.return_prob : none
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -       cfg.testing.return_attention : False
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.tokenize : 13a
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.lowercase : False
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -     cfg.active_learning.query_size : 10000
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -    cfg.active_learning.interactive : False
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -      cfg.active_learning.pool_size : 6
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -     cfg.active_learning.batch_size : 256
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -    cfg.active_learning.num_workers : 4
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -    cfg.active_learning.num_queries : 32
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -     cfg.active_learning.al_percent : 30
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers - cfg.active_learning.query_strategy : margin
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -          cfg.active_learning.epoch : 2
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers - cfg.active_learning.validation_freq : 1000
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -            cfg.training.load_model : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/enhi_transformer_t1_r3/180000.ckpt
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -       cfg.training.reset_best_ckpt : False
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -       cfg.training.reset_scheduler : False
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -       cfg.training.reset_optimizer : False
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -      cfg.training.reset_iter_state : False
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers - cfg.training.learning_rate_decay_length : 2500
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -    cfg.training.learning_rate_peak : 0.005
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -  cfg.training.learning_rate_warmup : 1000
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -  cfg.training.learning_rate_factor : 0.5
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -              cfg.training.patience : 5
2022-09-14 22:36:00,216 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 1e-08
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.1
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -            cfg.training.batch_size : 4096
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 1
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -                cfg.training.epochs : 1
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 1000
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -             cfg.training.model_dir : /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3]
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 3
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : False
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2022-09-14 22:36:00,217 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 1024
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.3
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers -       cfg.model.encoder.layer_norm : pre
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 4
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 1024
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.3
2022-09-14 22:36:00,218 - INFO - joeynmt.helpers -       cfg.model.decoder.layer_norm : pre
2022-09-14 22:36:01,297 - INFO - joeynmt.data - Building tokenizer...
2022-09-14 22:36:01,421 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-14 22:36:01,422 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-14 22:36:01,422 - INFO - joeynmt.data - Loading train set...
2022-09-14 22:39:02,000 - INFO - joeynmt.data - Building vocabulary...
2022-09-14 22:39:04,117 - INFO - joeynmt.data - Loading dev set...
2022-09-14 22:39:09,650 - INFO - joeynmt.data - Loading test set...
2022-09-14 22:39:15,053 - INFO - joeynmt.data - Data loaded.
2022-09-14 22:39:15,053 - INFO - joeynmt.helpers - Train dataset: PlaintextDatasetAC(split=train, len=1552563, src_lang=en, trg_lang=hi, has_trg=True, random_subset=-1)
2022-09-14 22:39:15,053 - INFO - joeynmt.helpers - Valid dataset: PlaintextDatasetAC(split=dev, len=40856, src_lang=en, trg_lang=hi, has_trg=True, random_subset=-1)
2022-09-14 22:39:15,053 - INFO - joeynmt.helpers -  Test dataset: PlaintextDatasetAC(split=test, len=40858, src_lang=en, trg_lang=hi, has_trg=True, random_subset=-1)
2022-09-14 22:39:15,054 - INFO - joeynmt.helpers - First training example:
	[SRC] give your application an accessibility work@@ out
	[TRG] अपने अनुप्रयोग को पहुंच@@ नीयता व्यायाम का लाभ दें
2022-09-14 22:39:15,054 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) , (6) . (7) of (8) and (9) to
2022-09-14 22:39:15,054 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) के (5) है (6) । (7) , (8) और (9) में
2022-09-14 22:39:15,054 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 16021
2022-09-14 22:39:15,054 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 16174
2022-09-14 22:39:15,137 - INFO - joeynmt.training - BASELINE MODEL START
2022-09-14 22:39:15,137 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-14 22:39:15,137 - INFO - joeynmt.model - Pytorch version 1.9.0+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-14 22:39:15,461 - INFO - joeynmt.model - Enc-dec model built.
2022-09-14 22:39:18,781 - INFO - joeynmt.model - Total params: 19302144
2022-09-14 22:39:18,782 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm="pre"),
	decoder=TransformerDecoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm="pre"),
	src_embed=Embeddings(embedding_dim=256, vocab_size=16021),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=16174),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.1))
2022-09-14 22:39:20,894 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=[0.9, 0.999])
2022-09-14 22:39:20,894 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=5)
2022-09-14 22:39:20,894 - INFO - joeynmt.training - Loading model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/enhi_transformer_t1_r3/180000.ckpt
2022-09-14 22:39:21,136 - INFO - joeynmt.helpers - Load model from /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/enhi_transformer_t1_r3/180000.ckpt.
2022-09-14 22:39:21,191 - INFO - joeynmt.training - BASELINE MODEL END
2022-09-14 22:39:21,192 - INFO - joeynmt.training - ACTIVE LEARNING MODEL START - RANDOM
2022-09-14 22:39:21,192 - INFO - joeynmt.training - Random Indices picked: [121958 146867 131932 365838 259178 119879 110268 207892  54886 137337] length: 27946
2022-09-14 22:39:21,193 - INFO - joeynmt.training - Executing Random Strategy
2022-09-14 22:39:21,193 - INFO - joeynmt.training - Final Query Indices picked: [121958, 146867, 131932, 365838, 259178, 119879, 110268, 207892, 54886, 137337] length: 10000
2022-09-14 22:39:21,193 - INFO - joeynmt.training - Query the samples, one at a time (interactive/file)
2022-09-14 22:39:22,808 - INFO - joeynmt.training - Remaining Pool Data size: 455884
2022-09-14 22:39:22,809 - INFO - joeynmt.training - Active Learning Data ready to train size: 1096795
2022-09-14 22:39:22,809 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 4096
	effective batch size (w. parallel & accumulation): 4096
2022-09-14 22:39:22,809 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-14 23:01:15,285 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-14 23:01:15,286 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  15.23, loss:   3.05, ppl:  21.15, acc:   0.43, generation: 1305.4354[sec], evaluation: 6.6988[sec]
2022-09-14 23:01:15,290 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-14 23:01:16,730 - INFO - joeynmt.training - Example #0
2022-09-14 23:01:16,742 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-14 23:01:16,743 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-14 23:01:16,743 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष पर , विषय ढकने वाले हैं
2022-09-14 23:01:16,743 - INFO - joeynmt.training - Example #1
2022-09-14 23:01:16,754 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-14 23:01:16,754 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-14 23:01:16,754 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-14 23:01:16,754 - INFO - joeynmt.training - Example #2
2022-09-14 23:01:16,764 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-14 23:01:16,765 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-14 23:01:16,765 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमई
2022-09-14 23:01:16,765 - INFO - joeynmt.training - Example #3
2022-09-14 23:01:16,775 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-14 23:01:16,776 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-14 23:01:16,776 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-14 23:01:16,865 - INFO - joeynmt.training - EPOCH 1
2022-09-14 23:01:40,339 - INFO - joeynmt.training - Epoch   1, Step:   180100, Batch Loss:     3.101286, Batch Acc: 0.004122, Tokens per Sec:     4930, Lr: 0.000035
2022-09-14 23:02:03,603 - INFO - joeynmt.training - Epoch   1, Step:   180200, Batch Loss:     3.026329, Batch Acc: 0.003955, Tokens per Sec:     4988, Lr: 0.000035
2022-09-14 23:02:26,861 - INFO - joeynmt.training - Epoch   1, Step:   180300, Batch Loss:     3.033374, Batch Acc: 0.004267, Tokens per Sec:     4887, Lr: 0.000035
2022-09-14 23:02:49,914 - INFO - joeynmt.training - Epoch   1, Step:   180400, Batch Loss:     3.037927, Batch Acc: 0.003317, Tokens per Sec:     4957, Lr: 0.000035
2022-09-14 23:03:12,910 - INFO - joeynmt.training - Epoch   1, Step:   180500, Batch Loss:     2.987492, Batch Acc: 0.004104, Tokens per Sec:     4927, Lr: 0.000035
2022-09-14 23:03:36,035 - INFO - joeynmt.training - Epoch   1, Step:   180600, Batch Loss:     2.947808, Batch Acc: 0.003779, Tokens per Sec:     4863, Lr: 0.000035
2022-09-14 23:03:59,103 - INFO - joeynmt.training - Epoch   1, Step:   180700, Batch Loss:     2.930070, Batch Acc: 0.003730, Tokens per Sec:     4928, Lr: 0.000035
2022-09-14 23:04:22,173 - INFO - joeynmt.training - Epoch   1, Step:   180800, Batch Loss:     3.117540, Batch Acc: 0.003457, Tokens per Sec:     4928, Lr: 0.000035
2022-09-14 23:04:45,179 - INFO - joeynmt.training - Epoch   1, Step:   180900, Batch Loss:     2.799219, Batch Acc: 0.004219, Tokens per Sec:     4997, Lr: 0.000035
2022-09-14 23:05:08,108 - INFO - joeynmt.training - Epoch   1, Step:   181000, Batch Loss:     2.911176, Batch Acc: 0.003932, Tokens per Sec:     4891, Lr: 0.000035
2022-09-14 23:05:08,108 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-14 23:28:13,204 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-14 23:28:13,206 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  17.06, loss:   2.91, ppl:  18.34, acc:   0.46, generation: 1377.7108[sec], evaluation: 6.8149[sec]
2022-09-14 23:28:13,209 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-14 23:28:15,315 - INFO - joeynmt.training - Example #0
2022-09-14 23:28:15,327 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-14 23:28:15,327 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-14 23:28:15,327 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष पर , विषय ढकने वाले हैं
2022-09-14 23:28:15,327 - INFO - joeynmt.training - Example #1
2022-09-14 23:28:15,338 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-14 23:28:15,338 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-14 23:28:15,338 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-14 23:28:15,338 - INFO - joeynmt.training - Example #2
2022-09-14 23:28:15,349 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-14 23:28:15,349 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-14 23:28:15,349 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमआईएमआईएमई
2022-09-14 23:28:15,349 - INFO - joeynmt.training - Example #3
2022-09-14 23:28:15,360 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-14 23:28:15,360 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-14 23:28:15,360 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-14 23:28:38,428 - INFO - joeynmt.training - Epoch   1, Step:   181100, Batch Loss:     2.736219, Batch Acc: 0.004995, Tokens per Sec:     4565, Lr: 0.000035
2022-09-14 23:29:01,200 - INFO - joeynmt.training - Epoch   1, Step:   181200, Batch Loss:     3.079721, Batch Acc: 0.003563, Tokens per Sec:     4979, Lr: 0.000035
2022-09-14 23:29:24,189 - INFO - joeynmt.training - Epoch   1, Step:   181300, Batch Loss:     2.844570, Batch Acc: 0.004917, Tokens per Sec:     4919, Lr: 0.000035
2022-09-14 23:29:46,904 - INFO - joeynmt.training - Epoch   1, Step:   181400, Batch Loss:     3.181087, Batch Acc: 0.003937, Tokens per Sec:     4965, Lr: 0.000035
2022-09-14 23:30:09,784 - INFO - joeynmt.training - Epoch   1, Step:   181500, Batch Loss:     2.965595, Batch Acc: 0.003719, Tokens per Sec:     4948, Lr: 0.000035
2022-09-14 23:30:32,610 - INFO - joeynmt.training - Epoch   1, Step:   181600, Batch Loss:     2.725780, Batch Acc: 0.004976, Tokens per Sec:     4940, Lr: 0.000035
2022-09-14 23:30:55,529 - INFO - joeynmt.training - Epoch   1, Step:   181700, Batch Loss:     3.094812, Batch Acc: 0.003640, Tokens per Sec:     5119, Lr: 0.000035
2022-09-14 23:31:18,315 - INFO - joeynmt.training - Epoch   1, Step:   181800, Batch Loss:     3.067086, Batch Acc: 0.004151, Tokens per Sec:     4980, Lr: 0.000035
2022-09-14 23:31:41,231 - INFO - joeynmt.training - Epoch   1, Step:   181900, Batch Loss:     3.159156, Batch Acc: 0.003760, Tokens per Sec:     5014, Lr: 0.000035
2022-09-14 23:32:04,218 - INFO - joeynmt.training - Epoch   1, Step:   182000, Batch Loss:     3.016039, Batch Acc: 0.004497, Tokens per Sec:     4963, Lr: 0.000035
2022-09-14 23:32:04,219 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-14 23:54:44,525 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-14 23:54:44,526 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  17.55, loss:   2.87, ppl:  17.71, acc:   0.47, generation: 1353.1838[sec], evaluation: 6.7764[sec]
2022-09-14 23:54:44,530 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-14 23:54:46,068 - INFO - joeynmt.training - Example #0
2022-09-14 23:54:46,081 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-14 23:54:46,081 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-14 23:54:46,081 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष पर , विषय ढोर हैं
2022-09-14 23:54:46,081 - INFO - joeynmt.training - Example #1
2022-09-14 23:54:46,091 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-14 23:54:46,091 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-14 23:54:46,091 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-14 23:54:46,091 - INFO - joeynmt.training - Example #2
2022-09-14 23:54:46,102 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-14 23:54:46,102 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-14 23:54:46,102 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमआईएमई
2022-09-14 23:54:46,102 - INFO - joeynmt.training - Example #3
2022-09-14 23:54:46,113 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-14 23:54:46,113 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-14 23:54:46,113 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-14 23:55:09,232 - INFO - joeynmt.training - Epoch   1, Step:   182100, Batch Loss:     2.929410, Batch Acc: 0.004822, Tokens per Sec:     4609, Lr: 0.000035
2022-09-14 23:55:31,891 - INFO - joeynmt.training - Epoch   1, Step:   182200, Batch Loss:     2.953255, Batch Acc: 0.004409, Tokens per Sec:     4975, Lr: 0.000035
2022-09-14 23:55:54,695 - INFO - joeynmt.training - Epoch   1, Step:   182300, Batch Loss:     2.798948, Batch Acc: 0.005048, Tokens per Sec:     5056, Lr: 0.000035
2022-09-14 23:56:17,566 - INFO - joeynmt.training - Epoch   1, Step:   182400, Batch Loss:     3.115314, Batch Acc: 0.003644, Tokens per Sec:     5099, Lr: 0.000035
2022-09-14 23:56:40,390 - INFO - joeynmt.training - Epoch   1, Step:   182500, Batch Loss:     2.996531, Batch Acc: 0.003694, Tokens per Sec:     5006, Lr: 0.000035
2022-09-14 23:57:03,148 - INFO - joeynmt.training - Epoch   1, Step:   182600, Batch Loss:     2.829016, Batch Acc: 0.004642, Tokens per Sec:     5055, Lr: 0.000035
2022-09-14 23:57:25,985 - INFO - joeynmt.training - Epoch   1, Step:   182700, Batch Loss:     2.791520, Batch Acc: 0.003542, Tokens per Sec:     4982, Lr: 0.000035
2022-09-14 23:57:48,893 - INFO - joeynmt.training - Epoch   1, Step:   182800, Batch Loss:     2.867266, Batch Acc: 0.004855, Tokens per Sec:     5026, Lr: 0.000035
2022-09-14 23:58:11,693 - INFO - joeynmt.training - Epoch   1, Step:   182900, Batch Loss:     2.867259, Batch Acc: 0.004643, Tokens per Sec:     4884, Lr: 0.000035
2022-09-14 23:58:34,480 - INFO - joeynmt.training - Epoch   1, Step:   183000, Batch Loss:     3.072885, Batch Acc: 0.004174, Tokens per Sec:     4973, Lr: 0.000035
2022-09-14 23:58:34,480 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 00:21:03,239 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 00:21:03,241 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  17.68, loss:   2.85, ppl:  17.34, acc:   0.47, generation: 1341.6146[sec], evaluation: 6.5206[sec]
2022-09-15 00:21:03,244 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 00:21:05,713 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/180000.ckpt
2022-09-15 00:21:05,756 - INFO - joeynmt.training - Example #0
2022-09-15 00:21:05,768 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 00:21:05,768 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 00:21:05,768 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 00:21:05,768 - INFO - joeynmt.training - Example #1
2022-09-15 00:21:05,779 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 00:21:05,779 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 00:21:05,779 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 00:21:05,779 - INFO - joeynmt.training - Example #2
2022-09-15 00:21:05,789 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 00:21:05,789 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 00:21:05,789 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमआईएमआईएम
2022-09-15 00:21:05,790 - INFO - joeynmt.training - Example #3
2022-09-15 00:21:05,800 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 00:21:05,800 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 00:21:05,800 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 00:21:28,772 - INFO - joeynmt.training - Epoch   1, Step:   183100, Batch Loss:     2.912664, Batch Acc: 0.004571, Tokens per Sec:     4525, Lr: 0.000035
2022-09-15 00:21:51,548 - INFO - joeynmt.training - Epoch   1, Step:   183200, Batch Loss:     2.996406, Batch Acc: 0.004555, Tokens per Sec:     5003, Lr: 0.000035
2022-09-15 00:22:14,418 - INFO - joeynmt.training - Epoch   1, Step:   183300, Batch Loss:     2.920285, Batch Acc: 0.004033, Tokens per Sec:     5031, Lr: 0.000035
2022-09-15 00:22:37,207 - INFO - joeynmt.training - Epoch   1, Step:   183400, Batch Loss:     2.746175, Batch Acc: 0.004704, Tokens per Sec:     4991, Lr: 0.000035
2022-09-15 00:23:00,297 - INFO - joeynmt.training - Epoch   1, Step:   183500, Batch Loss:     2.921814, Batch Acc: 0.004244, Tokens per Sec:     5020, Lr: 0.000035
2022-09-15 00:23:23,221 - INFO - joeynmt.training - Epoch   1, Step:   183600, Batch Loss:     3.095084, Batch Acc: 0.004062, Tokens per Sec:     5059, Lr: 0.000035
2022-09-15 00:23:46,138 - INFO - joeynmt.training - Epoch   1, Step:   183700, Batch Loss:     2.812469, Batch Acc: 0.003628, Tokens per Sec:     4968, Lr: 0.000035
2022-09-15 00:24:09,352 - INFO - joeynmt.training - Epoch   1, Step:   183800, Batch Loss:     3.006616, Batch Acc: 0.004035, Tokens per Sec:     4985, Lr: 0.000035
2022-09-15 00:24:32,205 - INFO - joeynmt.training - Epoch   1, Step:   183900, Batch Loss:     3.029201, Batch Acc: 0.003539, Tokens per Sec:     4995, Lr: 0.000035
2022-09-15 00:24:55,133 - INFO - joeynmt.training - Epoch   1, Step:   184000, Batch Loss:     2.950976, Batch Acc: 0.003685, Tokens per Sec:     4971, Lr: 0.000035
2022-09-15 00:24:55,133 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 00:46:39,467 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 00:46:39,468 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  17.99, loss:   2.83, ppl:  16.91, acc:   0.47, generation: 1297.0695[sec], evaluation: 6.9185[sec]
2022-09-15 00:46:39,472 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 00:46:41,176 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/181000.ckpt
2022-09-15 00:46:41,218 - INFO - joeynmt.training - Example #0
2022-09-15 00:46:41,230 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 00:46:41,230 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 00:46:41,230 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 00:46:41,230 - INFO - joeynmt.training - Example #1
2022-09-15 00:46:41,241 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 00:46:41,241 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 00:46:41,241 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 00:46:41,241 - INFO - joeynmt.training - Example #2
2022-09-15 00:46:41,252 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 00:46:41,252 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 00:46:41,252 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमआईएमएम
2022-09-15 00:46:41,252 - INFO - joeynmt.training - Example #3
2022-09-15 00:46:41,263 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 00:46:41,263 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 00:46:41,263 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 00:47:04,188 - INFO - joeynmt.training - Epoch   1, Step:   184100, Batch Loss:     2.809845, Batch Acc: 0.004305, Tokens per Sec:     4634, Lr: 0.000035
2022-09-15 00:47:26,891 - INFO - joeynmt.training - Epoch   1, Step:   184200, Batch Loss:     3.092948, Batch Acc: 0.004141, Tokens per Sec:     5021, Lr: 0.000035
2022-09-15 00:47:49,826 - INFO - joeynmt.training - Epoch   1, Step:   184300, Batch Loss:     2.957055, Batch Acc: 0.003766, Tokens per Sec:     5014, Lr: 0.000035
2022-09-15 00:48:12,745 - INFO - joeynmt.training - Epoch   1, Step:   184400, Batch Loss:     2.941078, Batch Acc: 0.004914, Tokens per Sec:     4990, Lr: 0.000035
2022-09-15 00:48:35,631 - INFO - joeynmt.training - Epoch   1, Step:   184500, Batch Loss:     3.088259, Batch Acc: 0.003648, Tokens per Sec:     5055, Lr: 0.000035
2022-09-15 00:48:58,320 - INFO - joeynmt.training - Epoch   1, Step:   184600, Batch Loss:     3.050706, Batch Acc: 0.003686, Tokens per Sec:     5094, Lr: 0.000035
2022-09-15 00:49:21,097 - INFO - joeynmt.training - Epoch   1, Step:   184700, Batch Loss:     2.731165, Batch Acc: 0.004009, Tokens per Sec:     5060, Lr: 0.000035
2022-09-15 00:49:43,963 - INFO - joeynmt.training - Epoch   1, Step:   184800, Batch Loss:     2.734295, Batch Acc: 0.005035, Tokens per Sec:     4985, Lr: 0.000035
2022-09-15 00:50:06,781 - INFO - joeynmt.training - Epoch   1, Step:   184900, Batch Loss:     3.102917, Batch Acc: 0.003904, Tokens per Sec:     5052, Lr: 0.000035
2022-09-15 00:50:29,779 - INFO - joeynmt.training - Epoch   1, Step:   185000, Batch Loss:     2.653249, Batch Acc: 0.004522, Tokens per Sec:     5115, Lr: 0.000035
2022-09-15 00:50:29,780 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 01:11:45,149 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 01:11:45,150 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.11, loss:   2.81, ppl:  16.55, acc:   0.47, generation: 1268.4920[sec], evaluation: 6.5301[sec]
2022-09-15 01:11:45,154 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 01:11:46,597 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/182000.ckpt
2022-09-15 01:11:46,641 - INFO - joeynmt.training - Example #0
2022-09-15 01:11:46,653 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 01:11:46,653 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 01:11:46,653 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय ढकने वाले हैं
2022-09-15 01:11:46,653 - INFO - joeynmt.training - Example #1
2022-09-15 01:11:46,663 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 01:11:46,663 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 01:11:46,663 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 01:11:46,663 - INFO - joeynmt.training - Example #2
2022-09-15 01:11:46,674 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 01:11:46,674 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 01:11:46,674 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमएम
2022-09-15 01:11:46,674 - INFO - joeynmt.training - Example #3
2022-09-15 01:11:46,684 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 01:11:46,685 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 01:11:46,685 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 01:12:09,553 - INFO - joeynmt.training - Epoch   1, Step:   185100, Batch Loss:     2.944378, Batch Acc: 0.004157, Tokens per Sec:     4712, Lr: 0.000035
2022-09-15 01:12:32,475 - INFO - joeynmt.training - Epoch   1, Step:   185200, Batch Loss:     3.070089, Batch Acc: 0.003917, Tokens per Sec:     5035, Lr: 0.000035
2022-09-15 01:12:55,251 - INFO - joeynmt.training - Epoch   1, Step:   185300, Batch Loss:     2.791632, Batch Acc: 0.004757, Tokens per Sec:     4874, Lr: 0.000035
2022-09-15 01:13:18,405 - INFO - joeynmt.training - Epoch   1, Step:   185400, Batch Loss:     2.758584, Batch Acc: 0.005004, Tokens per Sec:     4954, Lr: 0.000035
2022-09-15 01:13:41,281 - INFO - joeynmt.training - Epoch   1, Step:   185500, Batch Loss:     2.954321, Batch Acc: 0.004828, Tokens per Sec:     5061, Lr: 0.000035
2022-09-15 01:14:03,995 - INFO - joeynmt.training - Epoch   1, Step:   185600, Batch Loss:     2.780972, Batch Acc: 0.004182, Tokens per Sec:     4927, Lr: 0.000035
2022-09-15 01:14:26,797 - INFO - joeynmt.training - Epoch   1, Step:   185700, Batch Loss:     2.884415, Batch Acc: 0.004631, Tokens per Sec:     5086, Lr: 0.000035
2022-09-15 01:14:49,643 - INFO - joeynmt.training - Epoch   1, Step:   185800, Batch Loss:     2.848700, Batch Acc: 0.004613, Tokens per Sec:     5057, Lr: 0.000035
2022-09-15 01:15:12,518 - INFO - joeynmt.training - Epoch   1, Step:   185900, Batch Loss:     3.095895, Batch Acc: 0.003231, Tokens per Sec:     5101, Lr: 0.000035
2022-09-15 01:15:35,425 - INFO - joeynmt.training - Epoch   1, Step:   186000, Batch Loss:     2.697147, Batch Acc: 0.004185, Tokens per Sec:     4944, Lr: 0.000035
2022-09-15 01:15:35,425 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 01:38:09,364 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 01:38:09,367 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.35, loss:   2.79, ppl:  16.30, acc:   0.48, generation: 1347.0738[sec], evaluation: 6.5207[sec]
2022-09-15 01:38:09,370 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 01:38:10,906 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/183000.ckpt
2022-09-15 01:38:10,948 - INFO - joeynmt.training - Example #0
2022-09-15 01:38:10,960 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 01:38:10,960 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 01:38:10,960 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय ढकने वाले हैं
2022-09-15 01:38:10,960 - INFO - joeynmt.training - Example #1
2022-09-15 01:38:10,971 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 01:38:10,971 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 01:38:10,971 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 01:38:10,971 - INFO - joeynmt.training - Example #2
2022-09-15 01:38:10,981 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 01:38:10,981 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 01:38:10,981 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और आईएमएम
2022-09-15 01:38:10,981 - INFO - joeynmt.training - Example #3
2022-09-15 01:38:10,992 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 01:38:10,992 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 01:38:10,992 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 01:38:33,833 - INFO - joeynmt.training - Epoch   1, Step:   186100, Batch Loss:     3.335998, Batch Acc: 0.003870, Tokens per Sec:     4679, Lr: 0.000035
2022-09-15 01:38:56,738 - INFO - joeynmt.training - Epoch   1, Step:   186200, Batch Loss:     3.070542, Batch Acc: 0.004149, Tokens per Sec:     5009, Lr: 0.000035
2022-09-15 01:39:19,954 - INFO - joeynmt.training - Epoch   1, Step:   186300, Batch Loss:     2.931484, Batch Acc: 0.004394, Tokens per Sec:     4902, Lr: 0.000035
2022-09-15 01:39:42,646 - INFO - joeynmt.training - Epoch   1, Step:   186400, Batch Loss:     2.849386, Batch Acc: 0.004338, Tokens per Sec:     4957, Lr: 0.000035
2022-09-15 01:40:05,438 - INFO - joeynmt.training - Epoch   1, Step:   186500, Batch Loss:     2.934476, Batch Acc: 0.003807, Tokens per Sec:     5001, Lr: 0.000035
2022-09-15 01:40:28,249 - INFO - joeynmt.training - Epoch   1, Step:   186600, Batch Loss:     2.948687, Batch Acc: 0.004027, Tokens per Sec:     5085, Lr: 0.000035
2022-09-15 01:40:51,045 - INFO - joeynmt.training - Epoch   1, Step:   186700, Batch Loss:     2.962434, Batch Acc: 0.004238, Tokens per Sec:     5000, Lr: 0.000035
2022-09-15 01:41:13,801 - INFO - joeynmt.training - Epoch   1, Step:   186800, Batch Loss:     2.596545, Batch Acc: 0.004743, Tokens per Sec:     5059, Lr: 0.000035
2022-09-15 01:41:36,541 - INFO - joeynmt.training - Epoch   1, Step:   186900, Batch Loss:     3.133003, Batch Acc: 0.004329, Tokens per Sec:     4967, Lr: 0.000035
2022-09-15 01:41:59,369 - INFO - joeynmt.training - Epoch   1, Step:   187000, Batch Loss:     2.914850, Batch Acc: 0.004186, Tokens per Sec:     4971, Lr: 0.000035
2022-09-15 01:41:59,369 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 02:02:54,307 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 02:02:54,308 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.48, loss:   2.78, ppl:  16.09, acc:   0.48, generation: 1248.0585[sec], evaluation: 6.5340[sec]
2022-09-15 02:02:54,312 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 02:02:56,072 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/184000.ckpt
2022-09-15 02:02:56,116 - INFO - joeynmt.training - Example #0
2022-09-15 02:02:56,128 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 02:02:56,128 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 02:02:56,128 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय कवर किए गए हैं
2022-09-15 02:02:56,128 - INFO - joeynmt.training - Example #1
2022-09-15 02:02:56,138 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 02:02:56,138 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 02:02:56,138 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 02:02:56,138 - INFO - joeynmt.training - Example #2
2022-09-15 02:02:56,149 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 02:02:56,149 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 02:02:56,149 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमएम
2022-09-15 02:02:56,149 - INFO - joeynmt.training - Example #3
2022-09-15 02:02:56,159 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 02:02:56,160 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 02:02:56,160 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 02:03:18,999 - INFO - joeynmt.training - Epoch   1, Step:   187100, Batch Loss:     2.819584, Batch Acc: 0.003859, Tokens per Sec:     4608, Lr: 0.000035
2022-09-15 02:03:41,720 - INFO - joeynmt.training - Epoch   1, Step:   187200, Batch Loss:     2.743662, Batch Acc: 0.004611, Tokens per Sec:     4954, Lr: 0.000035
2022-09-15 02:04:04,648 - INFO - joeynmt.training - Epoch   1, Step:   187300, Batch Loss:     3.098873, Batch Acc: 0.004226, Tokens per Sec:     5016, Lr: 0.000035
2022-09-15 02:04:27,462 - INFO - joeynmt.training - Epoch   1, Step:   187400, Batch Loss:     2.782271, Batch Acc: 0.005139, Tokens per Sec:     5032, Lr: 0.000035
2022-09-15 02:04:50,270 - INFO - joeynmt.training - Epoch   1, Step:   187500, Batch Loss:     2.931974, Batch Acc: 0.004430, Tokens per Sec:     5058, Lr: 0.000035
2022-09-15 02:05:13,375 - INFO - joeynmt.training - Epoch   1, Step:   187600, Batch Loss:     2.912590, Batch Acc: 0.004051, Tokens per Sec:     4936, Lr: 0.000035
2022-09-15 02:05:36,206 - INFO - joeynmt.training - Epoch   1, Step:   187700, Batch Loss:     2.951637, Batch Acc: 0.003977, Tokens per Sec:     5121, Lr: 0.000035
2022-09-15 02:05:59,150 - INFO - joeynmt.training - Epoch   1, Step:   187800, Batch Loss:     2.806710, Batch Acc: 0.004317, Tokens per Sec:     5018, Lr: 0.000035
2022-09-15 02:06:21,915 - INFO - joeynmt.training - Epoch   1, Step:   187900, Batch Loss:     2.967054, Batch Acc: 0.003784, Tokens per Sec:     4980, Lr: 0.000035
2022-09-15 02:06:44,804 - INFO - joeynmt.training - Epoch   1, Step:   188000, Batch Loss:     2.945558, Batch Acc: 0.004530, Tokens per Sec:     5054, Lr: 0.000035
2022-09-15 02:06:44,804 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 02:27:32,324 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 02:27:32,325 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.64, loss:   2.76, ppl:  15.82, acc:   0.48, generation: 1240.5402[sec], evaluation: 6.6313[sec]
2022-09-15 02:27:32,329 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 02:27:33,945 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/185000.ckpt
2022-09-15 02:27:33,987 - INFO - joeynmt.training - Example #0
2022-09-15 02:27:34,000 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 02:27:34,000 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 02:27:34,000 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 02:27:34,000 - INFO - joeynmt.training - Example #1
2022-09-15 02:27:34,010 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 02:27:34,010 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 02:27:34,010 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 02:27:34,010 - INFO - joeynmt.training - Example #2
2022-09-15 02:27:34,021 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 02:27:34,021 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 02:27:34,021 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमएम
2022-09-15 02:27:34,021 - INFO - joeynmt.training - Example #3
2022-09-15 02:27:34,031 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 02:27:34,031 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 02:27:34,031 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 02:27:56,961 - INFO - joeynmt.training - Epoch   1, Step:   188100, Batch Loss:     2.990107, Batch Acc: 0.003672, Tokens per Sec:     4622, Lr: 0.000035
2022-09-15 02:28:19,825 - INFO - joeynmt.training - Epoch   1, Step:   188200, Batch Loss:     2.774274, Batch Acc: 0.004252, Tokens per Sec:     4978, Lr: 0.000035
2022-09-15 02:28:42,688 - INFO - joeynmt.training - Epoch   1, Step:   188300, Batch Loss:     2.872617, Batch Acc: 0.004374, Tokens per Sec:     4960, Lr: 0.000035
2022-09-15 02:29:05,483 - INFO - joeynmt.training - Epoch   1, Step:   188400, Batch Loss:     3.202240, Batch Acc: 0.003764, Tokens per Sec:     4976, Lr: 0.000035
2022-09-15 02:29:28,246 - INFO - joeynmt.training - Epoch   1, Step:   188500, Batch Loss:     2.888229, Batch Acc: 0.004033, Tokens per Sec:     5000, Lr: 0.000035
2022-09-15 02:29:50,963 - INFO - joeynmt.training - Epoch   1, Step:   188600, Batch Loss:     2.880863, Batch Acc: 0.004467, Tokens per Sec:     4977, Lr: 0.000035
2022-09-15 02:30:13,764 - INFO - joeynmt.training - Epoch   1, Step:   188700, Batch Loss:     2.878229, Batch Acc: 0.004008, Tokens per Sec:     5033, Lr: 0.000035
2022-09-15 02:30:36,586 - INFO - joeynmt.training - Epoch   1, Step:   188800, Batch Loss:     2.833012, Batch Acc: 0.004273, Tokens per Sec:     5066, Lr: 0.000035
2022-09-15 02:30:59,421 - INFO - joeynmt.training - Epoch   1, Step:   188900, Batch Loss:     2.760223, Batch Acc: 0.004410, Tokens per Sec:     4906, Lr: 0.000035
2022-09-15 02:31:22,324 - INFO - joeynmt.training - Epoch   1, Step:   189000, Batch Loss:     3.184700, Batch Acc: 0.003951, Tokens per Sec:     5139, Lr: 0.000035
2022-09-15 02:31:22,325 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 02:52:14,460 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 02:52:14,463 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  19.08, loss:   2.75, ppl:  15.69, acc:   0.48, generation: 1244.7619[sec], evaluation: 6.6595[sec]
2022-09-15 02:52:14,466 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 02:52:15,924 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/186000.ckpt
2022-09-15 02:52:15,966 - INFO - joeynmt.training - Example #0
2022-09-15 02:52:15,978 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 02:52:15,978 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 02:52:15,978 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय कवर किए गए हैं
2022-09-15 02:52:15,978 - INFO - joeynmt.training - Example #1
2022-09-15 02:52:15,989 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 02:52:15,989 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 02:52:15,989 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 02:52:15,989 - INFO - joeynmt.training - Example #2
2022-09-15 02:52:16,000 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 02:52:16,000 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 02:52:16,000 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमएम
2022-09-15 02:52:16,000 - INFO - joeynmt.training - Example #3
2022-09-15 02:52:16,011 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 02:52:16,011 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 02:52:16,011 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 02:52:38,726 - INFO - joeynmt.training - Epoch   1, Step:   189100, Batch Loss:     3.011812, Batch Acc: 0.004000, Tokens per Sec:     4627, Lr: 0.000035
2022-09-15 02:53:01,404 - INFO - joeynmt.training - Epoch   1, Step:   189200, Batch Loss:     2.912446, Batch Acc: 0.004009, Tokens per Sec:     5016, Lr: 0.000035
2022-09-15 02:53:24,129 - INFO - joeynmt.training - Epoch   1, Step:   189300, Batch Loss:     2.871601, Batch Acc: 0.004203, Tokens per Sec:     5067, Lr: 0.000035
2022-09-15 02:53:47,084 - INFO - joeynmt.training - Epoch   1, Step:   189400, Batch Loss:     2.787082, Batch Acc: 0.004229, Tokens per Sec:     5088, Lr: 0.000035
2022-09-15 02:54:09,851 - INFO - joeynmt.training - Epoch   1, Step:   189500, Batch Loss:     2.887023, Batch Acc: 0.003980, Tokens per Sec:     4988, Lr: 0.000035
2022-09-15 02:54:32,505 - INFO - joeynmt.training - Epoch   1, Step:   189600, Batch Loss:     3.077711, Batch Acc: 0.003749, Tokens per Sec:     5004, Lr: 0.000035
2022-09-15 02:54:55,287 - INFO - joeynmt.training - Epoch   1, Step:   189700, Batch Loss:     2.934809, Batch Acc: 0.003902, Tokens per Sec:     5073, Lr: 0.000035
2022-09-15 02:55:18,139 - INFO - joeynmt.training - Epoch   1, Step:   189800, Batch Loss:     2.837546, Batch Acc: 0.004913, Tokens per Sec:     5184, Lr: 0.000035
2022-09-15 02:55:40,831 - INFO - joeynmt.training - Epoch   1, Step:   189900, Batch Loss:     2.575200, Batch Acc: 0.005420, Tokens per Sec:     5066, Lr: 0.000035
2022-09-15 02:56:03,615 - INFO - joeynmt.training - Epoch   1, Step:   190000, Batch Loss:     2.822368, Batch Acc: 0.003812, Tokens per Sec:     5043, Lr: 0.000035
2022-09-15 02:56:03,615 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 03:16:19,922 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 03:16:19,925 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  18.99, loss:   2.74, ppl:  15.49, acc:   0.48, generation: 1208.9084[sec], evaluation: 7.0550[sec]
2022-09-15 03:16:19,929 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 03:16:21,520 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/187000.ckpt
2022-09-15 03:16:21,562 - INFO - joeynmt.training - Example #0
2022-09-15 03:16:21,574 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 03:16:21,574 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 03:16:21,574 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 03:16:21,574 - INFO - joeynmt.training - Example #1
2022-09-15 03:16:21,585 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 03:16:21,585 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 03:16:21,585 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 03:16:21,585 - INFO - joeynmt.training - Example #2
2022-09-15 03:16:21,595 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 03:16:21,595 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 03:16:21,595 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमएम
2022-09-15 03:16:21,595 - INFO - joeynmt.training - Example #3
2022-09-15 03:16:21,606 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 03:16:21,606 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 03:16:21,606 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 03:16:44,696 - INFO - joeynmt.training - Epoch   1, Step:   190100, Batch Loss:     2.983704, Batch Acc: 0.004698, Tokens per Sec:     4641, Lr: 0.000035
2022-09-15 03:17:07,501 - INFO - joeynmt.training - Epoch   1, Step:   190200, Batch Loss:     2.848974, Batch Acc: 0.004145, Tokens per Sec:     4993, Lr: 0.000035
2022-09-15 03:17:30,179 - INFO - joeynmt.training - Epoch   1, Step:   190300, Batch Loss:     2.656975, Batch Acc: 0.003939, Tokens per Sec:     5071, Lr: 0.000035
2022-09-15 03:17:52,954 - INFO - joeynmt.training - Epoch   1, Step:   190400, Batch Loss:     2.990782, Batch Acc: 0.004439, Tokens per Sec:     5173, Lr: 0.000035
2022-09-15 03:18:15,760 - INFO - joeynmt.training - Epoch   1, Step:   190500, Batch Loss:     2.929703, Batch Acc: 0.004544, Tokens per Sec:     5153, Lr: 0.000035
2022-09-15 03:18:38,544 - INFO - joeynmt.training - Epoch   1, Step:   190600, Batch Loss:     2.891140, Batch Acc: 0.004358, Tokens per Sec:     5076, Lr: 0.000035
2022-09-15 03:19:01,305 - INFO - joeynmt.training - Epoch   1, Step:   190700, Batch Loss:     2.950466, Batch Acc: 0.004171, Tokens per Sec:     5141, Lr: 0.000035
2022-09-15 03:19:24,213 - INFO - joeynmt.training - Epoch   1, Step:   190800, Batch Loss:     2.710520, Batch Acc: 0.004186, Tokens per Sec:     5047, Lr: 0.000035
2022-09-15 03:19:47,121 - INFO - joeynmt.training - Epoch   1, Step:   190900, Batch Loss:     2.860869, Batch Acc: 0.004619, Tokens per Sec:     5047, Lr: 0.000035
2022-09-15 03:20:10,035 - INFO - joeynmt.training - Epoch   1, Step:   191000, Batch Loss:     2.756564, Batch Acc: 0.004088, Tokens per Sec:     5135, Lr: 0.000035
2022-09-15 03:20:10,036 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 03:40:20,220 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 03:40:20,222 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  19.27, loss:   2.73, ppl:  15.28, acc:   0.49, generation: 1203.1473[sec], evaluation: 6.6898[sec]
2022-09-15 03:40:20,226 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 03:40:21,691 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/188000.ckpt
2022-09-15 03:40:21,733 - INFO - joeynmt.training - Example #0
2022-09-15 03:40:21,746 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 03:40:21,746 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 03:40:21,746 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 03:40:21,746 - INFO - joeynmt.training - Example #1
2022-09-15 03:40:21,756 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 03:40:21,756 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 03:40:21,756 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 03:40:21,756 - INFO - joeynmt.training - Example #2
2022-09-15 03:40:21,767 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 03:40:21,767 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 03:40:21,767 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और आईएमएम
2022-09-15 03:40:21,767 - INFO - joeynmt.training - Example #3
2022-09-15 03:40:21,777 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 03:40:21,777 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 03:40:21,777 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 03:40:44,755 - INFO - joeynmt.training - Epoch   1, Step:   191100, Batch Loss:     2.908355, Batch Acc: 0.004273, Tokens per Sec:     4742, Lr: 0.000035
2022-09-15 03:41:07,636 - INFO - joeynmt.training - Epoch   1, Step:   191200, Batch Loss:     3.138769, Batch Acc: 0.003533, Tokens per Sec:     5010, Lr: 0.000035
2022-09-15 03:41:30,361 - INFO - joeynmt.training - Epoch   1, Step:   191300, Batch Loss:     2.865178, Batch Acc: 0.003718, Tokens per Sec:     5030, Lr: 0.000035
2022-09-15 03:41:53,182 - INFO - joeynmt.training - Epoch   1, Step:   191400, Batch Loss:     3.122874, Batch Acc: 0.003651, Tokens per Sec:     5016, Lr: 0.000035
2022-09-15 03:42:15,992 - INFO - joeynmt.training - Epoch   1, Step:   191500, Batch Loss:     3.038470, Batch Acc: 0.003760, Tokens per Sec:     5002, Lr: 0.000035
2022-09-15 03:42:38,868 - INFO - joeynmt.training - Epoch   1, Step:   191600, Batch Loss:     2.847741, Batch Acc: 0.003948, Tokens per Sec:     5093, Lr: 0.000035
2022-09-15 03:43:01,635 - INFO - joeynmt.training - Epoch   1, Step:   191700, Batch Loss:     2.909057, Batch Acc: 0.003913, Tokens per Sec:     4996, Lr: 0.000035
2022-09-15 03:43:24,464 - INFO - joeynmt.training - Epoch   1, Step:   191800, Batch Loss:     2.510681, Batch Acc: 0.004138, Tokens per Sec:     4997, Lr: 0.000035
2022-09-15 03:43:47,191 - INFO - joeynmt.training - Epoch   1, Step:   191900, Batch Loss:     2.747587, Batch Acc: 0.003338, Tokens per Sec:     5114, Lr: 0.000035
2022-09-15 03:44:10,004 - INFO - joeynmt.training - Epoch   1, Step:   192000, Batch Loss:     2.992269, Batch Acc: 0.004208, Tokens per Sec:     5010, Lr: 0.000035
2022-09-15 03:44:10,004 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 04:04:17,630 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 04:04:17,632 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  19.54, loss:   2.72, ppl:  15.18, acc:   0.49, generation: 1200.1312[sec], evaluation: 7.1482[sec]
2022-09-15 04:04:17,636 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 04:04:19,182 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/189000.ckpt
2022-09-15 04:04:19,225 - INFO - joeynmt.training - Example #0
2022-09-15 04:04:19,237 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 04:04:19,237 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 04:04:19,237 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों पर शामिल हैं
2022-09-15 04:04:19,237 - INFO - joeynmt.training - Example #1
2022-09-15 04:04:19,248 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 04:04:19,248 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 04:04:19,248 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 04:04:19,248 - INFO - joeynmt.training - Example #2
2022-09-15 04:04:19,258 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 04:04:19,258 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 04:04:19,258 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और इम
2022-09-15 04:04:19,258 - INFO - joeynmt.training - Example #3
2022-09-15 04:04:19,269 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 04:04:19,269 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 04:04:19,269 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 04:04:42,150 - INFO - joeynmt.training - Epoch   1, Step:   192100, Batch Loss:     2.942381, Batch Acc: 0.003945, Tokens per Sec:     4622, Lr: 0.000035
2022-09-15 04:05:04,895 - INFO - joeynmt.training - Epoch   1, Step:   192200, Batch Loss:     2.852902, Batch Acc: 0.004179, Tokens per Sec:     4966, Lr: 0.000035
2022-09-15 04:05:27,739 - INFO - joeynmt.training - Epoch   1, Step:   192300, Batch Loss:     2.894291, Batch Acc: 0.003976, Tokens per Sec:     4976, Lr: 0.000035
2022-09-15 04:05:50,457 - INFO - joeynmt.training - Epoch   1, Step:   192400, Batch Loss:     2.772505, Batch Acc: 0.004854, Tokens per Sec:     5088, Lr: 0.000035
2022-09-15 04:06:13,190 - INFO - joeynmt.training - Epoch   1, Step:   192500, Batch Loss:     3.046392, Batch Acc: 0.003758, Tokens per Sec:     5022, Lr: 0.000035
2022-09-15 04:06:36,031 - INFO - joeynmt.training - Epoch   1, Step:   192600, Batch Loss:     2.741679, Batch Acc: 0.005009, Tokens per Sec:     5026, Lr: 0.000035
2022-09-15 04:06:58,895 - INFO - joeynmt.training - Epoch   1, Step:   192700, Batch Loss:     2.944114, Batch Acc: 0.004112, Tokens per Sec:     5095, Lr: 0.000035
2022-09-15 04:07:21,730 - INFO - joeynmt.training - Epoch   1, Step:   192800, Batch Loss:     2.812586, Batch Acc: 0.004050, Tokens per Sec:     5061, Lr: 0.000035
2022-09-15 04:07:44,499 - INFO - joeynmt.training - Epoch   1, Step:   192900, Batch Loss:     2.947968, Batch Acc: 0.003584, Tokens per Sec:     4963, Lr: 0.000035
2022-09-15 04:08:07,383 - INFO - joeynmt.training - Epoch   1, Step:   193000, Batch Loss:     2.748670, Batch Acc: 0.005020, Tokens per Sec:     4979, Lr: 0.000035
2022-09-15 04:08:07,384 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 04:29:40,783 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 04:29:40,784 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  19.62, loss:   2.70, ppl:  14.95, acc:   0.49, generation: 1286.3178[sec], evaluation: 6.7320[sec]
2022-09-15 04:29:40,788 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 04:29:42,278 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/190000.ckpt
2022-09-15 04:29:42,320 - INFO - joeynmt.training - Example #0
2022-09-15 04:29:42,332 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 04:29:42,332 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 04:29:42,332 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 04:29:42,332 - INFO - joeynmt.training - Example #1
2022-09-15 04:29:42,343 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 04:29:42,343 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 04:29:42,343 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 04:29:42,343 - INFO - joeynmt.training - Example #2
2022-09-15 04:29:42,353 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 04:29:42,353 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 04:29:42,353 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और इम
2022-09-15 04:29:42,353 - INFO - joeynmt.training - Example #3
2022-09-15 04:29:42,364 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 04:29:42,364 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 04:29:42,364 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 04:30:05,215 - INFO - joeynmt.training - Epoch   1, Step:   193100, Batch Loss:     2.787038, Batch Acc: 0.004373, Tokens per Sec:     4728, Lr: 0.000035
2022-09-15 04:30:27,909 - INFO - joeynmt.training - Epoch   1, Step:   193200, Batch Loss:     2.846979, Batch Acc: 0.004156, Tokens per Sec:     5036, Lr: 0.000035
2022-09-15 04:30:50,687 - INFO - joeynmt.training - Epoch   1, Step:   193300, Batch Loss:     2.593731, Batch Acc: 0.004684, Tokens per Sec:     5071, Lr: 0.000035
2022-09-15 04:31:13,439 - INFO - joeynmt.training - Epoch   1, Step:   193400, Batch Loss:     2.380951, Batch Acc: 0.005025, Tokens per Sec:     5021, Lr: 0.000035
2022-09-15 04:31:36,223 - INFO - joeynmt.training - Epoch   1, Step:   193500, Batch Loss:     2.735201, Batch Acc: 0.004989, Tokens per Sec:     5112, Lr: 0.000035
2022-09-15 04:31:58,998 - INFO - joeynmt.training - Epoch   1, Step:   193600, Batch Loss:     2.807085, Batch Acc: 0.004389, Tokens per Sec:     5002, Lr: 0.000035
2022-09-15 04:32:21,979 - INFO - joeynmt.training - Epoch   1, Step:   193700, Batch Loss:     2.864379, Batch Acc: 0.004132, Tokens per Sec:     5066, Lr: 0.000035
2022-09-15 04:32:44,798 - INFO - joeynmt.training - Epoch   1, Step:   193800, Batch Loss:     2.568259, Batch Acc: 0.005812, Tokens per Sec:     5059, Lr: 0.000035
2022-09-15 04:33:07,588 - INFO - joeynmt.training - Epoch   1, Step:   193900, Batch Loss:     2.625645, Batch Acc: 0.004906, Tokens per Sec:     5133, Lr: 0.000035
2022-09-15 04:33:30,629 - INFO - joeynmt.training - Epoch   1, Step:   194000, Batch Loss:     2.876255, Batch Acc: 0.003638, Tokens per Sec:     5070, Lr: 0.000035
2022-09-15 04:33:30,630 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 04:52:55,445 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 04:52:55,446 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  19.67, loss:   2.70, ppl:  14.86, acc:   0.49, generation: 1157.7799[sec], evaluation: 6.6882[sec]
2022-09-15 04:52:55,450 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 04:52:56,942 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/191000.ckpt
2022-09-15 04:52:56,984 - INFO - joeynmt.training - Example #0
2022-09-15 04:52:56,996 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 04:52:56,996 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 04:52:56,996 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 04:52:56,996 - INFO - joeynmt.training - Example #1
2022-09-15 04:52:57,007 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 04:52:57,007 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 04:52:57,007 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 04:52:57,007 - INFO - joeynmt.training - Example #2
2022-09-15 04:52:57,018 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 04:52:57,018 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 04:52:57,018 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और इम
2022-09-15 04:52:57,018 - INFO - joeynmt.training - Example #3
2022-09-15 04:52:57,028 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 04:52:57,028 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 04:52:57,028 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 04:53:20,032 - INFO - joeynmt.training - Epoch   1, Step:   194100, Batch Loss:     2.811909, Batch Acc: 0.004022, Tokens per Sec:     4632, Lr: 0.000035
2022-09-15 04:53:42,875 - INFO - joeynmt.training - Epoch   1, Step:   194200, Batch Loss:     2.672616, Batch Acc: 0.003865, Tokens per Sec:     5109, Lr: 0.000035
2022-09-15 04:54:05,612 - INFO - joeynmt.training - Epoch   1, Step:   194300, Batch Loss:     2.857432, Batch Acc: 0.004289, Tokens per Sec:     4933, Lr: 0.000035
2022-09-15 04:54:11,583 - INFO - joeynmt.training - Epoch   1: total training loss 41565.03
2022-09-15 04:54:11,583 - INFO - joeynmt.training - EPOCH 2
2022-09-15 04:54:28,160 - INFO - joeynmt.training - Epoch   2, Step:   194400, Batch Loss:     2.943583, Batch Acc: 0.005167, Tokens per Sec:     5091, Lr: 0.000035
2022-09-15 04:54:50,931 - INFO - joeynmt.training - Epoch   2, Step:   194500, Batch Loss:     2.997986, Batch Acc: 0.004261, Tokens per Sec:     5040, Lr: 0.000035
2022-09-15 04:55:13,623 - INFO - joeynmt.training - Epoch   2, Step:   194600, Batch Loss:     2.727179, Batch Acc: 0.004142, Tokens per Sec:     4969, Lr: 0.000035
2022-09-15 04:55:36,341 - INFO - joeynmt.training - Epoch   2, Step:   194700, Batch Loss:     2.883156, Batch Acc: 0.003965, Tokens per Sec:     5029, Lr: 0.000035
2022-09-15 04:55:59,014 - INFO - joeynmt.training - Epoch   2, Step:   194800, Batch Loss:     2.744349, Batch Acc: 0.005357, Tokens per Sec:     5047, Lr: 0.000035
2022-09-15 04:56:21,874 - INFO - joeynmt.training - Epoch   2, Step:   194900, Batch Loss:     2.900767, Batch Acc: 0.004475, Tokens per Sec:     5073, Lr: 0.000035
2022-09-15 04:56:44,646 - INFO - joeynmt.training - Epoch   2, Step:   195000, Batch Loss:     2.958248, Batch Acc: 0.004715, Tokens per Sec:     5058, Lr: 0.000035
2022-09-15 04:56:44,646 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 05:15:40,570 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 05:15:40,572 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  19.82, loss:   2.69, ppl:  14.76, acc:   0.49, generation: 1128.4517[sec], evaluation: 6.6912[sec]
2022-09-15 05:15:40,576 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 05:15:42,129 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/192000.ckpt
2022-09-15 05:15:42,172 - INFO - joeynmt.training - Example #0
2022-09-15 05:15:42,184 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 05:15:42,184 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 05:15:42,184 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 05:15:42,184 - INFO - joeynmt.training - Example #1
2022-09-15 05:15:42,195 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 05:15:42,195 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 05:15:42,195 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 05:15:42,195 - INFO - joeynmt.training - Example #2
2022-09-15 05:15:42,206 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 05:15:42,206 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 05:15:42,206 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और इम
2022-09-15 05:15:42,206 - INFO - joeynmt.training - Example #3
2022-09-15 05:15:42,217 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 05:15:42,217 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 05:15:42,217 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 05:16:04,997 - INFO - joeynmt.training - Epoch   2, Step:   195100, Batch Loss:     3.004717, Batch Acc: 0.003922, Tokens per Sec:     4667, Lr: 0.000035
2022-09-15 05:16:27,743 - INFO - joeynmt.training - Epoch   2, Step:   195200, Batch Loss:     3.027936, Batch Acc: 0.004002, Tokens per Sec:     5075, Lr: 0.000035
2022-09-15 05:16:50,547 - INFO - joeynmt.training - Epoch   2, Step:   195300, Batch Loss:     3.009899, Batch Acc: 0.003794, Tokens per Sec:     5097, Lr: 0.000035
2022-09-15 05:17:13,300 - INFO - joeynmt.training - Epoch   2, Step:   195400, Batch Loss:     2.545414, Batch Acc: 0.004241, Tokens per Sec:     5099, Lr: 0.000035
2022-09-15 05:17:36,067 - INFO - joeynmt.training - Epoch   2, Step:   195500, Batch Loss:     2.973841, Batch Acc: 0.003978, Tokens per Sec:     5068, Lr: 0.000035
2022-09-15 05:17:58,764 - INFO - joeynmt.training - Epoch   2, Step:   195600, Batch Loss:     2.605567, Batch Acc: 0.004624, Tokens per Sec:     5021, Lr: 0.000035
2022-09-15 05:18:21,495 - INFO - joeynmt.training - Epoch   2, Step:   195700, Batch Loss:     2.730541, Batch Acc: 0.003913, Tokens per Sec:     5037, Lr: 0.000035
2022-09-15 05:18:44,215 - INFO - joeynmt.training - Epoch   2, Step:   195800, Batch Loss:     2.725955, Batch Acc: 0.005385, Tokens per Sec:     5117, Lr: 0.000035
2022-09-15 05:19:06,799 - INFO - joeynmt.training - Epoch   2, Step:   195900, Batch Loss:     2.685081, Batch Acc: 0.004647, Tokens per Sec:     5060, Lr: 0.000035
2022-09-15 05:19:29,378 - INFO - joeynmt.training - Epoch   2, Step:   196000, Batch Loss:     2.750356, Batch Acc: 0.004981, Tokens per Sec:     5051, Lr: 0.000035
2022-09-15 05:19:29,378 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 05:38:53,736 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 05:38:53,737 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.19, loss:   2.68, ppl:  14.60, acc:   0.49, generation: 1157.2616[sec], evaluation: 6.7489[sec]
2022-09-15 05:38:53,741 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 05:38:55,200 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/193000.ckpt
2022-09-15 05:38:55,243 - INFO - joeynmt.training - Example #0
2022-09-15 05:38:55,256 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 05:38:55,256 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 05:38:55,256 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 05:38:55,256 - INFO - joeynmt.training - Example #1
2022-09-15 05:38:55,266 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 05:38:55,266 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 05:38:55,266 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 05:38:55,266 - INFO - joeynmt.training - Example #2
2022-09-15 05:38:55,277 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 05:38:55,277 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 05:38:55,277 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 05:38:55,277 - INFO - joeynmt.training - Example #3
2022-09-15 05:38:55,288 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 05:38:55,288 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 05:38:55,288 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 05:39:18,025 - INFO - joeynmt.training - Epoch   2, Step:   196100, Batch Loss:     2.799880, Batch Acc: 0.004346, Tokens per Sec:     4662, Lr: 0.000035
2022-09-15 05:39:40,797 - INFO - joeynmt.training - Epoch   2, Step:   196200, Batch Loss:     2.699608, Batch Acc: 0.004352, Tokens per Sec:     5076, Lr: 0.000035
2022-09-15 05:40:03,569 - INFO - joeynmt.training - Epoch   2, Step:   196300, Batch Loss:     2.813494, Batch Acc: 0.004126, Tokens per Sec:     5088, Lr: 0.000035
2022-09-15 05:40:26,336 - INFO - joeynmt.training - Epoch   2, Step:   196400, Batch Loss:     2.900611, Batch Acc: 0.004651, Tokens per Sec:     4977, Lr: 0.000035
2022-09-15 05:40:48,994 - INFO - joeynmt.training - Epoch   2, Step:   196500, Batch Loss:     2.752345, Batch Acc: 0.004278, Tokens per Sec:     5024, Lr: 0.000035
2022-09-15 05:41:11,702 - INFO - joeynmt.training - Epoch   2, Step:   196600, Batch Loss:     2.902818, Batch Acc: 0.003572, Tokens per Sec:     4968, Lr: 0.000035
2022-09-15 05:41:34,423 - INFO - joeynmt.training - Epoch   2, Step:   196700, Batch Loss:     3.001684, Batch Acc: 0.004599, Tokens per Sec:     5072, Lr: 0.000035
2022-09-15 05:41:57,230 - INFO - joeynmt.training - Epoch   2, Step:   196800, Batch Loss:     2.674809, Batch Acc: 0.005005, Tokens per Sec:     4994, Lr: 0.000035
2022-09-15 05:42:20,101 - INFO - joeynmt.training - Epoch   2, Step:   196900, Batch Loss:     3.016856, Batch Acc: 0.003498, Tokens per Sec:     5038, Lr: 0.000035
2022-09-15 05:42:42,911 - INFO - joeynmt.training - Epoch   2, Step:   197000, Batch Loss:     2.722703, Batch Acc: 0.005784, Tokens per Sec:     4995, Lr: 0.000035
2022-09-15 05:42:42,911 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 06:02:04,289 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 06:02:04,290 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.29, loss:   2.68, ppl:  14.54, acc:   0.49, generation: 1154.3959[sec], evaluation: 6.6325[sec]
2022-09-15 06:02:04,294 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 06:02:06,479 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/194000.ckpt
2022-09-15 06:02:06,521 - INFO - joeynmt.training - Example #0
2022-09-15 06:02:06,533 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 06:02:06,533 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 06:02:06,533 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 06:02:06,533 - INFO - joeynmt.training - Example #1
2022-09-15 06:02:06,543 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 06:02:06,543 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 06:02:06,543 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 06:02:06,543 - INFO - joeynmt.training - Example #2
2022-09-15 06:02:06,553 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 06:02:06,553 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 06:02:06,553 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और इम
2022-09-15 06:02:06,553 - INFO - joeynmt.training - Example #3
2022-09-15 06:02:06,564 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 06:02:06,564 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 06:02:06,564 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 06:02:29,364 - INFO - joeynmt.training - Epoch   2, Step:   197100, Batch Loss:     2.908132, Batch Acc: 0.004173, Tokens per Sec:     4541, Lr: 0.000035
2022-09-15 06:02:52,128 - INFO - joeynmt.training - Epoch   2, Step:   197200, Batch Loss:     2.779969, Batch Acc: 0.004708, Tokens per Sec:     5076, Lr: 0.000035
2022-09-15 06:03:14,924 - INFO - joeynmt.training - Epoch   2, Step:   197300, Batch Loss:     2.922796, Batch Acc: 0.004285, Tokens per Sec:     4986, Lr: 0.000035
2022-09-15 06:03:37,600 - INFO - joeynmt.training - Epoch   2, Step:   197400, Batch Loss:     2.404596, Batch Acc: 0.004658, Tokens per Sec:     4999, Lr: 0.000035
2022-09-15 06:04:00,378 - INFO - joeynmt.training - Epoch   2, Step:   197500, Batch Loss:     3.018724, Batch Acc: 0.004191, Tokens per Sec:     5007, Lr: 0.000035
2022-09-15 06:04:23,197 - INFO - joeynmt.training - Epoch   2, Step:   197600, Batch Loss:     2.756185, Batch Acc: 0.005258, Tokens per Sec:     5009, Lr: 0.000035
2022-09-15 06:04:45,918 - INFO - joeynmt.training - Epoch   2, Step:   197700, Batch Loss:     2.924367, Batch Acc: 0.003784, Tokens per Sec:     5013, Lr: 0.000035
2022-09-15 06:05:08,729 - INFO - joeynmt.training - Epoch   2, Step:   197800, Batch Loss:     2.699042, Batch Acc: 0.004149, Tokens per Sec:     5029, Lr: 0.000035
2022-09-15 06:05:31,531 - INFO - joeynmt.training - Epoch   2, Step:   197900, Batch Loss:     2.721470, Batch Acc: 0.004641, Tokens per Sec:     4999, Lr: 0.000035
2022-09-15 06:05:54,466 - INFO - joeynmt.training - Epoch   2, Step:   198000, Batch Loss:     2.794541, Batch Acc: 0.004948, Tokens per Sec:     4961, Lr: 0.000035
2022-09-15 06:05:54,466 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 06:24:21,851 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 06:24:21,852 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.35, loss:   2.67, ppl:  14.50, acc:   0.49, generation: 1099.9661[sec], evaluation: 6.6613[sec]
2022-09-15 06:24:21,856 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 06:24:23,333 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/195000.ckpt
2022-09-15 06:24:23,375 - INFO - joeynmt.training - Example #0
2022-09-15 06:24:23,387 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 06:24:23,387 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 06:24:23,387 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 06:24:23,387 - INFO - joeynmt.training - Example #1
2022-09-15 06:24:23,398 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 06:24:23,398 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 06:24:23,398 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 06:24:23,398 - INFO - joeynmt.training - Example #2
2022-09-15 06:24:23,409 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 06:24:23,409 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 06:24:23,409 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और इम
2022-09-15 06:24:23,409 - INFO - joeynmt.training - Example #3
2022-09-15 06:24:23,420 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 06:24:23,420 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 06:24:23,420 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 06:24:46,281 - INFO - joeynmt.training - Epoch   2, Step:   198100, Batch Loss:     2.757334, Batch Acc: 0.004530, Tokens per Sec:     4646, Lr: 0.000035
2022-09-15 06:25:09,045 - INFO - joeynmt.training - Epoch   2, Step:   198200, Batch Loss:     2.966832, Batch Acc: 0.003708, Tokens per Sec:     5023, Lr: 0.000035
2022-09-15 06:25:31,816 - INFO - joeynmt.training - Epoch   2, Step:   198300, Batch Loss:     2.744278, Batch Acc: 0.004175, Tokens per Sec:     4954, Lr: 0.000035
2022-09-15 06:25:54,681 - INFO - joeynmt.training - Epoch   2, Step:   198400, Batch Loss:     2.759696, Batch Acc: 0.004636, Tokens per Sec:     4991, Lr: 0.000035
2022-09-15 06:26:17,608 - INFO - joeynmt.training - Epoch   2, Step:   198500, Batch Loss:     2.800926, Batch Acc: 0.004652, Tokens per Sec:     4969, Lr: 0.000035
2022-09-15 06:26:40,362 - INFO - joeynmt.training - Epoch   2, Step:   198600, Batch Loss:     3.070003, Batch Acc: 0.004146, Tokens per Sec:     5014, Lr: 0.000035
2022-09-15 06:27:03,077 - INFO - joeynmt.training - Epoch   2, Step:   198700, Batch Loss:     2.830786, Batch Acc: 0.004305, Tokens per Sec:     5082, Lr: 0.000035
2022-09-15 06:27:25,791 - INFO - joeynmt.training - Epoch   2, Step:   198800, Batch Loss:     3.193580, Batch Acc: 0.003625, Tokens per Sec:     5113, Lr: 0.000035
2022-09-15 06:27:48,521 - INFO - joeynmt.training - Epoch   2, Step:   198900, Batch Loss:     2.769883, Batch Acc: 0.004280, Tokens per Sec:     4996, Lr: 0.000035
2022-09-15 06:28:11,239 - INFO - joeynmt.training - Epoch   2, Step:   199000, Batch Loss:     2.731771, Batch Acc: 0.004671, Tokens per Sec:     4929, Lr: 0.000035
2022-09-15 06:28:11,239 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 06:45:51,017 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 06:45:51,019 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.44, loss:   2.67, ppl:  14.45, acc:   0.49, generation: 1052.8004[sec], evaluation: 6.6340[sec]
2022-09-15 06:45:51,023 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 06:45:52,508 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/196000.ckpt
2022-09-15 06:45:52,550 - INFO - joeynmt.training - Example #0
2022-09-15 06:45:52,562 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 06:45:52,562 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 06:45:52,562 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को ढका हुआ है
2022-09-15 06:45:52,562 - INFO - joeynmt.training - Example #1
2022-09-15 06:45:52,573 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 06:45:52,573 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 06:45:52,573 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 06:45:52,573 - INFO - joeynmt.training - Example #2
2022-09-15 06:45:52,583 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 06:45:52,583 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 06:45:52,583 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और इम
2022-09-15 06:45:52,583 - INFO - joeynmt.training - Example #3
2022-09-15 06:45:52,594 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 06:45:52,594 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 06:45:52,594 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 06:46:15,464 - INFO - joeynmt.training - Epoch   2, Step:   199100, Batch Loss:     2.575809, Batch Acc: 0.004068, Tokens per Sec:     4737, Lr: 0.000035
2022-09-15 06:46:38,251 - INFO - joeynmt.training - Epoch   2, Step:   199200, Batch Loss:     2.643629, Batch Acc: 0.005683, Tokens per Sec:     5058, Lr: 0.000035
2022-09-15 06:47:00,965 - INFO - joeynmt.training - Epoch   2, Step:   199300, Batch Loss:     2.892365, Batch Acc: 0.003791, Tokens per Sec:     4993, Lr: 0.000035
2022-09-15 06:47:23,595 - INFO - joeynmt.training - Epoch   2, Step:   199400, Batch Loss:     2.720987, Batch Acc: 0.004631, Tokens per Sec:     5067, Lr: 0.000035
2022-09-15 06:47:46,393 - INFO - joeynmt.training - Epoch   2, Step:   199500, Batch Loss:     2.678196, Batch Acc: 0.004974, Tokens per Sec:     5001, Lr: 0.000035
2022-09-15 06:48:09,175 - INFO - joeynmt.training - Epoch   2, Step:   199600, Batch Loss:     2.466534, Batch Acc: 0.005090, Tokens per Sec:     5036, Lr: 0.000035
2022-09-15 06:48:31,953 - INFO - joeynmt.training - Epoch   2, Step:   199700, Batch Loss:     2.673551, Batch Acc: 0.004877, Tokens per Sec:     5095, Lr: 0.000035
2022-09-15 06:48:54,733 - INFO - joeynmt.training - Epoch   2, Step:   199800, Batch Loss:     2.768292, Batch Acc: 0.004228, Tokens per Sec:     5108, Lr: 0.000035
2022-09-15 06:49:17,391 - INFO - joeynmt.training - Epoch   2, Step:   199900, Batch Loss:     2.915219, Batch Acc: 0.004042, Tokens per Sec:     5078, Lr: 0.000035
2022-09-15 06:49:40,045 - INFO - joeynmt.training - Epoch   2, Step:   200000, Batch Loss:     2.676454, Batch Acc: 0.005238, Tokens per Sec:     4972, Lr: 0.000035
2022-09-15 06:49:40,045 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 07:08:09,713 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 07:08:09,715 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.50, loss:   2.66, ppl:  14.28, acc:   0.49, generation: 1102.6982[sec], evaluation: 6.6219[sec]
2022-09-15 07:08:09,719 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 07:08:11,891 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/197000.ckpt
2022-09-15 07:08:11,934 - INFO - joeynmt.training - Example #0
2022-09-15 07:08:11,946 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 07:08:11,946 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 07:08:11,946 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को ढका हुआ है
2022-09-15 07:08:11,946 - INFO - joeynmt.training - Example #1
2022-09-15 07:08:11,957 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 07:08:11,957 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 07:08:11,957 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-15 07:08:11,957 - INFO - joeynmt.training - Example #2
2022-09-15 07:08:11,967 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 07:08:11,967 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 07:08:11,967 - INFO - joeynmt.training - 	Hypothesis: कार्य , आंदोलन और इम
2022-09-15 07:08:11,967 - INFO - joeynmt.training - Example #3
2022-09-15 07:08:11,978 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 07:08:11,978 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 07:08:11,978 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 07:08:34,835 - INFO - joeynmt.training - Epoch   2, Step:   200100, Batch Loss:     2.901125, Batch Acc: 0.004396, Tokens per Sec:     4529, Lr: 0.000035
2022-09-15 07:08:57,504 - INFO - joeynmt.training - Epoch   2, Step:   200200, Batch Loss:     2.671663, Batch Acc: 0.004523, Tokens per Sec:     5052, Lr: 0.000035
2022-09-15 07:09:20,259 - INFO - joeynmt.training - Epoch   2, Step:   200300, Batch Loss:     2.803390, Batch Acc: 0.004396, Tokens per Sec:     5099, Lr: 0.000035
2022-09-15 07:09:43,091 - INFO - joeynmt.training - Epoch   2, Step:   200400, Batch Loss:     2.807390, Batch Acc: 0.003775, Tokens per Sec:     5024, Lr: 0.000035
2022-09-15 07:10:05,910 - INFO - joeynmt.training - Epoch   2, Step:   200500, Batch Loss:     2.677365, Batch Acc: 0.004408, Tokens per Sec:     5041, Lr: 0.000035
2022-09-15 07:10:28,726 - INFO - joeynmt.training - Epoch   2, Step:   200600, Batch Loss:     2.595201, Batch Acc: 0.005171, Tokens per Sec:     5043, Lr: 0.000035
2022-09-15 07:10:51,407 - INFO - joeynmt.training - Epoch   2, Step:   200700, Batch Loss:     2.667221, Batch Acc: 0.004604, Tokens per Sec:     5094, Lr: 0.000035
2022-09-15 07:11:14,257 - INFO - joeynmt.training - Epoch   2, Step:   200800, Batch Loss:     2.841647, Batch Acc: 0.004706, Tokens per Sec:     5012, Lr: 0.000035
2022-09-15 07:11:37,105 - INFO - joeynmt.training - Epoch   2, Step:   200900, Batch Loss:     2.453547, Batch Acc: 0.004302, Tokens per Sec:     5108, Lr: 0.000035
2022-09-15 07:11:59,857 - INFO - joeynmt.training - Epoch   2, Step:   201000, Batch Loss:     3.060385, Batch Acc: 0.003742, Tokens per Sec:     5050, Lr: 0.000035
2022-09-15 07:11:59,857 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 07:30:46,159 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 07:30:46,160 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.56, loss:   2.65, ppl:  14.17, acc:   0.50, generation: 1119.1880[sec], evaluation: 6.7653[sec]
2022-09-15 07:30:46,164 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 07:30:47,632 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/198000.ckpt
2022-09-15 07:30:47,674 - INFO - joeynmt.training - Example #0
2022-09-15 07:30:47,687 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 07:30:47,687 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 07:30:47,687 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को ढका हुआ है
2022-09-15 07:30:47,687 - INFO - joeynmt.training - Example #1
2022-09-15 07:30:47,697 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 07:30:47,697 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 07:30:47,697 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 07:30:47,697 - INFO - joeynmt.training - Example #2
2022-09-15 07:30:47,708 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 07:30:47,708 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 07:30:47,708 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और इम
2022-09-15 07:30:47,708 - INFO - joeynmt.training - Example #3
2022-09-15 07:30:47,718 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 07:30:47,718 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 07:30:47,718 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 07:31:10,591 - INFO - joeynmt.training - Epoch   2, Step:   201100, Batch Loss:     2.816682, Batch Acc: 0.003715, Tokens per Sec:     4672, Lr: 0.000035
2022-09-15 07:31:33,399 - INFO - joeynmt.training - Epoch   2, Step:   201200, Batch Loss:     2.891230, Batch Acc: 0.003580, Tokens per Sec:     5071, Lr: 0.000035
2022-09-15 07:31:56,155 - INFO - joeynmt.training - Epoch   2, Step:   201300, Batch Loss:     2.876618, Batch Acc: 0.004161, Tokens per Sec:     5049, Lr: 0.000035
2022-09-15 07:32:18,926 - INFO - joeynmt.training - Epoch   2, Step:   201400, Batch Loss:     2.714729, Batch Acc: 0.003975, Tokens per Sec:     4994, Lr: 0.000035
2022-09-15 07:32:41,603 - INFO - joeynmt.training - Epoch   2, Step:   201500, Batch Loss:     2.729680, Batch Acc: 0.004531, Tokens per Sec:     5032, Lr: 0.000035
2022-09-15 07:33:04,232 - INFO - joeynmt.training - Epoch   2, Step:   201600, Batch Loss:     2.947062, Batch Acc: 0.003950, Tokens per Sec:     5034, Lr: 0.000035
2022-09-15 07:33:26,862 - INFO - joeynmt.training - Epoch   2, Step:   201700, Batch Loss:     2.900627, Batch Acc: 0.004256, Tokens per Sec:     5026, Lr: 0.000035
2022-09-15 07:33:49,615 - INFO - joeynmt.training - Epoch   2, Step:   201800, Batch Loss:     2.492532, Batch Acc: 0.004339, Tokens per Sec:     5035, Lr: 0.000035
2022-09-15 07:34:12,319 - INFO - joeynmt.training - Epoch   2, Step:   201900, Batch Loss:     2.942937, Batch Acc: 0.004208, Tokens per Sec:     5035, Lr: 0.000035
2022-09-15 07:34:35,090 - INFO - joeynmt.training - Epoch   2, Step:   202000, Batch Loss:     2.843464, Batch Acc: 0.004303, Tokens per Sec:     5082, Lr: 0.000035
2022-09-15 07:34:35,090 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 07:53:43,809 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 07:53:43,810 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.65, loss:   2.64, ppl:  14.04, acc:   0.50, generation: 1141.7384[sec], evaluation: 6.6335[sec]
2022-09-15 07:53:43,814 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 07:53:45,241 - INFO - joeynmt.helpers - delete /home/common/other_files/shakya_workspace/joey_nmt/machine_nmt/joeynmt/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/199000.ckpt
2022-09-15 07:53:45,284 - INFO - joeynmt.training - Example #0
2022-09-15 07:53:45,296 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 07:53:45,296 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 07:53:45,296 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को ढका हुआ है
2022-09-15 07:53:45,296 - INFO - joeynmt.training - Example #1
2022-09-15 07:53:45,307 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 07:53:45,307 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 07:53:45,307 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 07:53:45,307 - INFO - joeynmt.training - Example #2
2022-09-15 07:53:45,318 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 07:53:45,318 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 07:53:45,318 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और इम
2022-09-15 07:53:45,318 - INFO - joeynmt.training - Example #3
2022-09-15 07:53:45,329 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 07:53:45,329 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 07:53:45,329 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 07:54:08,206 - INFO - joeynmt.training - Epoch   2, Step:   202100, Batch Loss:     2.837337, Batch Acc: 0.003937, Tokens per Sec:     4821, Lr: 0.000035
2022-09-15 07:54:31,010 - INFO - joeynmt.training - Epoch   2, Step:   202200, Batch Loss:     2.703827, Batch Acc: 0.004480, Tokens per Sec:     5022, Lr: 0.000035
2022-09-15 07:54:53,736 - INFO - joeynmt.training - Epoch   2, Step:   202300, Batch Loss:     2.960382, Batch Acc: 0.004055, Tokens per Sec:     4991, Lr: 0.000035
2022-09-15 07:55:16,351 - INFO - joeynmt.training - Epoch   2, Step:   202400, Batch Loss:     2.675699, Batch Acc: 0.004731, Tokens per Sec:     5122, Lr: 0.000035
2022-09-15 07:55:39,026 - INFO - joeynmt.training - Epoch   2, Step:   202500, Batch Loss:     2.896457, Batch Acc: 0.005305, Tokens per Sec:     4947, Lr: 0.000035
2022-09-15 07:56:01,692 - INFO - joeynmt.training - Epoch   2, Step:   202600, Batch Loss:     2.607372, Batch Acc: 0.004675, Tokens per Sec:     4974, Lr: 0.000035
2022-09-15 07:56:24,485 - INFO - joeynmt.training - Epoch   2, Step:   202700, Batch Loss:     3.147129, Batch Acc: 0.004027, Tokens per Sec:     4990, Lr: 0.000035
2022-09-15 07:56:47,277 - INFO - joeynmt.training - Epoch   2, Step:   202800, Batch Loss:     2.789565, Batch Acc: 0.004729, Tokens per Sec:     5019, Lr: 0.000035
2022-09-15 07:57:10,027 - INFO - joeynmt.training - Epoch   2, Step:   202900, Batch Loss:     2.963069, Batch Acc: 0.003372, Tokens per Sec:     5044, Lr: 0.000035
2022-09-15 07:57:32,859 - INFO - joeynmt.training - Epoch   2, Step:   203000, Batch Loss:     2.904061, Batch Acc: 0.004179, Tokens per Sec:     5031, Lr: 0.000035
2022-09-15 07:57:32,859 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)

2022-09-15 14:35:41,751 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).
2022-09-15 14:35:41,751 - INFO - joeynmt.helpers -                           cfg.name : transformer_100_enhi_bpe
2022-09-15 14:35:41,751 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2022-09-15 14:35:41,751 - INFO - joeynmt.helpers -                     cfg.data.train : /home/ubuntu/joeynmt_kriti/data/datasets_enhi_100/train_tok
2022-09-15 14:35:41,751 - INFO - joeynmt.helpers -                       cfg.data.dev : /home/ubuntu/joeynmt_kriti/data/datasets_enhi_100/val_tok
2022-09-15 14:35:41,751 - INFO - joeynmt.helpers -                      cfg.data.test : /home/ubuntu/joeynmt_kriti/data/datasets_enhi_100/test_tok
2022-09-15 14:35:41,751 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain_ac
2022-09-15 14:35:41,751 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en
2022-09-15 14:35:41,751 - INFO - joeynmt.helpers -            cfg.data.src.max_length : 60
2022-09-15 14:35:41,751 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2022-09-15 14:35:41,751 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2022-09-15 14:35:41,751 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : /home/ubuntu/joeynmt_kriti/data/datasets_enhi_100/vocab.en
2022-09-15 14:35:41,751 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : /home/ubuntu/joeynmt_kriti/data/datasets_enhi_100/en.bpe.codes
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 16000
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : moses
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : hi
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers -            cfg.data.trg.max_length : 60
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : /home/ubuntu/joeynmt_kriti/data/datasets_enhi_100/vocab.hi
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : /home/ubuntu/joeynmt_kriti/data/datasets_enhi_100/hi.bpe.codes
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 16000
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers -                 cfg.testing.n_best : 1
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers -             cfg.testing.batch_size : 1024
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers -             cfg.testing.batch_type : token
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers -      cfg.testing.max_output_length : 130
2022-09-15 14:35:41,752 - INFO - joeynmt.helpers -           cfg.testing.eval_metrics : ['bleu']
2022-09-15 14:35:41,753 - INFO - joeynmt.helpers -            cfg.testing.return_prob : none
2022-09-15 14:35:41,753 - INFO - joeynmt.helpers -       cfg.testing.return_attention : False
2022-09-15 14:35:41,753 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.tokenize : 13a
2022-09-15 14:35:41,753 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.lowercase : False
2022-09-15 14:35:41,753 - INFO - joeynmt.helpers -     cfg.active_learning.query_size : 10000
2022-09-15 14:35:41,753 - INFO - joeynmt.helpers -    cfg.active_learning.interactive : False
2022-09-15 14:35:41,753 - INFO - joeynmt.helpers -      cfg.active_learning.pool_size : 6
2022-09-15 14:35:41,753 - INFO - joeynmt.helpers -     cfg.active_learning.batch_size : 256
2022-09-15 14:35:41,753 - INFO - joeynmt.helpers -    cfg.active_learning.num_workers : 4
2022-09-15 14:35:41,753 - INFO - joeynmt.helpers -    cfg.active_learning.num_queries : 5
2022-09-15 14:35:41,753 - INFO - joeynmt.helpers -     cfg.active_learning.al_percent : 30
2022-09-15 14:35:41,753 - INFO - joeynmt.helpers - cfg.active_learning.query_strategy : least_confidence
2022-09-15 14:35:41,753 - INFO - joeynmt.helpers -          cfg.active_learning.epoch : 1
2022-09-15 14:35:41,753 - INFO - joeynmt.helpers - cfg.active_learning.validation_freq : 1000
2022-09-15 14:35:41,753 - INFO - joeynmt.helpers -            cfg.training.load_model : /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt
2022-09-15 14:35:41,753 - INFO - joeynmt.helpers -       cfg.training.reset_best_ckpt : False
2022-09-15 14:35:41,753 - INFO - joeynmt.helpers -       cfg.training.reset_scheduler : False
2022-09-15 14:35:41,754 - INFO - joeynmt.helpers -       cfg.training.reset_optimizer : False
2022-09-15 14:35:41,754 - INFO - joeynmt.helpers -      cfg.training.reset_iter_state : False
2022-09-15 14:35:41,754 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2022-09-15 14:35:41,754 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2022-09-15 14:35:41,754 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2022-09-15 14:35:41,754 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]
2022-09-15 14:35:41,754 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2022-09-15 14:35:41,754 - INFO - joeynmt.helpers - cfg.training.learning_rate_decay_length : 2500
2022-09-15 14:35:41,754 - INFO - joeynmt.helpers -    cfg.training.learning_rate_peak : 0.005
2022-09-15 14:35:41,754 - INFO - joeynmt.helpers -  cfg.training.learning_rate_warmup : 1000
2022-09-15 14:35:41,754 - INFO - joeynmt.helpers -  cfg.training.learning_rate_factor : 0.5
2022-09-15 14:35:41,754 - INFO - joeynmt.helpers -              cfg.training.patience : 5
2022-09-15 14:35:41,754 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2022-09-15 14:35:41,754 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy
2022-09-15 14:35:41,754 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2022-09-15 14:35:41,754 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 1e-08
2022-09-15 14:35:41,754 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2022-09-15 14:35:41,754 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.1
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers -            cfg.training.batch_size : 4096
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 1
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers -                cfg.training.epochs : 1
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 1000
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers -             cfg.training.model_dir : /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3]
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 3
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2022-09-15 14:35:41,755 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : False
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 1024
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.3
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers -       cfg.model.encoder.layer_norm : pre
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 4
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2022-09-15 14:35:41,756 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 1024
2022-09-15 14:35:41,757 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.3
2022-09-15 14:35:41,757 - INFO - joeynmt.helpers -       cfg.model.decoder.layer_norm : pre
2022-09-15 14:35:41,803 - INFO - joeynmt.data - Building tokenizer...
2022-09-15 14:35:41,908 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:35:41,908 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:35:41,908 - INFO - joeynmt.data - Loading train set...
2022-09-15 14:39:38,622 - INFO - joeynmt.data - Building vocabulary...
2022-09-15 14:39:41,791 - INFO - joeynmt.data - Loading dev set...
2022-09-15 14:39:48,817 - INFO - joeynmt.data - Loading test set...
2022-09-15 14:39:55,702 - INFO - joeynmt.data - Data loaded.
2022-09-15 14:39:55,703 - INFO - joeynmt.helpers - Train dataset: PlaintextDatasetAC(split=train, len=1552563, src_lang=en, trg_lang=hi, has_trg=True, random_subset=-1)
2022-09-15 14:39:55,703 - INFO - joeynmt.helpers - Valid dataset: PlaintextDatasetAC(split=dev, len=40856, src_lang=en, trg_lang=hi, has_trg=True, random_subset=-1)
2022-09-15 14:39:55,703 - INFO - joeynmt.helpers -  Test dataset: PlaintextDatasetAC(split=test, len=40858, src_lang=en, trg_lang=hi, has_trg=True, random_subset=-1)
2022-09-15 14:39:55,704 - INFO - joeynmt.helpers - First training example:
	[SRC] give your application an accessibility work@@ out
	[TRG] अपने अनुप्रयोग को पहुंच@@ नीयता व्यायाम का लाभ दें
2022-09-15 14:39:55,704 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) , (6) . (7) of (8) and (9) to
2022-09-15 14:39:55,704 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) के (5) है (6) । (7) , (8) और (9) में
2022-09-15 14:39:55,704 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 16021
2022-09-15 14:39:55,704 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 16174
2022-09-15 14:39:55,715 - INFO - joeynmt.training - BASELINE MODEL START
2022-09-15 14:39:55,715 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:39:55,715 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:39:56,147 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:40:01,090 - INFO - joeynmt.model - Total params: 19302144
2022-09-15 14:40:01,091 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2022-09-15 14:40:01,091 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm="pre"),
	decoder=TransformerDecoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm="pre"),
	src_embed=Embeddings(embedding_dim=256, vocab_size=16021),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=16174),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.1))
2022-09-15 14:40:03,884 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=[0.9, 0.999])
2022-09-15 14:40:03,885 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=5)
2022-09-15 14:40:03,885 - INFO - joeynmt.training - Loading model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt
2022-09-15 14:40:04,234 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:40:04,334 - INFO - joeynmt.training - BASELINE MODEL END
2022-09-15 14:40:04,336 - INFO - joeynmt.training - Random Indices picked: [121958 146867 131932 365838 259178 119879 110268 207892  54886 137337] length: 27946
2022-09-15 14:40:04,336 - INFO - joeynmt.training - Executing Random Strategy
2022-09-15 14:40:04,337 - INFO - joeynmt.training - Final Query Indices picked: [121958, 146867, 131932, 365838, 259178, 119879, 110268, 207892, 54886, 137337] length: 10000
2022-09-15 14:40:04,337 - INFO - joeynmt.training - Query the samples, one at a time (interactive/file)
2022-09-15 14:40:06,642 - INFO - joeynmt.training - Remaining Pool Data size: 455884
2022-09-15 14:40:06,642 - INFO - joeynmt.training - Active Learning Data ready to train size: 1096795
2022-09-15 14:40:06,642 - INFO - joeynmt.training - ACTIVE LEARNING MODEL END - RANDOM
2022-09-15 14:40:06,642 - INFO - joeynmt.training - ACTIVE LEARNING MODEL START - LEAST CONFIDENCE 0
2022-09-15 14:40:06,644 - INFO - joeynmt.training - Random Indices picked: [ 21058 388370 228845 308840 240035  38872  54971 344227  23306 421426] length: 27353
2022-09-15 14:40:06,644 - INFO - joeynmt.training - Processing Predictions on Batch 0/107
2022-09-15 14:40:10,103 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:40:10,105 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:40:10,471 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:40:10,748 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:40:10,923 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:40:10,923 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:40:10,961 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:40:15,189 - INFO - joeynmt.prediction - Generation took 4.2153[sec]. (No references given)
2022-09-15 14:40:15,196 - INFO - joeynmt.training - Processing Predictions on Batch 1/107
2022-09-15 14:40:18,615 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:40:18,615 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:40:18,964 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:40:19,252 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:40:19,363 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:40:19,363 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:40:19,402 - INFO - joeynmt.prediction - Predicting 248 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:40:23,608 - INFO - joeynmt.prediction - Generation took 4.1945[sec]. (No references given)
2022-09-15 14:40:23,617 - INFO - joeynmt.training - Processing Predictions on Batch 2/107
2022-09-15 14:40:27,091 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:40:27,091 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:40:27,425 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:40:27,761 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:40:27,873 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:40:27,873 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:40:27,910 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:40:31,574 - INFO - joeynmt.prediction - Generation took 3.6521[sec]. (No references given)
2022-09-15 14:40:31,585 - INFO - joeynmt.training - Processing Predictions on Batch 3/107
2022-09-15 14:40:35,124 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:40:35,124 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:40:35,459 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:40:35,731 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:40:35,843 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:40:35,843 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:40:35,882 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:40:39,872 - INFO - joeynmt.prediction - Generation took 3.9785[sec]. (No references given)
2022-09-15 14:40:39,881 - INFO - joeynmt.training - Processing Predictions on Batch 4/107
2022-09-15 14:40:43,461 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:40:43,461 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:40:43,796 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:40:44,050 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:40:44,162 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:40:44,162 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:40:44,198 - INFO - joeynmt.prediction - Predicting 251 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:40:48,518 - INFO - joeynmt.prediction - Generation took 4.3080[sec]. (No references given)
2022-09-15 14:40:48,526 - INFO - joeynmt.training - Processing Predictions on Batch 5/107
2022-09-15 14:40:52,117 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:40:52,117 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:40:52,451 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:40:52,701 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:40:52,813 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:40:52,813 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:40:52,847 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:40:56,616 - INFO - joeynmt.prediction - Generation took 3.7573[sec]. (No references given)
2022-09-15 14:40:56,626 - INFO - joeynmt.training - Processing Predictions on Batch 6/107
2022-09-15 14:41:00,294 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:41:00,294 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:41:00,627 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:41:00,881 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:41:00,993 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:41:00,993 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:41:01,035 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:41:04,822 - INFO - joeynmt.prediction - Generation took 3.7744[sec]. (No references given)
2022-09-15 14:41:04,831 - INFO - joeynmt.training - Processing Predictions on Batch 7/107
2022-09-15 14:41:08,596 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:41:08,597 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:41:08,992 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:41:09,246 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:41:09,358 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:41:09,358 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:41:09,393 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:41:13,232 - INFO - joeynmt.prediction - Generation took 3.8269[sec]. (No references given)
2022-09-15 14:41:13,242 - INFO - joeynmt.training - Processing Predictions on Batch 8/107
2022-09-15 14:41:17,023 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:41:17,023 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:41:17,361 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:41:17,615 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:41:17,728 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:41:17,728 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:41:17,766 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:41:21,733 - INFO - joeynmt.prediction - Generation took 3.9545[sec]. (No references given)
2022-09-15 14:41:21,742 - INFO - joeynmt.training - Processing Predictions on Batch 9/107
2022-09-15 14:41:25,580 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:41:25,580 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:41:25,916 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:41:26,233 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:41:26,347 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:41:26,347 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:41:26,388 - INFO - joeynmt.prediction - Predicting 250 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:41:30,223 - INFO - joeynmt.prediction - Generation took 3.8225[sec]. (No references given)
2022-09-15 14:41:30,233 - INFO - joeynmt.training - Processing Predictions on Batch 10/107
2022-09-15 14:41:34,084 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:41:34,084 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:41:34,419 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:41:34,674 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:41:34,786 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:41:34,786 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:41:34,824 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:41:38,668 - INFO - joeynmt.prediction - Generation took 3.8322[sec]. (No references given)
2022-09-15 14:41:38,678 - INFO - joeynmt.training - Processing Predictions on Batch 11/107
2022-09-15 14:41:42,510 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:41:42,510 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:41:42,846 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:41:43,099 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:41:43,211 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:41:43,211 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:41:43,252 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:41:47,575 - INFO - joeynmt.prediction - Generation took 4.3107[sec]. (No references given)
2022-09-15 14:41:47,585 - INFO - joeynmt.training - Processing Predictions on Batch 12/107
2022-09-15 14:41:51,357 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:41:51,357 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:41:51,693 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:41:51,946 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:41:52,058 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:41:52,059 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:41:52,100 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:41:56,329 - INFO - joeynmt.prediction - Generation took 4.2167[sec]. (No references given)
2022-09-15 14:41:56,339 - INFO - joeynmt.training - Processing Predictions on Batch 13/107
2022-09-15 14:42:00,098 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:42:00,098 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:42:00,432 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:42:00,687 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:42:00,799 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:42:00,799 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:42:00,837 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:42:04,667 - INFO - joeynmt.prediction - Generation took 3.8184[sec]. (No references given)
2022-09-15 14:42:04,676 - INFO - joeynmt.training - Processing Predictions on Batch 14/107
2022-09-15 14:42:08,480 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:42:08,481 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:42:08,814 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:42:09,129 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:42:09,241 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:42:09,242 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:42:09,283 - INFO - joeynmt.prediction - Predicting 248 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:42:13,097 - INFO - joeynmt.prediction - Generation took 3.8026[sec]. (No references given)
2022-09-15 14:42:13,107 - INFO - joeynmt.training - Processing Predictions on Batch 15/107
2022-09-15 14:42:16,937 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:42:16,938 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:42:17,272 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:42:17,527 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:42:17,638 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:42:17,640 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:42:17,677 - INFO - joeynmt.prediction - Predicting 251 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:42:21,853 - INFO - joeynmt.prediction - Generation took 4.1632[sec]. (No references given)
2022-09-15 14:42:21,862 - INFO - joeynmt.training - Processing Predictions on Batch 16/107
2022-09-15 14:42:25,669 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:42:25,669 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:42:26,006 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:42:26,256 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:42:26,369 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:42:26,369 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:42:26,406 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:42:30,285 - INFO - joeynmt.prediction - Generation took 3.8662[sec]. (No references given)
2022-09-15 14:42:30,294 - INFO - joeynmt.training - Processing Predictions on Batch 17/107
2022-09-15 14:42:34,047 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:42:34,048 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:42:34,384 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:42:34,636 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:42:34,747 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:42:34,748 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:42:34,786 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:42:38,920 - INFO - joeynmt.prediction - Generation took 4.1225[sec]. (No references given)
2022-09-15 14:42:38,929 - INFO - joeynmt.training - Processing Predictions on Batch 18/107
2022-09-15 14:42:42,725 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:42:42,725 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:42:43,060 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:42:43,310 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:42:43,422 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:42:43,422 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:42:43,464 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:42:47,563 - INFO - joeynmt.prediction - Generation took 4.0860[sec]. (No references given)
2022-09-15 14:42:47,572 - INFO - joeynmt.training - Processing Predictions on Batch 19/107
2022-09-15 14:42:51,433 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:42:51,433 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:42:51,767 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:42:52,078 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:42:52,191 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:42:52,191 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:42:52,231 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:42:56,205 - INFO - joeynmt.prediction - Generation took 3.9620[sec]. (No references given)
2022-09-15 14:42:56,215 - INFO - joeynmt.training - Processing Predictions on Batch 20/107
2022-09-15 14:43:00,077 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:43:00,078 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:43:00,416 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:43:00,668 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:43:00,781 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:43:00,782 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:43:00,818 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:43:04,722 - INFO - joeynmt.prediction - Generation took 3.8914[sec]. (No references given)
2022-09-15 14:43:04,731 - INFO - joeynmt.training - Processing Predictions on Batch 21/107
2022-09-15 14:43:08,590 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:43:08,590 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:43:08,923 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:43:09,176 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:43:09,288 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:43:09,288 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:43:09,326 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:43:13,188 - INFO - joeynmt.prediction - Generation took 3.8501[sec]. (No references given)
2022-09-15 14:43:13,198 - INFO - joeynmt.training - Processing Predictions on Batch 22/107
2022-09-15 14:43:17,066 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:43:17,067 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:43:17,403 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:43:17,672 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:43:17,788 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:43:17,788 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:43:17,824 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:43:21,504 - INFO - joeynmt.prediction - Generation took 3.6675[sec]. (No references given)
2022-09-15 14:43:21,515 - INFO - joeynmt.training - Processing Predictions on Batch 23/107
2022-09-15 14:43:25,323 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:43:25,323 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:43:25,657 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:43:25,909 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:43:26,020 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:43:26,020 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:43:26,056 - INFO - joeynmt.prediction - Predicting 248 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:43:29,655 - INFO - joeynmt.prediction - Generation took 3.5872[sec]. (No references given)
2022-09-15 14:43:29,664 - INFO - joeynmt.training - Processing Predictions on Batch 24/107
2022-09-15 14:43:33,537 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:43:33,537 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:43:33,871 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:43:34,187 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:43:34,300 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:43:34,300 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:43:34,337 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:43:38,548 - INFO - joeynmt.prediction - Generation took 4.1990[sec]. (No references given)
2022-09-15 14:43:38,558 - INFO - joeynmt.training - Processing Predictions on Batch 25/107
2022-09-15 14:43:42,391 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:43:42,392 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:43:42,725 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:43:42,980 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:43:43,093 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:43:43,093 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:43:43,129 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:43:46,716 - INFO - joeynmt.prediction - Generation took 3.5751[sec]. (No references given)
2022-09-15 14:43:46,726 - INFO - joeynmt.training - Processing Predictions on Batch 26/107
2022-09-15 14:43:50,581 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:43:50,581 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:43:50,917 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:43:51,167 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:43:51,279 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:43:51,280 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:43:51,317 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:43:55,201 - INFO - joeynmt.prediction - Generation took 3.8719[sec]. (No references given)
2022-09-15 14:43:55,210 - INFO - joeynmt.training - Processing Predictions on Batch 27/107
2022-09-15 14:43:59,021 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:43:59,021 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:43:59,356 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:43:59,606 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:43:59,718 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:43:59,719 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:43:59,757 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:44:03,533 - INFO - joeynmt.prediction - Generation took 3.7645[sec]. (No references given)
2022-09-15 14:44:03,543 - INFO - joeynmt.training - Processing Predictions on Batch 28/107
2022-09-15 14:44:07,325 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:44:07,325 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:44:07,660 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:44:07,913 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:44:08,025 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:44:08,026 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:44:08,061 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:44:11,817 - INFO - joeynmt.prediction - Generation took 3.7440[sec]. (No references given)
2022-09-15 14:44:11,826 - INFO - joeynmt.training - Processing Predictions on Batch 29/107
2022-09-15 14:44:15,647 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:44:15,647 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:44:15,980 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:44:16,298 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:44:16,410 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:44:16,410 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:44:16,449 - INFO - joeynmt.prediction - Predicting 251 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:44:20,632 - INFO - joeynmt.prediction - Generation took 4.1705[sec]. (No references given)
2022-09-15 14:44:20,641 - INFO - joeynmt.training - Processing Predictions on Batch 30/107
2022-09-15 14:44:24,484 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:44:24,484 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:44:24,819 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:44:25,069 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:44:25,182 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:44:25,182 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:44:25,223 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:44:29,444 - INFO - joeynmt.prediction - Generation took 4.2081[sec]. (No references given)
2022-09-15 14:44:29,453 - INFO - joeynmt.training - Processing Predictions on Batch 31/107
2022-09-15 14:44:33,291 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:44:33,291 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:44:33,627 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:44:33,881 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:44:33,992 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:44:33,992 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:44:34,029 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:44:37,937 - INFO - joeynmt.prediction - Generation took 3.8960[sec]. (No references given)
2022-09-15 14:44:37,946 - INFO - joeynmt.training - Processing Predictions on Batch 32/107
2022-09-15 14:44:41,778 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:44:41,778 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:44:42,112 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:44:42,363 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:44:42,476 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:44:42,476 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:44:42,515 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:44:46,371 - INFO - joeynmt.prediction - Generation took 3.8440[sec]. (No references given)
2022-09-15 14:44:46,380 - INFO - joeynmt.training - Processing Predictions on Batch 33/107
2022-09-15 14:44:50,142 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:44:50,142 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:44:50,477 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:44:50,754 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:44:50,879 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:44:50,879 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:44:50,918 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:44:54,842 - INFO - joeynmt.prediction - Generation took 3.9105[sec]. (No references given)
2022-09-15 14:44:54,852 - INFO - joeynmt.training - Processing Predictions on Batch 34/107
2022-09-15 14:44:58,692 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:44:58,692 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:44:59,027 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:44:59,307 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:44:59,503 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:44:59,503 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:44:59,543 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:45:03,691 - INFO - joeynmt.prediction - Generation took 4.1362[sec]. (No references given)
2022-09-15 14:45:03,701 - INFO - joeynmt.training - Processing Predictions on Batch 35/107
2022-09-15 14:45:07,495 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:45:07,495 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:45:07,832 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:45:08,110 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:45:08,235 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:45:08,235 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:45:08,273 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:45:12,128 - INFO - joeynmt.prediction - Generation took 3.8425[sec]. (No references given)
2022-09-15 14:45:12,138 - INFO - joeynmt.training - Processing Predictions on Batch 36/107
2022-09-15 14:45:15,947 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:45:15,947 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:45:16,282 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:45:16,562 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:45:16,689 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:45:16,689 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:45:16,727 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:45:21,325 - INFO - joeynmt.prediction - Generation took 4.5857[sec]. (No references given)
2022-09-15 14:45:21,335 - INFO - joeynmt.training - Processing Predictions on Batch 37/107
2022-09-15 14:45:25,159 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:45:25,159 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:45:25,496 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:45:25,779 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:45:25,904 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:45:25,904 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:45:25,946 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:45:30,002 - INFO - joeynmt.prediction - Generation took 4.0444[sec]. (No references given)
2022-09-15 14:45:30,013 - INFO - joeynmt.training - Processing Predictions on Batch 38/107
2022-09-15 14:45:33,834 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:45:33,834 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:45:34,169 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:45:34,449 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:45:34,573 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:45:34,573 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:45:34,611 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:45:38,397 - INFO - joeynmt.prediction - Generation took 3.7744[sec]. (No references given)
2022-09-15 14:45:38,407 - INFO - joeynmt.training - Processing Predictions on Batch 39/107
2022-09-15 14:45:42,286 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:45:42,286 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:45:42,691 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:45:42,970 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:45:43,095 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:45:43,095 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:45:43,138 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:45:47,302 - INFO - joeynmt.prediction - Generation took 4.1515[sec]. (No references given)
2022-09-15 14:45:47,313 - INFO - joeynmt.training - Processing Predictions on Batch 40/107
2022-09-15 14:45:51,179 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:45:51,180 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:45:51,521 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:45:51,800 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:45:51,925 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:45:51,925 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:45:51,966 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:45:56,080 - INFO - joeynmt.prediction - Generation took 4.1014[sec]. (No references given)
2022-09-15 14:45:56,090 - INFO - joeynmt.training - Processing Predictions on Batch 41/107
2022-09-15 14:45:59,945 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:45:59,945 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:46:00,280 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:46:00,563 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:46:00,763 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:46:00,763 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:46:00,804 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:46:04,729 - INFO - joeynmt.prediction - Generation took 3.9127[sec]. (No references given)
2022-09-15 14:46:04,739 - INFO - joeynmt.training - Processing Predictions on Batch 42/107
2022-09-15 14:46:08,553 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:46:08,553 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:46:08,888 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:46:09,168 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:46:09,293 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:46:09,293 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:46:09,330 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:46:13,060 - INFO - joeynmt.prediction - Generation took 3.7186[sec]. (No references given)
2022-09-15 14:46:13,070 - INFO - joeynmt.training - Processing Predictions on Batch 43/107
2022-09-15 14:46:16,870 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:46:16,870 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:46:17,211 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:46:17,490 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:46:17,615 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:46:17,615 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:46:17,658 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:46:21,666 - INFO - joeynmt.prediction - Generation took 3.9953[sec]. (No references given)
2022-09-15 14:46:21,676 - INFO - joeynmt.training - Processing Predictions on Batch 44/107
2022-09-15 14:46:25,486 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:46:25,486 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:46:25,820 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:46:26,102 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:46:26,227 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:46:26,227 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:46:26,267 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:46:30,227 - INFO - joeynmt.prediction - Generation took 3.9481[sec]. (No references given)
2022-09-15 14:46:30,237 - INFO - joeynmt.training - Processing Predictions on Batch 45/107
2022-09-15 14:46:34,032 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:46:34,032 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:46:34,367 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:46:34,647 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:46:34,771 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:46:34,771 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:46:34,808 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:46:38,681 - INFO - joeynmt.prediction - Generation took 3.8621[sec]. (No references given)
2022-09-15 14:46:38,691 - INFO - joeynmt.training - Processing Predictions on Batch 46/107
2022-09-15 14:46:42,524 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:46:42,525 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:46:42,861 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:46:43,141 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:46:43,266 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:46:43,266 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:46:43,304 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:46:47,426 - INFO - joeynmt.prediction - Generation took 4.1103[sec]. (No references given)
2022-09-15 14:46:47,436 - INFO - joeynmt.training - Processing Predictions on Batch 47/107
2022-09-15 14:46:51,266 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:46:51,266 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:46:51,603 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:46:51,882 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:46:52,006 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:46:52,006 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:46:52,041 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:46:55,558 - INFO - joeynmt.prediction - Generation took 3.5052[sec]. (No references given)
2022-09-15 14:46:55,567 - INFO - joeynmt.training - Processing Predictions on Batch 48/107
2022-09-15 14:46:59,328 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:46:59,328 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:46:59,663 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:46:59,915 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:47:00,027 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:47:00,028 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:47:00,066 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:47:03,920 - INFO - joeynmt.prediction - Generation took 3.8433[sec]. (No references given)
2022-09-15 14:47:03,930 - INFO - joeynmt.training - Processing Predictions on Batch 49/107
2022-09-15 14:47:07,725 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:47:07,726 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:47:08,062 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:47:08,383 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:47:08,496 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:47:08,497 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:47:08,538 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:47:12,113 - INFO - joeynmt.prediction - Generation took 3.5639[sec]. (No references given)
2022-09-15 14:47:12,123 - INFO - joeynmt.training - Processing Predictions on Batch 50/107
2022-09-15 14:47:15,928 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:47:15,929 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:47:16,264 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:47:16,516 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:47:16,628 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:47:16,629 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:47:16,670 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:47:20,883 - INFO - joeynmt.prediction - Generation took 4.2003[sec]. (No references given)
2022-09-15 14:47:20,893 - INFO - joeynmt.training - Processing Predictions on Batch 51/107
2022-09-15 14:47:24,722 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:47:24,722 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:47:25,055 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:47:25,308 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:47:25,420 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:47:25,421 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:47:25,454 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:47:29,009 - INFO - joeynmt.prediction - Generation took 3.5440[sec]. (No references given)
2022-09-15 14:47:29,018 - INFO - joeynmt.training - Processing Predictions on Batch 52/107
2022-09-15 14:47:32,854 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:47:32,855 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:47:33,256 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:47:33,505 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:47:33,618 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:47:33,618 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:47:33,658 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:47:37,517 - INFO - joeynmt.prediction - Generation took 3.8476[sec]. (No references given)
2022-09-15 14:47:37,527 - INFO - joeynmt.training - Processing Predictions on Batch 53/107
2022-09-15 14:47:41,355 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:47:41,355 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:47:41,690 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:47:41,944 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:47:42,057 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:47:42,057 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:47:42,097 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:47:46,058 - INFO - joeynmt.prediction - Generation took 3.9481[sec]. (No references given)
2022-09-15 14:47:46,067 - INFO - joeynmt.training - Processing Predictions on Batch 54/107
2022-09-15 14:47:49,917 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:47:49,917 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:47:50,251 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:47:50,501 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:47:50,614 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:47:50,614 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:47:50,655 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:47:55,492 - INFO - joeynmt.prediction - Generation took 4.8241[sec]. (No references given)
2022-09-15 14:47:55,502 - INFO - joeynmt.training - Processing Predictions on Batch 55/107
2022-09-15 14:47:59,269 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:47:59,269 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:47:59,602 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:47:59,854 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:47:59,966 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:47:59,967 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:48:00,003 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:48:03,437 - INFO - joeynmt.prediction - Generation took 3.4223[sec]. (No references given)
2022-09-15 14:48:03,446 - INFO - joeynmt.training - Processing Predictions on Batch 56/107
2022-09-15 14:48:07,215 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:48:07,215 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:48:07,552 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:48:07,802 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:48:07,915 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:48:07,915 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:48:07,953 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:48:11,940 - INFO - joeynmt.prediction - Generation took 3.9752[sec]. (No references given)
2022-09-15 14:48:11,951 - INFO - joeynmt.training - Processing Predictions on Batch 57/107
2022-09-15 14:48:15,748 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:48:15,748 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:48:16,083 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:48:16,333 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:48:16,445 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:48:16,445 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:48:16,483 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:48:20,961 - INFO - joeynmt.prediction - Generation took 4.4648[sec]. (No references given)
2022-09-15 14:48:20,970 - INFO - joeynmt.training - Processing Predictions on Batch 58/107
2022-09-15 14:48:24,706 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:48:24,706 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:48:25,039 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:48:25,291 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:48:25,404 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:48:25,404 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:48:25,443 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:48:28,890 - INFO - joeynmt.prediction - Generation took 3.4353[sec]. (No references given)
2022-09-15 14:48:28,900 - INFO - joeynmt.training - Processing Predictions on Batch 59/107
2022-09-15 14:48:32,665 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:48:32,665 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:48:33,000 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:48:33,250 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:48:33,362 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:48:33,363 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:48:33,397 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:48:37,514 - INFO - joeynmt.prediction - Generation took 4.1051[sec]. (No references given)
2022-09-15 14:48:37,524 - INFO - joeynmt.training - Processing Predictions on Batch 60/107
2022-09-15 14:48:41,314 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:48:41,314 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:48:41,650 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:48:41,904 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:48:42,016 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:48:42,018 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:48:42,057 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:48:46,285 - INFO - joeynmt.prediction - Generation took 4.2151[sec]. (No references given)
2022-09-15 14:48:46,294 - INFO - joeynmt.training - Processing Predictions on Batch 61/107
2022-09-15 14:48:50,046 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:48:50,046 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:48:50,380 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:48:50,632 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:48:50,746 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:48:50,746 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:48:50,788 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:48:55,014 - INFO - joeynmt.prediction - Generation took 4.2134[sec]. (No references given)
2022-09-15 14:48:55,024 - INFO - joeynmt.training - Processing Predictions on Batch 62/107
2022-09-15 14:48:58,786 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:48:58,786 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:48:59,122 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:48:59,372 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:48:59,485 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:48:59,485 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:48:59,521 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:49:03,489 - INFO - joeynmt.prediction - Generation took 3.9567[sec]. (No references given)
2022-09-15 14:49:03,499 - INFO - joeynmt.training - Processing Predictions on Batch 63/107
2022-09-15 14:49:07,294 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:49:07,294 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:49:07,628 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:49:07,882 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:49:07,994 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:49:07,994 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:49:08,034 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:49:12,185 - INFO - joeynmt.prediction - Generation took 4.1391[sec]. (No references given)
2022-09-15 14:49:12,195 - INFO - joeynmt.training - Processing Predictions on Batch 64/107
2022-09-15 14:49:15,950 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:49:15,950 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:49:16,283 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:49:16,535 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:49:16,648 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:49:16,648 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:49:16,684 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:49:20,704 - INFO - joeynmt.prediction - Generation took 4.0085[sec]. (No references given)
2022-09-15 14:49:20,714 - INFO - joeynmt.training - Processing Predictions on Batch 65/107
2022-09-15 14:49:24,483 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:49:24,483 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:49:24,817 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:49:25,070 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:49:25,182 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:49:25,183 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:49:25,219 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:49:29,171 - INFO - joeynmt.prediction - Generation took 3.9402[sec]. (No references given)
2022-09-15 14:49:29,180 - INFO - joeynmt.training - Processing Predictions on Batch 66/107
2022-09-15 14:49:32,976 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:49:32,976 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:49:33,310 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:49:33,563 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:49:33,675 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:49:33,675 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:49:33,712 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:49:37,390 - INFO - joeynmt.prediction - Generation took 3.6661[sec]. (No references given)
2022-09-15 14:49:37,399 - INFO - joeynmt.training - Processing Predictions on Batch 67/107
2022-09-15 14:49:41,154 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:49:41,154 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:49:41,488 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:49:41,741 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:49:41,854 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:49:41,854 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:49:41,891 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:49:46,068 - INFO - joeynmt.prediction - Generation took 4.1655[sec]. (No references given)
2022-09-15 14:49:46,077 - INFO - joeynmt.training - Processing Predictions on Batch 68/107
2022-09-15 14:49:49,834 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:49:49,834 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:49:50,168 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:49:50,419 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:49:50,531 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:49:50,531 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:49:50,566 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:49:54,393 - INFO - joeynmt.prediction - Generation took 3.8155[sec]. (No references given)
2022-09-15 14:49:54,402 - INFO - joeynmt.training - Processing Predictions on Batch 69/107
2022-09-15 14:49:58,198 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:49:58,198 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:49:58,534 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:49:58,786 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:49:58,898 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:49:58,898 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:49:58,935 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:50:02,444 - INFO - joeynmt.prediction - Generation took 3.4981[sec]. (No references given)
2022-09-15 14:50:02,454 - INFO - joeynmt.training - Processing Predictions on Batch 70/107
2022-09-15 14:50:06,192 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:50:06,193 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:50:06,528 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:50:06,783 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:50:06,895 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:50:06,895 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:50:06,932 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:50:10,579 - INFO - joeynmt.prediction - Generation took 3.6349[sec]. (No references given)
2022-09-15 14:50:10,588 - INFO - joeynmt.training - Processing Predictions on Batch 71/107
2022-09-15 14:50:14,343 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:50:14,343 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:50:14,678 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:50:14,930 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:50:15,042 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:50:15,042 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:50:15,075 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:50:18,903 - INFO - joeynmt.prediction - Generation took 3.8158[sec]. (No references given)
2022-09-15 14:50:18,912 - INFO - joeynmt.training - Processing Predictions on Batch 72/107
2022-09-15 14:50:22,705 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:50:22,705 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:50:23,041 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:50:23,298 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:50:23,411 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:50:23,411 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:50:23,447 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:50:26,938 - INFO - joeynmt.prediction - Generation took 3.4795[sec]. (No references given)
2022-09-15 14:50:26,948 - INFO - joeynmt.training - Processing Predictions on Batch 73/107
2022-09-15 14:50:30,672 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:50:30,672 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:50:31,009 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:50:31,263 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:50:31,377 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:50:31,378 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:50:31,418 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:50:35,068 - INFO - joeynmt.prediction - Generation took 3.6385[sec]. (No references given)
2022-09-15 14:50:35,077 - INFO - joeynmt.training - Processing Predictions on Batch 74/107
2022-09-15 14:50:38,822 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:50:38,822 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:50:39,158 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:50:39,413 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:50:39,525 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:50:39,525 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:50:39,563 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:50:44,239 - INFO - joeynmt.prediction - Generation took 4.6642[sec]. (No references given)
2022-09-15 14:50:44,248 - INFO - joeynmt.training - Processing Predictions on Batch 75/107
2022-09-15 14:50:48,058 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:50:48,058 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:50:48,394 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:50:48,650 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:50:48,762 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:50:48,762 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:50:48,800 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:50:52,498 - INFO - joeynmt.prediction - Generation took 3.6862[sec]. (No references given)
2022-09-15 14:50:52,507 - INFO - joeynmt.training - Processing Predictions on Batch 76/107
2022-09-15 14:50:56,252 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:50:56,252 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:50:56,586 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:50:56,838 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:50:56,949 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:50:56,949 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:50:56,986 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:51:00,963 - INFO - joeynmt.prediction - Generation took 3.9650[sec]. (No references given)
2022-09-15 14:51:00,973 - INFO - joeynmt.training - Processing Predictions on Batch 77/107
2022-09-15 14:51:04,726 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:51:04,726 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:51:05,063 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:51:05,316 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:51:05,429 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:51:05,429 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:51:05,465 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:51:09,403 - INFO - joeynmt.prediction - Generation took 3.9268[sec]. (No references given)
2022-09-15 14:51:09,413 - INFO - joeynmt.training - Processing Predictions on Batch 78/107
2022-09-15 14:51:13,191 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:51:13,191 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:51:13,529 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:51:13,785 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:51:13,898 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:51:13,898 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:51:13,936 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:51:18,101 - INFO - joeynmt.prediction - Generation took 4.1532[sec]. (No references given)
2022-09-15 14:51:18,110 - INFO - joeynmt.training - Processing Predictions on Batch 79/107
2022-09-15 14:51:21,882 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:51:21,882 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:51:22,218 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:51:22,472 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:51:22,584 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:51:22,586 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:51:22,621 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:51:26,593 - INFO - joeynmt.prediction - Generation took 3.9598[sec]. (No references given)
2022-09-15 14:51:26,603 - INFO - joeynmt.training - Processing Predictions on Batch 80/107
2022-09-15 14:51:30,361 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:51:30,361 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:51:30,697 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:51:30,953 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:51:31,067 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:51:31,067 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:51:31,104 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:51:35,089 - INFO - joeynmt.prediction - Generation took 3.9734[sec]. (No references given)
2022-09-15 14:51:35,100 - INFO - joeynmt.training - Processing Predictions on Batch 81/107
2022-09-15 14:51:38,894 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:51:38,894 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:51:39,229 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:51:39,481 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:51:39,593 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:51:39,593 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:51:39,631 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:51:43,667 - INFO - joeynmt.prediction - Generation took 4.0238[sec]. (No references given)
2022-09-15 14:51:43,676 - INFO - joeynmt.training - Processing Predictions on Batch 82/107
2022-09-15 14:51:47,468 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:51:47,468 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:51:47,805 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:51:48,058 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:51:48,170 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:51:48,170 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:51:48,211 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:51:51,881 - INFO - joeynmt.prediction - Generation took 3.6574[sec]. (No references given)
2022-09-15 14:51:51,891 - INFO - joeynmt.training - Processing Predictions on Batch 83/107
2022-09-15 14:51:55,638 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:51:55,638 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:51:55,974 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:51:56,227 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:51:56,339 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:51:56,339 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:51:56,380 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:52:00,205 - INFO - joeynmt.prediction - Generation took 3.8127[sec]. (No references given)
2022-09-15 14:52:00,215 - INFO - joeynmt.training - Processing Predictions on Batch 84/107
2022-09-15 14:52:04,010 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:52:04,010 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:52:04,345 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:52:04,599 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:52:04,787 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:52:04,788 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:52:04,827 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:52:09,181 - INFO - joeynmt.prediction - Generation took 4.3413[sec]. (No references given)
2022-09-15 14:52:09,190 - INFO - joeynmt.training - Processing Predictions on Batch 85/107
2022-09-15 14:52:12,961 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:52:12,961 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:52:13,298 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:52:13,553 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:52:13,665 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:52:13,666 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:52:13,701 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:52:16,991 - INFO - joeynmt.prediction - Generation took 3.2796[sec]. (No references given)
2022-09-15 14:52:17,001 - INFO - joeynmt.training - Processing Predictions on Batch 86/107
2022-09-15 14:52:20,759 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:52:20,759 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:52:21,094 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:52:21,346 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:52:21,457 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:52:21,457 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:52:21,494 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:52:25,060 - INFO - joeynmt.prediction - Generation took 3.5544[sec]. (No references given)
2022-09-15 14:52:25,070 - INFO - joeynmt.training - Processing Predictions on Batch 87/107
2022-09-15 14:52:28,876 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:52:28,876 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:52:29,210 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:52:29,536 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:52:29,648 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:52:29,648 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:52:29,684 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:52:33,473 - INFO - joeynmt.prediction - Generation took 3.7769[sec]. (No references given)
2022-09-15 14:52:33,482 - INFO - joeynmt.training - Processing Predictions on Batch 88/107
2022-09-15 14:52:37,298 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:52:37,298 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:52:37,636 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:52:37,890 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:52:38,005 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:52:38,005 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:52:38,039 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:52:41,547 - INFO - joeynmt.prediction - Generation took 3.4965[sec]. (No references given)
2022-09-15 14:52:41,556 - INFO - joeynmt.training - Processing Predictions on Batch 89/107
2022-09-15 14:52:45,378 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:52:45,378 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:52:45,713 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:52:45,967 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:52:46,079 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:52:46,080 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:52:46,116 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:52:49,920 - INFO - joeynmt.prediction - Generation took 3.7925[sec]. (No references given)
2022-09-15 14:52:49,931 - INFO - joeynmt.training - Processing Predictions on Batch 90/107
2022-09-15 14:52:53,750 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:52:53,750 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:52:54,085 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:52:54,413 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:52:54,525 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:52:54,525 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:52:54,564 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:52:58,455 - INFO - joeynmt.prediction - Generation took 3.8794[sec]. (No references given)
2022-09-15 14:52:58,466 - INFO - joeynmt.training - Processing Predictions on Batch 91/107
2022-09-15 14:53:02,293 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:53:02,293 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:53:02,630 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:53:02,884 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:53:02,997 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:53:02,997 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:53:03,031 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:53:06,634 - INFO - joeynmt.prediction - Generation took 3.5921[sec]. (No references given)
2022-09-15 14:53:06,643 - INFO - joeynmt.training - Processing Predictions on Batch 92/107
2022-09-15 14:53:10,461 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:53:10,461 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:53:10,795 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:53:11,049 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:53:11,162 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:53:11,162 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:53:11,197 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:53:15,036 - INFO - joeynmt.prediction - Generation took 3.8269[sec]. (No references given)
2022-09-15 14:53:15,046 - INFO - joeynmt.training - Processing Predictions on Batch 93/107
2022-09-15 14:53:18,868 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:53:18,868 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:53:19,202 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:53:19,532 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:53:19,644 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:53:19,644 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:53:19,688 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:53:23,912 - INFO - joeynmt.prediction - Generation took 4.2119[sec]. (No references given)
2022-09-15 14:53:23,922 - INFO - joeynmt.training - Processing Predictions on Batch 94/107
2022-09-15 14:53:27,764 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:53:27,764 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:53:28,100 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:53:28,352 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:53:28,464 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:53:28,464 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:53:28,505 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:53:32,446 - INFO - joeynmt.prediction - Generation took 3.9289[sec]. (No references given)
2022-09-15 14:53:32,456 - INFO - joeynmt.training - Processing Predictions on Batch 95/107
2022-09-15 14:53:36,298 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:53:36,298 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:53:36,634 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:53:36,884 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:53:36,996 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:53:36,996 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:53:37,034 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:53:40,924 - INFO - joeynmt.prediction - Generation took 3.8783[sec]. (No references given)
2022-09-15 14:53:40,933 - INFO - joeynmt.training - Processing Predictions on Batch 96/107
2022-09-15 14:53:44,768 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:53:44,768 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:53:45,102 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:53:45,354 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:53:45,542 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:53:45,543 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:53:45,575 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:53:49,338 - INFO - joeynmt.prediction - Generation took 3.7526[sec]. (No references given)
2022-09-15 14:53:49,348 - INFO - joeynmt.training - Processing Predictions on Batch 97/107
2022-09-15 14:53:53,106 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:53:53,106 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:53:53,439 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:53:53,691 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:53:53,803 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:53:53,803 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:53:53,840 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:53:57,953 - INFO - joeynmt.prediction - Generation took 4.1008[sec]. (No references given)
2022-09-15 14:53:57,962 - INFO - joeynmt.training - Processing Predictions on Batch 98/107
2022-09-15 14:54:01,720 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:54:01,720 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:54:02,056 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:54:02,307 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:54:02,419 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:54:02,419 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:54:02,456 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:54:06,579 - INFO - joeynmt.prediction - Generation took 4.1108[sec]. (No references given)
2022-09-15 14:54:06,589 - INFO - joeynmt.training - Processing Predictions on Batch 99/107
2022-09-15 14:54:10,377 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:54:10,377 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:54:10,711 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:54:10,964 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:54:11,075 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:54:11,076 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:54:11,110 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:54:14,637 - INFO - joeynmt.prediction - Generation took 3.5162[sec]. (No references given)
2022-09-15 14:54:14,646 - INFO - joeynmt.training - Processing Predictions on Batch 100/107
2022-09-15 14:54:18,399 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:54:18,399 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:54:18,735 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:54:18,993 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:54:19,107 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:54:19,107 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:54:19,142 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:54:22,744 - INFO - joeynmt.prediction - Generation took 3.5911[sec]. (No references given)
2022-09-15 14:54:22,754 - INFO - joeynmt.training - Processing Predictions on Batch 101/107
2022-09-15 14:54:26,515 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:54:26,515 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:54:26,851 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:54:27,106 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:54:27,218 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:54:27,219 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:54:27,257 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:54:31,421 - INFO - joeynmt.prediction - Generation took 4.1521[sec]. (No references given)
2022-09-15 14:54:31,430 - INFO - joeynmt.training - Processing Predictions on Batch 102/107
2022-09-15 14:54:35,223 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:54:35,224 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:54:35,558 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:54:35,808 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:54:35,920 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:54:35,920 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:54:35,959 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:54:39,718 - INFO - joeynmt.prediction - Generation took 3.7470[sec]. (No references given)
2022-09-15 14:54:39,727 - INFO - joeynmt.training - Processing Predictions on Batch 103/107
2022-09-15 14:54:43,520 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:54:43,521 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:54:43,933 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:54:44,188 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:54:44,300 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:54:44,301 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:54:44,340 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:54:47,948 - INFO - joeynmt.prediction - Generation took 3.5968[sec]. (No references given)
2022-09-15 14:54:47,958 - INFO - joeynmt.training - Processing Predictions on Batch 104/107
2022-09-15 14:54:51,748 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:54:51,748 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:54:52,082 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:54:52,339 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:54:52,456 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:54:52,457 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:54:52,495 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:54:56,483 - INFO - joeynmt.prediction - Generation took 3.9762[sec]. (No references given)
2022-09-15 14:54:56,493 - INFO - joeynmt.training - Processing Predictions on Batch 105/107
2022-09-15 14:55:00,314 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:55:00,314 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:55:00,650 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:55:00,903 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:55:01,016 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:55:01,016 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:55:01,053 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:55:05,028 - INFO - joeynmt.prediction - Generation took 3.9638[sec]. (No references given)
2022-09-15 14:55:05,037 - INFO - joeynmt.training - Processing Predictions on Batch 106/107
2022-09-15 14:55:08,858 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 14:55:08,858 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 14:55:09,196 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 14:55:09,528 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r3/203000.ckpt.
2022-09-15 14:55:09,640 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:55:09,640 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 14:55:09,673 - INFO - joeynmt.prediction - Predicting 205 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 14:55:12,773 - INFO - joeynmt.prediction - Generation took 3.0898[sec]. (No references given)
2022-09-15 14:55:12,913 - INFO - joeynmt.training - Final Query Indices picked: [130328, 205529, 102960, 7997, 111011, 196211, 233542, 43013, 187776, 342341] length: 10000
2022-09-15 14:55:12,913 - INFO - joeynmt.training - Query the samples, one at a time (interactive/file)
2022-09-15 14:55:16,083 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 4096
	effective batch size (w. parallel & accumulation): 4096
2022-09-15 14:55:16,083 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 15:05:08,076 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 15:05:08,077 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.70, loss:   2.64, ppl:  14.00, acc:   0.50, generation: 581.9628[sec], evaluation: 9.5263[sec]
2022-09-15 15:05:08,083 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 15:05:08,539 - INFO - joeynmt.training - Example #0
2022-09-15 15:05:08,539 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 15:05:08,539 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 15:05:08,539 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'ढ@@', 'का', 'हुआ', 'है', '</s>']
2022-09-15 15:05:08,556 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 15:05:08,556 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 15:05:08,557 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को ढका हुआ है
2022-09-15 15:05:08,557 - INFO - joeynmt.training - Example #1
2022-09-15 15:05:08,557 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 15:05:08,557 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 15:05:08,557 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 15:05:08,572 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 15:05:08,572 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 15:05:08,572 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 15:05:08,572 - INFO - joeynmt.training - Example #2
2022-09-15 15:05:08,572 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 15:05:08,572 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 15:05:08,572 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['कार्य', ',', 'गति', 'और', 'इ@@', 'म', '</s>']
2022-09-15 15:05:08,587 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 15:05:08,587 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 15:05:08,587 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और इम
2022-09-15 15:05:08,587 - INFO - joeynmt.training - Example #3
2022-09-15 15:05:08,587 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 15:05:08,587 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 15:05:08,588 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 15:05:08,602 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 15:05:08,602 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 15:05:08,602 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 15:05:08,661 - INFO - joeynmt.training - EPOCH 1
2022-09-15 15:05:21,551 - INFO - joeynmt.training - Epoch   1, Step:   203100, Batch Loss:     2.866307, Batch Acc: 0.004379, Tokens per Sec:     8840, Lr: 0.000035
2022-09-15 15:05:34,169 - INFO - joeynmt.training - Epoch   1, Step:   203200, Batch Loss:     2.698168, Batch Acc: 0.003707, Tokens per Sec:     9215, Lr: 0.000035
2022-09-15 15:05:46,554 - INFO - joeynmt.training - Epoch   1, Step:   203300, Batch Loss:     2.769541, Batch Acc: 0.004864, Tokens per Sec:     9297, Lr: 0.000035
2022-09-15 15:05:59,195 - INFO - joeynmt.training - Epoch   1, Step:   203400, Batch Loss:     2.601391, Batch Acc: 0.004342, Tokens per Sec:     9147, Lr: 0.000035
2022-09-15 15:06:11,639 - INFO - joeynmt.training - Epoch   1, Step:   203500, Batch Loss:     2.665431, Batch Acc: 0.004693, Tokens per Sec:     9161, Lr: 0.000035
2022-09-15 15:06:23,956 - INFO - joeynmt.training - Epoch   1, Step:   203600, Batch Loss:     2.787809, Batch Acc: 0.004409, Tokens per Sec:     9243, Lr: 0.000035
2022-09-15 15:06:36,280 - INFO - joeynmt.training - Epoch   1, Step:   203700, Batch Loss:     2.844117, Batch Acc: 0.003823, Tokens per Sec:     9192, Lr: 0.000035
2022-09-15 15:06:48,904 - INFO - joeynmt.training - Epoch   1, Step:   203800, Batch Loss:     2.670537, Batch Acc: 0.003726, Tokens per Sec:     9078, Lr: 0.000035
2022-09-15 15:07:01,186 - INFO - joeynmt.training - Epoch   1, Step:   203900, Batch Loss:     2.671949, Batch Acc: 0.005431, Tokens per Sec:     9264, Lr: 0.000035
2022-09-15 15:07:13,434 - INFO - joeynmt.training - Epoch   1, Step:   204000, Batch Loss:     2.755700, Batch Acc: 0.004546, Tokens per Sec:     9412, Lr: 0.000035
2022-09-15 15:07:13,435 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 15:16:51,991 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 15:16:51,992 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.71, loss:   2.63, ppl:  13.90, acc:   0.50, generation: 568.5179[sec], evaluation: 9.5258[sec]
2022-09-15 15:16:51,998 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 15:16:52,462 - INFO - joeynmt.training - Example #0
2022-09-15 15:16:52,463 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 15:16:52,463 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 15:16:52,463 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 15:16:52,480 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 15:16:52,481 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 15:16:52,481 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 15:16:52,481 - INFO - joeynmt.training - Example #1
2022-09-15 15:16:52,481 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 15:16:52,481 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 15:16:52,481 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 15:16:52,496 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 15:16:52,496 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 15:16:52,496 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 15:16:52,496 - INFO - joeynmt.training - Example #2
2022-09-15 15:16:52,496 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 15:16:52,497 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 15:16:52,497 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['कार्य', ',', 'गति', 'और', 'इ@@', 'म', '</s>']
2022-09-15 15:16:52,512 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 15:16:52,512 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 15:16:52,512 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और इम
2022-09-15 15:16:52,512 - INFO - joeynmt.training - Example #3
2022-09-15 15:16:52,512 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 15:16:52,512 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 15:16:52,512 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 15:16:52,527 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 15:16:52,527 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 15:16:52,528 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 15:17:04,832 - INFO - joeynmt.training - Epoch   1, Step:   204100, Batch Loss:     2.796040, Batch Acc: 0.005623, Tokens per Sec:     9006, Lr: 0.000035
2022-09-15 15:17:16,954 - INFO - joeynmt.training - Epoch   1, Step:   204200, Batch Loss:     2.786192, Batch Acc: 0.004720, Tokens per Sec:     9351, Lr: 0.000035
2022-09-15 15:17:29,077 - INFO - joeynmt.training - Epoch   1, Step:   204300, Batch Loss:     2.699837, Batch Acc: 0.004855, Tokens per Sec:     9344, Lr: 0.000035
2022-09-15 15:17:41,209 - INFO - joeynmt.training - Epoch   1, Step:   204400, Batch Loss:     2.583114, Batch Acc: 0.003966, Tokens per Sec:     9415, Lr: 0.000035
2022-09-15 15:17:53,318 - INFO - joeynmt.training - Epoch   1, Step:   204500, Batch Loss:     2.611945, Batch Acc: 0.004997, Tokens per Sec:     9454, Lr: 0.000035
2022-09-15 15:18:05,472 - INFO - joeynmt.training - Epoch   1, Step:   204600, Batch Loss:     2.579831, Batch Acc: 0.005173, Tokens per Sec:     9320, Lr: 0.000035
2022-09-15 15:18:17,454 - INFO - joeynmt.training - Epoch   1, Step:   204700, Batch Loss:     2.567692, Batch Acc: 0.004726, Tokens per Sec:     9695, Lr: 0.000035
2022-09-15 15:18:29,398 - INFO - joeynmt.training - Epoch   1, Step:   204800, Batch Loss:     2.940532, Batch Acc: 0.005085, Tokens per Sec:     9764, Lr: 0.000035
2022-09-15 15:18:41,352 - INFO - joeynmt.training - Epoch   1, Step:   204900, Batch Loss:     2.945491, Batch Acc: 0.003994, Tokens per Sec:     9676, Lr: 0.000035
2022-09-15 15:18:53,292 - INFO - joeynmt.training - Epoch   1, Step:   205000, Batch Loss:     2.847423, Batch Acc: 0.003978, Tokens per Sec:     9707, Lr: 0.000035
2022-09-15 15:18:53,292 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 15:28:33,960 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 15:28:33,961 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.76, loss:   2.63, ppl:  13.86, acc:   0.50, generation: 570.6463[sec], evaluation: 9.5119[sec]
2022-09-15 15:28:33,967 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 15:28:34,425 - INFO - joeynmt.training - Example #0
2022-09-15 15:28:34,425 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 15:28:34,425 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 15:28:34,425 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-15 15:28:34,442 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 15:28:34,442 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 15:28:34,442 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 15:28:34,442 - INFO - joeynmt.training - Example #1
2022-09-15 15:28:34,443 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 15:28:34,443 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 15:28:34,443 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 15:28:34,458 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 15:28:34,458 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 15:28:34,458 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 15:28:34,458 - INFO - joeynmt.training - Example #2
2022-09-15 15:28:34,458 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 15:28:34,458 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 15:28:34,458 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['कार्य', ',', 'गति', 'और', 'इ@@', 'म', '</s>']
2022-09-15 15:28:34,473 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 15:28:34,473 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 15:28:34,473 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और इम
2022-09-15 15:28:34,473 - INFO - joeynmt.training - Example #3
2022-09-15 15:28:34,473 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 15:28:34,473 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 15:28:34,473 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 15:28:34,488 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 15:28:34,488 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 15:28:34,488 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 15:28:46,765 - INFO - joeynmt.training - Epoch   1, Step:   205100, Batch Loss:     2.679572, Batch Acc: 0.005270, Tokens per Sec:     8941, Lr: 0.000035
2022-09-15 15:28:59,040 - INFO - joeynmt.training - Epoch   1, Step:   205200, Batch Loss:     2.762292, Batch Acc: 0.003980, Tokens per Sec:     9396, Lr: 0.000035
2022-09-15 15:29:11,179 - INFO - joeynmt.training - Epoch   1, Step:   205300, Batch Loss:     2.825707, Batch Acc: 0.004949, Tokens per Sec:     9390, Lr: 0.000035
2022-09-15 15:29:23,299 - INFO - joeynmt.training - Epoch   1, Step:   205400, Batch Loss:     3.078331, Batch Acc: 0.004251, Tokens per Sec:     9473, Lr: 0.000035
2022-09-15 15:29:35,484 - INFO - joeynmt.training - Epoch   1, Step:   205500, Batch Loss:     2.869574, Batch Acc: 0.003972, Tokens per Sec:     9463, Lr: 0.000035
2022-09-15 15:29:47,937 - INFO - joeynmt.training - Epoch   1, Step:   205600, Batch Loss:     2.726393, Batch Acc: 0.003874, Tokens per Sec:     9141, Lr: 0.000035
2022-09-15 15:30:00,211 - INFO - joeynmt.training - Epoch   1, Step:   205700, Batch Loss:     2.860888, Batch Acc: 0.003971, Tokens per Sec:     9252, Lr: 0.000035
2022-09-15 15:30:12,418 - INFO - joeynmt.training - Epoch   1, Step:   205800, Batch Loss:     2.889968, Batch Acc: 0.003914, Tokens per Sec:     9356, Lr: 0.000035
2022-09-15 15:30:24,515 - INFO - joeynmt.training - Epoch   1, Step:   205900, Batch Loss:     2.742270, Batch Acc: 0.004286, Tokens per Sec:     9316, Lr: 0.000035
2022-09-15 15:30:36,563 - INFO - joeynmt.training - Epoch   1, Step:   206000, Batch Loss:     2.663668, Batch Acc: 0.004009, Tokens per Sec:     9421, Lr: 0.000035
2022-09-15 15:30:36,564 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 15:39:58,585 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 15:39:58,586 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  20.82, loss:   2.63, ppl:  13.85, acc:   0.50, generation: 551.8977[sec], evaluation: 9.3059[sec]
2022-09-15 15:39:58,592 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 15:39:59,038 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/203000.ckpt
2022-09-15 15:39:59,065 - INFO - joeynmt.training - Example #0
2022-09-15 15:39:59,066 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 15:39:59,066 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 15:39:59,066 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 15:39:59,084 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 15:39:59,084 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 15:39:59,084 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 15:39:59,084 - INFO - joeynmt.training - Example #1
2022-09-15 15:39:59,084 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 15:39:59,084 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 15:39:59,084 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 15:39:59,099 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 15:39:59,099 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 15:39:59,099 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 15:39:59,099 - INFO - joeynmt.training - Example #2
2022-09-15 15:39:59,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 15:39:59,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 15:39:59,100 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 15:39:59,115 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 15:39:59,115 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 15:39:59,115 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 15:39:59,115 - INFO - joeynmt.training - Example #3
2022-09-15 15:39:59,115 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 15:39:59,115 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 15:39:59,115 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 15:39:59,130 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 15:39:59,131 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 15:39:59,131 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 15:40:11,427 - INFO - joeynmt.training - Epoch   1, Step:   206100, Batch Loss:     2.728032, Batch Acc: 0.004242, Tokens per Sec:     8963, Lr: 0.000035
2022-09-15 15:40:23,675 - INFO - joeynmt.training - Epoch   1, Step:   206200, Batch Loss:     2.654752, Batch Acc: 0.004310, Tokens per Sec:     9340, Lr: 0.000035
2022-09-15 15:40:35,892 - INFO - joeynmt.training - Epoch   1, Step:   206300, Batch Loss:     2.758199, Batch Acc: 0.004722, Tokens per Sec:     9552, Lr: 0.000035
2022-09-15 15:40:48,239 - INFO - joeynmt.training - Epoch   1, Step:   206400, Batch Loss:     2.826833, Batch Acc: 0.003916, Tokens per Sec:     9205, Lr: 0.000035
2022-09-15 15:41:00,687 - INFO - joeynmt.training - Epoch   1, Step:   206500, Batch Loss:     2.751632, Batch Acc: 0.004216, Tokens per Sec:     9070, Lr: 0.000035
2022-09-15 15:41:12,942 - INFO - joeynmt.training - Epoch   1, Step:   206600, Batch Loss:     3.116868, Batch Acc: 0.003596, Tokens per Sec:     9304, Lr: 0.000035
2022-09-15 15:41:25,051 - INFO - joeynmt.training - Epoch   1, Step:   206700, Batch Loss:     2.814550, Batch Acc: 0.003594, Tokens per Sec:     9376, Lr: 0.000035
2022-09-15 15:41:37,268 - INFO - joeynmt.training - Epoch   1, Step:   206800, Batch Loss:     2.899241, Batch Acc: 0.004328, Tokens per Sec:     9457, Lr: 0.000035
2022-09-15 15:41:49,681 - INFO - joeynmt.training - Epoch   1, Step:   206900, Batch Loss:     2.625451, Batch Acc: 0.003700, Tokens per Sec:     9168, Lr: 0.000035
2022-09-15 15:42:01,874 - INFO - joeynmt.training - Epoch   1, Step:   207000, Batch Loss:     2.833424, Batch Acc: 0.004165, Tokens per Sec:     9551, Lr: 0.000035
2022-09-15 15:42:01,875 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 15:51:30,160 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 15:51:30,167 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 15:51:30,602 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/204000.ckpt
2022-09-15 15:51:30,630 - INFO - joeynmt.training - Example #0
2022-09-15 15:51:30,630 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 15:51:30,630 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 15:51:30,630 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 15:51:30,647 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 15:51:30,648 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 15:51:30,648 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 15:51:30,648 - INFO - joeynmt.training - Example #1
2022-09-15 15:51:30,648 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 15:51:30,648 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 15:51:30,648 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 15:51:30,663 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 15:51:30,663 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 15:51:30,663 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 15:51:30,663 - INFO - joeynmt.training - Example #2
2022-09-15 15:51:30,663 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 15:51:30,663 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 15:51:30,663 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 15:51:30,678 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 15:51:30,678 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 15:51:30,678 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 15:51:30,678 - INFO - joeynmt.training - Example #3
2022-09-15 15:51:30,679 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 15:51:30,679 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 15:51:30,679 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 15:51:30,694 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 15:51:30,694 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 15:51:30,694 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 15:51:42,904 - INFO - joeynmt.training - Epoch   1, Step:   207100, Batch Loss:     3.024484, Batch Acc: 0.004586, Tokens per Sec:     8920, Lr: 0.000035
2022-09-15 15:51:55,451 - INFO - joeynmt.training - Epoch   1, Step:   207200, Batch Loss:     2.865843, Batch Acc: 0.003653, Tokens per Sec:     9230, Lr: 0.000035
2022-09-15 15:52:07,725 - INFO - joeynmt.training - Epoch   1, Step:   207300, Batch Loss:     2.737245, Batch Acc: 0.004055, Tokens per Sec:     9323, Lr: 0.000035
2022-09-15 15:52:19,920 - INFO - joeynmt.training - Epoch   1, Step:   207400, Batch Loss:     2.613576, Batch Acc: 0.005028, Tokens per Sec:     9329, Lr: 0.000035
2022-09-15 15:52:32,107 - INFO - joeynmt.training - Epoch   1, Step:   207500, Batch Loss:     2.694733, Batch Acc: 0.004307, Tokens per Sec:     9316, Lr: 0.000035
2022-09-15 15:52:44,286 - INFO - joeynmt.training - Epoch   1, Step:   207600, Batch Loss:     2.766863, Batch Acc: 0.004327, Tokens per Sec:     9430, Lr: 0.000035
2022-09-15 15:52:56,501 - INFO - joeynmt.training - Epoch   1, Step:   207700, Batch Loss:     2.784148, Batch Acc: 0.003880, Tokens per Sec:     9454, Lr: 0.000035
2022-09-15 15:53:08,759 - INFO - joeynmt.training - Epoch   1, Step:   207800, Batch Loss:     2.651880, Batch Acc: 0.004901, Tokens per Sec:     9437, Lr: 0.000035
2022-09-15 15:53:20,969 - INFO - joeynmt.training - Epoch   1, Step:   207900, Batch Loss:     2.670097, Batch Acc: 0.003651, Tokens per Sec:     9444, Lr: 0.000035
2022-09-15 15:53:33,192 - INFO - joeynmt.training - Epoch   1, Step:   208000, Batch Loss:     2.451315, Batch Acc: 0.004262, Tokens per Sec:     9502, Lr: 0.000035
2022-09-15 15:53:33,193 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:03:08,090 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 16:03:08,091 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.01, loss:   2.62, ppl:  13.67, acc:   0.50, generation: 564.5087[sec], evaluation: 9.8811[sec]
2022-09-15 16:03:08,097 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 16:03:08,538 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/205000.ckpt
2022-09-15 16:03:08,566 - INFO - joeynmt.training - Example #0
2022-09-15 16:03:08,566 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 16:03:08,566 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 16:03:08,566 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 16:03:08,584 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 16:03:08,584 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 16:03:08,584 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 16:03:08,584 - INFO - joeynmt.training - Example #1
2022-09-15 16:03:08,585 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 16:03:08,585 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 16:03:08,585 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 16:03:08,600 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 16:03:08,600 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 16:03:08,600 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 16:03:08,600 - INFO - joeynmt.training - Example #2
2022-09-15 16:03:08,600 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 16:03:08,600 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 16:03:08,601 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 16:03:08,616 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 16:03:08,616 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 16:03:08,616 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 16:03:08,616 - INFO - joeynmt.training - Example #3
2022-09-15 16:03:08,617 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 16:03:08,617 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 16:03:08,617 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 16:03:08,632 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 16:03:08,632 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 16:03:08,632 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 16:03:21,195 - INFO - joeynmt.training - Epoch   1, Step:   208100, Batch Loss:     2.565687, Batch Acc: 0.004383, Tokens per Sec:     8745, Lr: 0.000035
2022-09-15 16:03:33,255 - INFO - joeynmt.training - Epoch   1, Step:   208200, Batch Loss:     2.765008, Batch Acc: 0.005124, Tokens per Sec:     9500, Lr: 0.000035
2022-09-15 16:03:45,264 - INFO - joeynmt.training - Epoch   1, Step:   208300, Batch Loss:     2.784224, Batch Acc: 0.004296, Tokens per Sec:     9557, Lr: 0.000035
2022-09-15 16:03:57,189 - INFO - joeynmt.training - Epoch   1, Step:   208400, Batch Loss:     2.713654, Batch Acc: 0.003662, Tokens per Sec:     9526, Lr: 0.000035
2022-09-15 16:04:09,128 - INFO - joeynmt.training - Epoch   1, Step:   208500, Batch Loss:     2.712359, Batch Acc: 0.004387, Tokens per Sec:     9413, Lr: 0.000035
2022-09-15 16:04:21,036 - INFO - joeynmt.training - Epoch   1, Step:   208600, Batch Loss:     2.756788, Batch Acc: 0.004241, Tokens per Sec:     9465, Lr: 0.000035
2022-09-15 16:04:33,040 - INFO - joeynmt.training - Epoch   1, Step:   208700, Batch Loss:     2.651514, Batch Acc: 0.004419, Tokens per Sec:     9670, Lr: 0.000035
2022-09-15 16:04:45,021 - INFO - joeynmt.training - Epoch   1, Step:   208800, Batch Loss:     2.513603, Batch Acc: 0.003297, Tokens per Sec:     9543, Lr: 0.000035
2022-09-15 16:04:57,234 - INFO - joeynmt.training - Epoch   1, Step:   208900, Batch Loss:     2.625696, Batch Acc: 0.004333, Tokens per Sec:     9449, Lr: 0.000035
2022-09-15 16:05:09,460 - INFO - joeynmt.training - Epoch   1, Step:   209000, Batch Loss:     2.549869, Batch Acc: 0.004921, Tokens per Sec:     9359, Lr: 0.000035
2022-09-15 16:05:09,460 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:14:38,406 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 16:14:38,407 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.04, loss:   2.61, ppl:  13.60, acc:   0.50, generation: 559.0131[sec], evaluation: 9.4136[sec]
2022-09-15 16:14:38,413 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 16:14:38,870 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/206000.ckpt
2022-09-15 16:14:38,898 - INFO - joeynmt.training - Example #0
2022-09-15 16:14:38,898 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 16:14:38,898 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 16:14:38,898 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 16:14:38,915 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 16:14:38,916 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 16:14:38,916 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 16:14:38,916 - INFO - joeynmt.training - Example #1
2022-09-15 16:14:38,916 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 16:14:38,916 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 16:14:38,916 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 16:14:38,931 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 16:14:38,931 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 16:14:38,931 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 16:14:38,931 - INFO - joeynmt.training - Example #2
2022-09-15 16:14:38,931 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 16:14:38,931 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 16:14:38,931 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 16:14:38,946 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 16:14:38,946 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 16:14:38,946 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 16:14:38,947 - INFO - joeynmt.training - Example #3
2022-09-15 16:14:38,947 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 16:14:38,947 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 16:14:38,947 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 16:14:38,962 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 16:14:38,962 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 16:14:38,962 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 16:14:51,326 - INFO - joeynmt.training - Epoch   1, Step:   209100, Batch Loss:     2.785883, Batch Acc: 0.004698, Tokens per Sec:     8951, Lr: 0.000035
2022-09-15 16:15:03,591 - INFO - joeynmt.training - Epoch   1, Step:   209200, Batch Loss:     2.875213, Batch Acc: 0.003456, Tokens per Sec:     9388, Lr: 0.000035
2022-09-15 16:15:15,661 - INFO - joeynmt.training - Epoch   1, Step:   209300, Batch Loss:     3.016591, Batch Acc: 0.003628, Tokens per Sec:     9773, Lr: 0.000035
2022-09-15 16:15:27,711 - INFO - joeynmt.training - Epoch   1, Step:   209400, Batch Loss:     2.744545, Batch Acc: 0.004506, Tokens per Sec:     9448, Lr: 0.000035
2022-09-15 16:15:40,060 - INFO - joeynmt.training - Epoch   1, Step:   209500, Batch Loss:     2.522764, Batch Acc: 0.003944, Tokens per Sec:     9260, Lr: 0.000035
2022-09-15 16:15:52,068 - INFO - joeynmt.training - Epoch   1, Step:   209600, Batch Loss:     2.614458, Batch Acc: 0.004102, Tokens per Sec:     9563, Lr: 0.000035
2022-09-15 16:16:04,122 - INFO - joeynmt.training - Epoch   1, Step:   209700, Batch Loss:     2.804151, Batch Acc: 0.004628, Tokens per Sec:     9573, Lr: 0.000035
2022-09-15 16:16:16,159 - INFO - joeynmt.training - Epoch   1, Step:   209800, Batch Loss:     2.526762, Batch Acc: 0.005205, Tokens per Sec:     9513, Lr: 0.000035
2022-09-15 16:16:28,123 - INFO - joeynmt.training - Epoch   1, Step:   209900, Batch Loss:     2.767867, Batch Acc: 0.003722, Tokens per Sec:     9590, Lr: 0.000035
2022-09-15 16:16:40,074 - INFO - joeynmt.training - Epoch   1, Step:   210000, Batch Loss:     2.914523, Batch Acc: 0.004238, Tokens per Sec:     9595, Lr: 0.000035
2022-09-15 16:16:40,075 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:26:07,815 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 16:26:07,817 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.08, loss:   2.60, ppl:  13.51, acc:   0.50, generation: 557.8094[sec], evaluation: 9.4193[sec]
2022-09-15 16:26:07,823 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 16:26:08,272 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/207000.ckpt
2022-09-15 16:26:08,300 - INFO - joeynmt.training - Example #0
2022-09-15 16:26:08,301 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 16:26:08,301 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 16:26:08,301 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'ढ@@', 'का', 'हुआ', 'है', '</s>']
2022-09-15 16:26:08,318 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 16:26:08,318 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 16:26:08,318 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को ढका हुआ है
2022-09-15 16:26:08,318 - INFO - joeynmt.training - Example #1
2022-09-15 16:26:08,319 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 16:26:08,319 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 16:26:08,319 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 16:26:08,333 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 16:26:08,333 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 16:26:08,334 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 16:26:08,334 - INFO - joeynmt.training - Example #2
2022-09-15 16:26:08,334 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 16:26:08,334 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 16:26:08,334 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 16:26:08,349 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 16:26:08,349 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 16:26:08,349 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 16:26:08,349 - INFO - joeynmt.training - Example #3
2022-09-15 16:26:08,349 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 16:26:08,349 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 16:26:08,349 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 16:26:08,364 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 16:26:08,364 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 16:26:08,364 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 16:26:20,528 - INFO - joeynmt.training - Epoch   1, Step:   210100, Batch Loss:     2.420882, Batch Acc: 0.004369, Tokens per Sec:     8972, Lr: 0.000035
2022-09-15 16:26:32,619 - INFO - joeynmt.training - Epoch   1, Step:   210200, Batch Loss:     2.631238, Batch Acc: 0.004400, Tokens per Sec:     9512, Lr: 0.000035
2022-09-15 16:26:44,717 - INFO - joeynmt.training - Epoch   1, Step:   210300, Batch Loss:     2.737340, Batch Acc: 0.004142, Tokens per Sec:     9559, Lr: 0.000035
2022-09-15 16:26:56,681 - INFO - joeynmt.training - Epoch   1, Step:   210400, Batch Loss:     3.015942, Batch Acc: 0.004045, Tokens per Sec:     9671, Lr: 0.000035
2022-09-15 16:27:08,610 - INFO - joeynmt.training - Epoch   1, Step:   210500, Batch Loss:     2.675854, Batch Acc: 0.004665, Tokens per Sec:     9580, Lr: 0.000035
2022-09-15 16:27:20,521 - INFO - joeynmt.training - Epoch   1, Step:   210600, Batch Loss:     2.797758, Batch Acc: 0.004365, Tokens per Sec:     9618, Lr: 0.000035
2022-09-15 16:27:32,411 - INFO - joeynmt.training - Epoch   1, Step:   210700, Batch Loss:     2.763613, Batch Acc: 0.004187, Tokens per Sec:     9462, Lr: 0.000035
2022-09-15 16:27:44,330 - INFO - joeynmt.training - Epoch   1, Step:   210800, Batch Loss:     2.790813, Batch Acc: 0.003857, Tokens per Sec:     9768, Lr: 0.000035
2022-09-15 16:27:56,301 - INFO - joeynmt.training - Epoch   1, Step:   210900, Batch Loss:     2.962751, Batch Acc: 0.004264, Tokens per Sec:     9598, Lr: 0.000035
2022-09-15 16:28:08,242 - INFO - joeynmt.training - Epoch   1, Step:   211000, Batch Loss:     2.805703, Batch Acc: 0.004277, Tokens per Sec:     9497, Lr: 0.000035
2022-09-15 16:28:08,242 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:37:49,284 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 16:37:49,285 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.15, loss:   2.60, ppl:  13.41, acc:   0.50, generation: 570.6605[sec], evaluation: 9.8687[sec]
2022-09-15 16:37:49,291 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 16:37:49,726 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/208000.ckpt
2022-09-15 16:37:49,754 - INFO - joeynmt.training - Example #0
2022-09-15 16:37:49,754 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 16:37:49,754 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 16:37:49,754 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-15 16:37:49,771 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 16:37:49,771 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 16:37:49,772 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 16:37:49,772 - INFO - joeynmt.training - Example #1
2022-09-15 16:37:49,772 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 16:37:49,772 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 16:37:49,772 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 16:37:49,787 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 16:37:49,787 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 16:37:49,787 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 16:37:49,787 - INFO - joeynmt.training - Example #2
2022-09-15 16:37:49,787 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 16:37:49,787 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 16:37:49,787 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 16:37:49,802 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 16:37:49,803 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 16:37:49,803 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 16:37:49,803 - INFO - joeynmt.training - Example #3
2022-09-15 16:37:49,803 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 16:37:49,803 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 16:37:49,803 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 16:37:49,818 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 16:37:49,818 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 16:37:49,818 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 16:38:01,919 - INFO - joeynmt.training - Epoch   1, Step:   211100, Batch Loss:     2.765383, Batch Acc: 0.004218, Tokens per Sec:     9029, Lr: 0.000035
2022-09-15 16:38:13,911 - INFO - joeynmt.training - Epoch   1, Step:   211200, Batch Loss:     2.914928, Batch Acc: 0.004322, Tokens per Sec:     9725, Lr: 0.000035
2022-09-15 16:38:25,824 - INFO - joeynmt.training - Epoch   1, Step:   211300, Batch Loss:     2.599240, Batch Acc: 0.003661, Tokens per Sec:     9562, Lr: 0.000035
2022-09-15 16:38:37,689 - INFO - joeynmt.training - Epoch   1, Step:   211400, Batch Loss:     2.770027, Batch Acc: 0.004562, Tokens per Sec:     9570, Lr: 0.000035
2022-09-15 16:38:49,827 - INFO - joeynmt.training - Epoch   1, Step:   211500, Batch Loss:     2.890354, Batch Acc: 0.004072, Tokens per Sec:     9469, Lr: 0.000035
2022-09-15 16:39:01,716 - INFO - joeynmt.training - Epoch   1, Step:   211600, Batch Loss:     2.687026, Batch Acc: 0.003899, Tokens per Sec:     9794, Lr: 0.000035
2022-09-15 16:39:13,572 - INFO - joeynmt.training - Epoch   1, Step:   211700, Batch Loss:     2.451163, Batch Acc: 0.005541, Tokens per Sec:     9636, Lr: 0.000035
2022-09-15 16:39:25,455 - INFO - joeynmt.training - Epoch   1, Step:   211800, Batch Loss:     2.736672, Batch Acc: 0.003724, Tokens per Sec:     9718, Lr: 0.000035
2022-09-15 16:39:37,308 - INFO - joeynmt.training - Epoch   1, Step:   211900, Batch Loss:     2.826593, Batch Acc: 0.004876, Tokens per Sec:     9585, Lr: 0.000035
2022-09-15 16:39:49,191 - INFO - joeynmt.training - Epoch   1, Step:   212000, Batch Loss:     2.709371, Batch Acc: 0.005198, Tokens per Sec:     9796, Lr: 0.000035
2022-09-15 16:39:49,191 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 16:49:21,849 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 16:49:21,851 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.15, loss:   2.60, ppl:  13.45, acc:   0.50, generation: 562.8148[sec], evaluation: 9.3399[sec]
2022-09-15 16:49:22,295 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/209000.ckpt
2022-09-15 16:49:22,323 - INFO - joeynmt.training - Example #0
2022-09-15 16:49:22,323 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 16:49:22,323 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 16:49:22,324 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 16:49:22,341 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 16:49:22,341 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 16:49:22,341 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 16:49:22,341 - INFO - joeynmt.training - Example #1
2022-09-15 16:49:22,341 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 16:49:22,341 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 16:49:22,341 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 16:49:22,356 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 16:49:22,356 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 16:49:22,356 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 16:49:22,356 - INFO - joeynmt.training - Example #2
2022-09-15 16:49:22,356 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 16:49:22,356 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 16:49:22,356 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 16:49:22,371 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 16:49:22,371 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 16:49:22,371 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 16:49:22,371 - INFO - joeynmt.training - Example #3
2022-09-15 16:49:22,371 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 16:49:22,371 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 16:49:22,372 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 16:49:22,386 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 16:49:22,386 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 16:49:22,387 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 16:49:34,355 - INFO - joeynmt.training - Epoch   1, Step:   212100, Batch Loss:     2.778485, Batch Acc: 0.004657, Tokens per Sec:     9276, Lr: 0.000035
2022-09-15 16:49:46,284 - INFO - joeynmt.training - Epoch   1, Step:   212200, Batch Loss:     2.722610, Batch Acc: 0.003541, Tokens per Sec:     9519, Lr: 0.000035
2022-09-15 16:49:58,243 - INFO - joeynmt.training - Epoch   1, Step:   212300, Batch Loss:     2.879040, Batch Acc: 0.004184, Tokens per Sec:     9512, Lr: 0.000035
2022-09-15 16:50:10,187 - INFO - joeynmt.training - Epoch   1, Step:   212400, Batch Loss:     2.903107, Batch Acc: 0.004458, Tokens per Sec:     9653, Lr: 0.000035
2022-09-15 16:50:22,123 - INFO - joeynmt.training - Epoch   1, Step:   212500, Batch Loss:     2.664098, Batch Acc: 0.004724, Tokens per Sec:     9684, Lr: 0.000035
2022-09-15 16:50:34,025 - INFO - joeynmt.training - Epoch   1, Step:   212600, Batch Loss:     2.777127, Batch Acc: 0.004780, Tokens per Sec:     9616, Lr: 0.000035
2022-09-15 16:50:45,992 - INFO - joeynmt.training - Epoch   1, Step:   212700, Batch Loss:     2.522516, Batch Acc: 0.004948, Tokens per Sec:     9660, Lr: 0.000035
2022-09-15 16:50:57,907 - INFO - joeynmt.training - Epoch   1, Step:   212800, Batch Loss:     2.623128, Batch Acc: 0.005369, Tokens per Sec:     9521, Lr: 0.000035
2022-09-15 16:51:09,965 - INFO - joeynmt.training - Epoch   1, Step:   212900, Batch Loss:     2.716574, Batch Acc: 0.004362, Tokens per Sec:     9431, Lr: 0.000035
2022-09-15 16:51:22,117 - INFO - joeynmt.training - Epoch   1, Step:   213000, Batch Loss:     2.794270, Batch Acc: 0.005054, Tokens per Sec:     9445, Lr: 0.000035
2022-09-15 16:51:22,118 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 17:00:40,587 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 17:00:40,588 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.28, loss:   2.59, ppl:  13.39, acc:   0.50, generation: 548.1229[sec], evaluation: 9.4295[sec]
2022-09-15 17:00:40,594 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 17:00:41,031 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/210000.ckpt
2022-09-15 17:00:41,059 - INFO - joeynmt.training - Example #0
2022-09-15 17:00:41,059 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 17:00:41,059 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 17:00:41,060 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 17:00:41,077 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 17:00:41,077 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 17:00:41,077 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 17:00:41,077 - INFO - joeynmt.training - Example #1
2022-09-15 17:00:41,077 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 17:00:41,077 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 17:00:41,077 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 17:00:41,092 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 17:00:41,092 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 17:00:41,092 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 17:00:41,093 - INFO - joeynmt.training - Example #2
2022-09-15 17:00:41,093 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 17:00:41,093 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 17:00:41,093 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 17:00:41,108 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 17:00:41,108 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 17:00:41,108 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 17:00:41,108 - INFO - joeynmt.training - Example #3
2022-09-15 17:00:41,108 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 17:00:41,108 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 17:00:41,108 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 17:00:41,123 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 17:00:41,123 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 17:00:41,123 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 17:00:53,101 - INFO - joeynmt.training - Epoch   1, Step:   213100, Batch Loss:     2.552793, Batch Acc: 0.004648, Tokens per Sec:     9152, Lr: 0.000035
2022-09-15 17:01:04,989 - INFO - joeynmt.training - Epoch   1, Step:   213200, Batch Loss:     2.612190, Batch Acc: 0.004354, Tokens per Sec:     9661, Lr: 0.000035
2022-09-15 17:01:16,885 - INFO - joeynmt.training - Epoch   1, Step:   213300, Batch Loss:     2.664739, Batch Acc: 0.005091, Tokens per Sec:     9710, Lr: 0.000035
2022-09-15 17:01:28,787 - INFO - joeynmt.training - Epoch   1, Step:   213400, Batch Loss:     2.386747, Batch Acc: 0.004476, Tokens per Sec:     9479, Lr: 0.000035
2022-09-15 17:01:40,684 - INFO - joeynmt.training - Epoch   1, Step:   213500, Batch Loss:     2.866780, Batch Acc: 0.003944, Tokens per Sec:     9569, Lr: 0.000035
2022-09-15 17:01:52,679 - INFO - joeynmt.training - Epoch   1, Step:   213600, Batch Loss:     2.610929, Batch Acc: 0.004482, Tokens per Sec:     9616, Lr: 0.000035
2022-09-15 17:02:04,662 - INFO - joeynmt.training - Epoch   1, Step:   213700, Batch Loss:     2.404848, Batch Acc: 0.004703, Tokens per Sec:     9530, Lr: 0.000035
2022-09-15 17:02:16,623 - INFO - joeynmt.training - Epoch   1, Step:   213800, Batch Loss:     2.622666, Batch Acc: 0.004133, Tokens per Sec:     9811, Lr: 0.000035
2022-09-15 17:02:28,624 - INFO - joeynmt.training - Epoch   1, Step:   213900, Batch Loss:     2.982459, Batch Acc: 0.003929, Tokens per Sec:     9501, Lr: 0.000035
2022-09-15 17:02:40,569 - INFO - joeynmt.training - Epoch   1, Step:   214000, Batch Loss:     2.573302, Batch Acc: 0.004849, Tokens per Sec:     9531, Lr: 0.000035
2022-09-15 17:02:40,570 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 17:12:24,473 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 17:12:24,474 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.32, loss:   2.59, ppl:  13.35, acc:   0.50, generation: 573.7957[sec], evaluation: 9.5767[sec]
2022-09-15 17:12:24,480 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 17:12:24,931 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/212000.ckpt
2022-09-15 17:12:24,960 - INFO - joeynmt.training - Example #0
2022-09-15 17:12:24,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 17:12:24,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 17:12:24,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 17:12:24,978 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 17:12:24,978 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 17:12:24,978 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 17:12:24,978 - INFO - joeynmt.training - Example #1
2022-09-15 17:12:24,979 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 17:12:24,979 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 17:12:24,979 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 17:12:24,994 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 17:12:24,994 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 17:12:24,994 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 17:12:24,994 - INFO - joeynmt.training - Example #2
2022-09-15 17:12:24,994 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 17:12:24,994 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 17:12:24,994 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 17:12:25,010 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 17:12:25,010 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 17:12:25,010 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 17:12:25,010 - INFO - joeynmt.training - Example #3
2022-09-15 17:12:25,010 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 17:12:25,010 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 17:12:25,010 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 17:12:25,025 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 17:12:25,025 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 17:12:25,026 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 17:12:37,376 - INFO - joeynmt.training - Epoch   1, Step:   214100, Batch Loss:     2.569275, Batch Acc: 0.004110, Tokens per Sec:     8830, Lr: 0.000035
2022-09-15 17:12:49,583 - INFO - joeynmt.training - Epoch   1, Step:   214200, Batch Loss:     2.815374, Batch Acc: 0.003876, Tokens per Sec:     9470, Lr: 0.000035
2022-09-15 17:13:01,700 - INFO - joeynmt.training - Epoch   1, Step:   214300, Batch Loss:     2.753102, Batch Acc: 0.004798, Tokens per Sec:     9237, Lr: 0.000035
2022-09-15 17:13:13,798 - INFO - joeynmt.training - Epoch   1, Step:   214400, Batch Loss:     2.782781, Batch Acc: 0.004943, Tokens per Sec:     9316, Lr: 0.000035
2022-09-15 17:13:25,834 - INFO - joeynmt.training - Epoch   1, Step:   214500, Batch Loss:     2.468746, Batch Acc: 0.004713, Tokens per Sec:     9431, Lr: 0.000035
2022-09-15 17:13:37,809 - INFO - joeynmt.training - Epoch   1, Step:   214600, Batch Loss:     2.362823, Batch Acc: 0.005202, Tokens per Sec:     9520, Lr: 0.000035
2022-09-15 17:13:50,065 - INFO - joeynmt.training - Epoch   1, Step:   214700, Batch Loss:     2.765347, Batch Acc: 0.004640, Tokens per Sec:     9303, Lr: 0.000035
2022-09-15 17:14:02,438 - INFO - joeynmt.training - Epoch   1, Step:   214800, Batch Loss:     2.785164, Batch Acc: 0.004658, Tokens per Sec:     9404, Lr: 0.000035
2022-09-15 17:14:14,697 - INFO - joeynmt.training - Epoch   1, Step:   214900, Batch Loss:     2.783530, Batch Acc: 0.004317, Tokens per Sec:     9297, Lr: 0.000035
2022-09-15 17:14:26,863 - INFO - joeynmt.training - Epoch   1, Step:   215000, Batch Loss:     2.659657, Batch Acc: 0.004560, Tokens per Sec:     9410, Lr: 0.000035
2022-09-15 17:14:26,864 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 17:24:00,305 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 17:24:00,306 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.37, loss:   2.58, ppl:  13.24, acc:   0.50, generation: 562.9557[sec], evaluation: 9.9792[sec]
2022-09-15 17:24:00,312 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 17:24:00,754 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/211000.ckpt
2022-09-15 17:24:00,782 - INFO - joeynmt.training - Example #0
2022-09-15 17:24:00,782 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 17:24:00,782 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 17:24:00,782 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 17:24:00,799 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 17:24:00,799 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 17:24:00,800 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 17:24:00,800 - INFO - joeynmt.training - Example #1
2022-09-15 17:24:00,800 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 17:24:00,800 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 17:24:00,800 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 17:24:00,815 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 17:24:00,815 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 17:24:00,815 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 17:24:00,815 - INFO - joeynmt.training - Example #2
2022-09-15 17:24:00,815 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 17:24:00,815 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 17:24:00,815 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 17:24:00,830 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 17:24:00,830 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 17:24:00,830 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 17:24:00,830 - INFO - joeynmt.training - Example #3
2022-09-15 17:24:00,830 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 17:24:00,830 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 17:24:00,830 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 17:24:00,845 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 17:24:00,845 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 17:24:00,845 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 17:24:12,885 - INFO - joeynmt.training - Epoch   1, Step:   215100, Batch Loss:     2.693979, Batch Acc: 0.004155, Tokens per Sec:     9111, Lr: 0.000035
2022-09-15 17:24:24,891 - INFO - joeynmt.training - Epoch   1, Step:   215200, Batch Loss:     2.726101, Batch Acc: 0.003471, Tokens per Sec:     9359, Lr: 0.000035
2022-09-15 17:24:36,957 - INFO - joeynmt.training - Epoch   1, Step:   215300, Batch Loss:     2.651348, Batch Acc: 0.004850, Tokens per Sec:     9483, Lr: 0.000035
2022-09-15 17:24:48,989 - INFO - joeynmt.training - Epoch   1, Step:   215400, Batch Loss:     2.686860, Batch Acc: 0.004841, Tokens per Sec:     9374, Lr: 0.000035
2022-09-15 17:25:01,068 - INFO - joeynmt.training - Epoch   1, Step:   215500, Batch Loss:     2.736922, Batch Acc: 0.005086, Tokens per Sec:     9523, Lr: 0.000035
2022-09-15 17:25:13,034 - INFO - joeynmt.training - Epoch   1, Step:   215600, Batch Loss:     2.637369, Batch Acc: 0.004796, Tokens per Sec:     9533, Lr: 0.000035
2022-09-15 17:25:24,988 - INFO - joeynmt.training - Epoch   1, Step:   215700, Batch Loss:     2.739899, Batch Acc: 0.004667, Tokens per Sec:     9554, Lr: 0.000035
2022-09-15 17:25:37,027 - INFO - joeynmt.training - Epoch   1, Step:   215800, Batch Loss:     2.914576, Batch Acc: 0.003554, Tokens per Sec:     9418, Lr: 0.000035
2022-09-15 17:25:49,100 - INFO - joeynmt.training - Epoch   1, Step:   215900, Batch Loss:     2.925925, Batch Acc: 0.003287, Tokens per Sec:     9451, Lr: 0.000035
2022-09-15 17:26:01,138 - INFO - joeynmt.training - Epoch   1, Step:   216000, Batch Loss:     2.782845, Batch Acc: 0.004342, Tokens per Sec:     9451, Lr: 0.000035
2022-09-15 17:26:01,138 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 17:35:24,166 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 17:35:24,167 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.38, loss:   2.58, ppl:  13.22, acc:   0.51, generation: 552.9463[sec], evaluation: 9.5799[sec]
2022-09-15 17:35:24,173 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 17:35:24,635 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/213000.ckpt
2022-09-15 17:35:24,663 - INFO - joeynmt.training - Example #0
2022-09-15 17:35:24,663 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 17:35:24,663 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 17:35:24,663 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 17:35:24,681 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 17:35:24,681 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 17:35:24,681 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 17:35:24,681 - INFO - joeynmt.training - Example #1
2022-09-15 17:35:24,681 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 17:35:24,682 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 17:35:24,682 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 17:35:24,698 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 17:35:24,698 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 17:35:24,698 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 17:35:24,698 - INFO - joeynmt.training - Example #2
2022-09-15 17:35:24,698 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 17:35:24,698 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 17:35:24,698 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 17:35:24,713 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 17:35:24,713 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 17:35:24,713 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 17:35:24,713 - INFO - joeynmt.training - Example #3
2022-09-15 17:35:24,713 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 17:35:24,713 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 17:35:24,713 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 17:35:24,728 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 17:35:24,728 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 17:35:24,728 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 17:35:36,815 - INFO - joeynmt.training - Epoch   1, Step:   216100, Batch Loss:     2.660793, Batch Acc: 0.004266, Tokens per Sec:     9087, Lr: 0.000035
2022-09-15 17:35:48,676 - INFO - joeynmt.training - Epoch   1, Step:   216200, Batch Loss:     2.847272, Batch Acc: 0.004441, Tokens per Sec:     9738, Lr: 0.000035
2022-09-15 17:36:00,593 - INFO - joeynmt.training - Epoch   1, Step:   216300, Batch Loss:     2.535940, Batch Acc: 0.005131, Tokens per Sec:     9601, Lr: 0.000035
2022-09-15 17:36:12,466 - INFO - joeynmt.training - Epoch   1, Step:   216400, Batch Loss:     2.458545, Batch Acc: 0.004071, Tokens per Sec:     9601, Lr: 0.000035
2022-09-15 17:36:24,325 - INFO - joeynmt.training - Epoch   1, Step:   216500, Batch Loss:     2.626337, Batch Acc: 0.005033, Tokens per Sec:     9668, Lr: 0.000035
2022-09-15 17:36:36,182 - INFO - joeynmt.training - Epoch   1, Step:   216600, Batch Loss:     2.673924, Batch Acc: 0.003776, Tokens per Sec:     9717, Lr: 0.000035
2022-09-15 17:36:48,056 - INFO - joeynmt.training - Epoch   1, Step:   216700, Batch Loss:     2.554667, Batch Acc: 0.004594, Tokens per Sec:     9735, Lr: 0.000035
2022-09-15 17:36:59,952 - INFO - joeynmt.training - Epoch   1, Step:   216800, Batch Loss:     2.676191, Batch Acc: 0.004164, Tokens per Sec:     9692, Lr: 0.000035
2022-09-15 17:37:11,828 - INFO - joeynmt.training - Epoch   1, Step:   216900, Batch Loss:     2.766829, Batch Acc: 0.004087, Tokens per Sec:     9623, Lr: 0.000035
2022-09-15 17:37:23,697 - INFO - joeynmt.training - Epoch   1, Step:   217000, Batch Loss:     2.737993, Batch Acc: 0.003828, Tokens per Sec:     9551, Lr: 0.000035
2022-09-15 17:37:23,698 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 17:46:58,223 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 17:46:58,225 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.44, loss:   2.58, ppl:  13.16, acc:   0.51, generation: 564.5131[sec], evaluation: 9.5075[sec]
2022-09-15 17:46:58,231 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 17:46:58,679 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/214000.ckpt
2022-09-15 17:46:58,707 - INFO - joeynmt.training - Example #0
2022-09-15 17:46:58,707 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 17:46:58,707 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 17:46:58,707 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 17:46:58,725 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 17:46:58,725 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 17:46:58,725 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 17:46:58,725 - INFO - joeynmt.training - Example #1
2022-09-15 17:46:58,725 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 17:46:58,725 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 17:46:58,725 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 17:46:58,740 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 17:46:58,740 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 17:46:58,740 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 17:46:58,740 - INFO - joeynmt.training - Example #2
2022-09-15 17:46:58,740 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 17:46:58,741 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 17:46:58,741 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 17:46:58,755 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 17:46:58,755 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 17:46:58,756 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 17:46:58,756 - INFO - joeynmt.training - Example #3
2022-09-15 17:46:58,756 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 17:46:58,756 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 17:46:58,756 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 17:46:58,771 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 17:46:58,771 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 17:46:58,771 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 17:47:10,772 - INFO - joeynmt.training - Epoch   1, Step:   217100, Batch Loss:     2.510846, Batch Acc: 0.005174, Tokens per Sec:     9015, Lr: 0.000035
2022-09-15 17:47:22,691 - INFO - joeynmt.training - Epoch   1, Step:   217200, Batch Loss:     2.512427, Batch Acc: 0.004184, Tokens per Sec:     9766, Lr: 0.000035
2022-09-15 17:47:34,582 - INFO - joeynmt.training - Epoch   1, Step:   217300, Batch Loss:     2.874503, Batch Acc: 0.005296, Tokens per Sec:     9512, Lr: 0.000035
2022-09-15 17:47:46,438 - INFO - joeynmt.training - Epoch   1, Step:   217400, Batch Loss:     2.790029, Batch Acc: 0.003724, Tokens per Sec:     9626, Lr: 0.000035
2022-09-15 17:47:54,092 - INFO - joeynmt.training - Epoch   1: total training loss 39557.12
2022-09-15 17:47:54,093 - INFO - joeynmt.training - Training ended after   1 epochs.
2022-09-15 17:47:54,093 - INFO - joeynmt.training - Best validation result (greedy) at step   217000:  13.16 ppl.
2022-09-15 17:47:54,093 - INFO - joeynmt.training - Loading from ckpt file: /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt
2022-09-15 17:47:54,109 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 17:47:54,109 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 17:47:54,449 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 17:47:54,453 - INFO - joeynmt.model - Total params: 19302144
2022-09-15 17:47:54,454 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2022-09-15 17:47:54,745 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 17:47:54,812 - INFO - joeynmt.prediction - Decoding on dev set...
2022-09-15 17:47:54,812 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:14:24,928 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 18:14:24,930 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  22.72, generation: 1580.1645[sec], evaluation: 9.4559[sec]
2022-09-15 18:14:24,990 - INFO - joeynmt.prediction - Translations saved to: /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/00217000.hyps.dev.
2022-09-15 18:14:24,991 - INFO - joeynmt.prediction - Decoding on test set...
2022-09-15 18:14:24,991 - INFO - joeynmt.prediction - Predicting 40858 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:39:41,678 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 18:39:41,679 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  22.20, generation: 1506.0229[sec], evaluation: 10.1762[sec]
2022-09-15 18:39:41,747 - INFO - joeynmt.prediction - Translations saved to: /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/00217000.hyps.test.
2022-09-15 18:39:41,759 - INFO - joeynmt.training - ACTIVE LEARNING MODEL END - LEAST CONFIDENCE 0
2022-09-15 18:39:41,760 - INFO - joeynmt.training - ACTIVE LEARNING MODEL START - LEAST CONFIDENCE 1
2022-09-15 18:39:41,761 - INFO - joeynmt.training - Random Indices picked: [121958 146867 131932 365838 259178 119879 110268 207892  54886 137337] length: 26753
2022-09-15 18:39:41,761 - INFO - joeynmt.training - Processing Predictions on Batch 0/105
2022-09-15 18:39:45,521 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:39:45,521 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:39:45,860 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:39:46,135 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:39:46,260 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:39:46,260 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:39:46,296 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:39:50,244 - INFO - joeynmt.prediction - Generation took 3.9353[sec]. (No references given)
2022-09-15 18:39:50,255 - INFO - joeynmt.training - Processing Predictions on Batch 1/105
2022-09-15 18:39:54,024 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:39:54,025 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:39:54,360 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:39:54,632 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:39:54,757 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:39:54,757 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:39:54,791 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:39:58,669 - INFO - joeynmt.prediction - Generation took 3.8664[sec]. (No references given)
2022-09-15 18:39:58,684 - INFO - joeynmt.training - Processing Predictions on Batch 2/105
2022-09-15 18:40:02,367 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:40:02,367 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:40:02,704 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:40:02,982 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:40:03,109 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:40:03,109 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:40:03,144 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:40:06,787 - INFO - joeynmt.prediction - Generation took 3.6318[sec]. (No references given)
2022-09-15 18:40:06,800 - INFO - joeynmt.training - Processing Predictions on Batch 3/105
2022-09-15 18:40:10,409 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:40:10,409 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:40:10,747 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:40:11,011 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:40:11,138 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:40:11,138 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:40:11,176 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:40:15,431 - INFO - joeynmt.prediction - Generation took 4.2425[sec]. (No references given)
2022-09-15 18:40:15,446 - INFO - joeynmt.training - Processing Predictions on Batch 4/105
2022-09-15 18:40:19,079 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:40:19,079 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:40:19,415 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:40:19,687 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:40:19,812 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:40:19,812 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:40:19,848 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:40:23,697 - INFO - joeynmt.prediction - Generation took 3.8376[sec]. (No references given)
2022-09-15 18:40:23,709 - INFO - joeynmt.training - Processing Predictions on Batch 5/105
2022-09-15 18:40:27,384 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:40:27,384 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:40:27,720 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:40:27,977 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:40:28,101 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:40:28,101 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:40:28,138 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:40:31,841 - INFO - joeynmt.prediction - Generation took 3.6904[sec]. (No references given)
2022-09-15 18:40:31,853 - INFO - joeynmt.training - Processing Predictions on Batch 6/105
2022-09-15 18:40:35,562 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:40:35,563 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:40:35,897 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:40:36,164 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:40:36,288 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:40:36,289 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:40:36,325 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:40:40,127 - INFO - joeynmt.prediction - Generation took 3.7902[sec]. (No references given)
2022-09-15 18:40:40,141 - INFO - joeynmt.training - Processing Predictions on Batch 7/105
2022-09-15 18:40:43,854 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:40:43,855 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:40:44,190 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:40:44,470 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:40:44,595 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:40:44,595 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:40:44,636 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:40:49,494 - INFO - joeynmt.prediction - Generation took 4.8451[sec]. (No references given)
2022-09-15 18:40:49,508 - INFO - joeynmt.training - Processing Predictions on Batch 8/105
2022-09-15 18:40:53,203 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:40:53,204 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:40:53,551 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:40:53,830 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:40:53,954 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:40:53,955 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:40:53,994 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:40:58,425 - INFO - joeynmt.prediction - Generation took 4.4177[sec]. (No references given)
2022-09-15 18:40:58,439 - INFO - joeynmt.training - Processing Predictions on Batch 9/105
2022-09-15 18:41:02,094 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:41:02,094 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:41:02,430 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:41:02,711 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:41:02,835 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:41:02,835 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:41:02,876 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:41:07,847 - INFO - joeynmt.prediction - Generation took 4.9576[sec]. (No references given)
2022-09-15 18:41:07,861 - INFO - joeynmt.training - Processing Predictions on Batch 10/105
2022-09-15 18:41:11,510 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:41:11,510 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:41:11,846 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:41:12,113 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:41:12,238 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:41:12,238 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:41:12,277 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:41:15,667 - INFO - joeynmt.prediction - Generation took 3.3789[sec]. (No references given)
2022-09-15 18:41:15,679 - INFO - joeynmt.training - Processing Predictions on Batch 11/105
2022-09-15 18:41:19,388 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:41:19,388 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:41:19,724 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:41:19,995 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:41:20,119 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:41:20,119 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:41:20,155 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:41:24,023 - INFO - joeynmt.prediction - Generation took 3.8564[sec]. (No references given)
2022-09-15 18:41:24,037 - INFO - joeynmt.training - Processing Predictions on Batch 12/105
2022-09-15 18:41:27,732 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:41:27,732 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:41:28,068 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:41:28,341 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:41:28,466 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:41:28,466 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:41:28,498 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:41:32,021 - INFO - joeynmt.prediction - Generation took 3.5122[sec]. (No references given)
2022-09-15 18:41:32,033 - INFO - joeynmt.training - Processing Predictions on Batch 13/105
2022-09-15 18:41:35,689 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:41:35,689 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:41:36,027 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:41:36,291 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:41:36,417 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:41:36,417 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:41:36,451 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:41:40,379 - INFO - joeynmt.prediction - Generation took 3.9164[sec]. (No references given)
2022-09-15 18:41:40,393 - INFO - joeynmt.training - Processing Predictions on Batch 14/105
2022-09-15 18:41:44,041 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:41:44,041 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:41:44,387 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:41:44,658 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:41:44,783 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:41:44,783 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:41:44,819 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:41:49,047 - INFO - joeynmt.prediction - Generation took 4.2163[sec]. (No references given)
2022-09-15 18:41:49,061 - INFO - joeynmt.training - Processing Predictions on Batch 15/105
2022-09-15 18:41:52,714 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:41:52,715 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:41:53,051 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:41:53,324 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:41:53,448 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:41:53,448 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:41:53,484 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:41:57,637 - INFO - joeynmt.prediction - Generation took 4.1400[sec]. (No references given)
2022-09-15 18:41:57,649 - INFO - joeynmt.training - Processing Predictions on Batch 16/105
2022-09-15 18:42:01,342 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:42:01,343 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:42:01,679 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:42:01,947 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:42:02,073 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:42:02,073 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:42:02,110 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:42:05,969 - INFO - joeynmt.prediction - Generation took 3.8471[sec]. (No references given)
2022-09-15 18:42:05,983 - INFO - joeynmt.training - Processing Predictions on Batch 17/105
2022-09-15 18:42:09,681 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:42:09,681 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:42:10,017 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:42:10,298 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:42:10,423 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:42:10,423 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:42:10,460 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:42:14,726 - INFO - joeynmt.prediction - Generation took 4.2522[sec]. (No references given)
2022-09-15 18:42:14,740 - INFO - joeynmt.training - Processing Predictions on Batch 18/105
2022-09-15 18:42:18,423 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:42:18,423 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:42:18,758 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:42:19,025 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:42:19,149 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:42:19,150 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:42:19,186 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:42:23,146 - INFO - joeynmt.prediction - Generation took 3.9484[sec]. (No references given)
2022-09-15 18:42:23,158 - INFO - joeynmt.training - Processing Predictions on Batch 19/105
2022-09-15 18:42:26,817 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:42:26,818 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:42:27,154 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:42:27,425 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:42:27,549 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:42:27,549 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:42:27,590 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:42:31,759 - INFO - joeynmt.prediction - Generation took 4.1559[sec]. (No references given)
2022-09-15 18:42:31,773 - INFO - joeynmt.training - Processing Predictions on Batch 20/105
2022-09-15 18:42:35,427 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:42:35,427 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:42:35,762 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:42:36,043 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:42:36,633 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:42:36,633 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:42:36,672 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:42:40,544 - INFO - joeynmt.prediction - Generation took 3.8596[sec]. (No references given)
2022-09-15 18:42:40,558 - INFO - joeynmt.training - Processing Predictions on Batch 21/105
2022-09-15 18:42:44,346 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:42:44,346 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:42:44,690 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:42:44,968 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:42:45,094 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:42:45,094 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:42:45,131 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:42:49,194 - INFO - joeynmt.prediction - Generation took 4.0510[sec]. (No references given)
2022-09-15 18:42:49,209 - INFO - joeynmt.training - Processing Predictions on Batch 22/105
2022-09-15 18:42:53,005 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:42:53,005 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:42:53,342 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:42:53,607 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:42:53,732 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:42:53,732 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:42:53,771 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:42:58,562 - INFO - joeynmt.prediction - Generation took 4.7784[sec]. (No references given)
2022-09-15 18:42:58,574 - INFO - joeynmt.training - Processing Predictions on Batch 23/105
2022-09-15 18:43:02,342 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:43:02,342 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:43:02,678 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:43:02,940 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:43:03,065 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:43:03,065 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:43:03,103 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:43:07,116 - INFO - joeynmt.prediction - Generation took 4.0006[sec]. (No references given)
2022-09-15 18:43:07,128 - INFO - joeynmt.training - Processing Predictions on Batch 24/105
2022-09-15 18:43:10,946 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:43:10,946 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:43:11,283 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:43:11,539 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:43:11,662 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:43:11,663 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:43:11,699 - INFO - joeynmt.prediction - Predicting 224 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:43:16,168 - INFO - joeynmt.prediction - Generation took 4.4579[sec]. (No references given)
2022-09-15 18:43:16,181 - INFO - joeynmt.training - Processing Predictions on Batch 25/105
2022-09-15 18:43:19,944 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:43:19,950 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:43:20,287 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:43:20,547 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:43:20,672 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:43:20,672 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:43:20,708 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:43:24,313 - INFO - joeynmt.prediction - Generation took 3.5929[sec]. (No references given)
2022-09-15 18:43:24,324 - INFO - joeynmt.training - Processing Predictions on Batch 26/105
2022-09-15 18:43:28,058 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:43:28,058 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:43:28,393 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:43:28,660 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:43:28,783 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:43:28,783 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:43:28,819 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:43:32,911 - INFO - joeynmt.prediction - Generation took 4.0804[sec]. (No references given)
2022-09-15 18:43:32,925 - INFO - joeynmt.training - Processing Predictions on Batch 27/105
2022-09-15 18:43:36,654 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:43:36,654 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:43:36,997 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:43:37,272 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:43:37,395 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:43:37,395 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:43:37,433 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:43:41,432 - INFO - joeynmt.prediction - Generation took 3.9875[sec]. (No references given)
2022-09-15 18:43:41,447 - INFO - joeynmt.training - Processing Predictions on Batch 28/105
2022-09-15 18:43:45,153 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:43:45,154 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:43:45,489 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:43:45,762 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:43:45,887 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:43:45,887 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:43:45,928 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:43:50,241 - INFO - joeynmt.prediction - Generation took 4.3018[sec]. (No references given)
2022-09-15 18:43:50,254 - INFO - joeynmt.training - Processing Predictions on Batch 29/105
2022-09-15 18:43:53,979 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:43:53,979 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:43:54,314 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:43:54,586 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:43:54,711 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:43:54,711 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:43:54,749 - INFO - joeynmt.prediction - Predicting 222 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:43:58,452 - INFO - joeynmt.prediction - Generation took 3.6904[sec]. (No references given)
2022-09-15 18:43:58,466 - INFO - joeynmt.training - Processing Predictions on Batch 30/105
2022-09-15 18:44:02,187 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:44:02,187 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:44:02,524 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:44:02,799 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:44:02,923 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:44:02,923 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:44:02,961 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:44:06,646 - INFO - joeynmt.prediction - Generation took 3.6727[sec]. (No references given)
2022-09-15 18:44:06,659 - INFO - joeynmt.training - Processing Predictions on Batch 31/105
2022-09-15 18:44:10,350 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:44:10,350 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:44:10,685 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:44:10,944 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:44:11,069 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:44:11,069 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:44:11,106 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:44:15,559 - INFO - joeynmt.prediction - Generation took 4.4397[sec]. (No references given)
2022-09-15 18:44:15,570 - INFO - joeynmt.training - Processing Predictions on Batch 32/105
2022-09-15 18:44:19,260 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:44:19,261 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:44:19,596 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:44:19,862 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:44:19,987 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:44:19,987 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:44:20,024 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:44:23,899 - INFO - joeynmt.prediction - Generation took 3.8639[sec]. (No references given)
2022-09-15 18:44:23,913 - INFO - joeynmt.training - Processing Predictions on Batch 33/105
2022-09-15 18:44:27,595 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:44:27,596 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:44:27,931 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:44:28,205 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:44:28,800 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:44:28,800 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:44:28,840 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:44:33,307 - INFO - joeynmt.prediction - Generation took 4.4541[sec]. (No references given)
2022-09-15 18:44:33,318 - INFO - joeynmt.training - Processing Predictions on Batch 34/105
2022-09-15 18:44:37,170 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:44:37,170 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:44:37,506 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:44:37,772 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:44:37,897 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:44:37,897 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:44:37,934 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:44:42,538 - INFO - joeynmt.prediction - Generation took 4.5921[sec]. (No references given)
2022-09-15 18:44:42,552 - INFO - joeynmt.training - Processing Predictions on Batch 35/105
2022-09-15 18:44:46,395 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:44:46,395 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:44:46,730 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:44:47,012 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:44:47,135 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:44:47,135 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:44:47,168 - INFO - joeynmt.prediction - Predicting 224 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:44:51,720 - INFO - joeynmt.prediction - Generation took 4.5402[sec]. (No references given)
2022-09-15 18:44:51,734 - INFO - joeynmt.training - Processing Predictions on Batch 36/105
2022-09-15 18:44:55,556 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:44:55,556 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:44:55,891 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:44:56,150 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:44:56,273 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:44:56,273 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:44:56,309 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:44:59,853 - INFO - joeynmt.prediction - Generation took 3.5322[sec]. (No references given)
2022-09-15 18:44:59,865 - INFO - joeynmt.training - Processing Predictions on Batch 37/105
2022-09-15 18:45:03,699 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:45:03,699 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:45:04,034 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:45:04,308 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:45:04,432 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:45:04,432 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:45:04,466 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:45:08,132 - INFO - joeynmt.prediction - Generation took 3.6549[sec]. (No references given)
2022-09-15 18:45:08,146 - INFO - joeynmt.training - Processing Predictions on Batch 38/105
2022-09-15 18:45:11,957 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:45:11,958 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:45:12,294 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:45:12,561 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:45:12,686 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:45:12,686 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:45:12,721 - INFO - joeynmt.prediction - Predicting 222 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:45:16,355 - INFO - joeynmt.prediction - Generation took 3.6230[sec]. (No references given)
2022-09-15 18:45:16,367 - INFO - joeynmt.training - Processing Predictions on Batch 39/105
2022-09-15 18:45:20,158 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:45:20,158 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:45:20,492 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:45:20,750 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:45:20,876 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:45:20,876 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:45:20,914 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:45:24,533 - INFO - joeynmt.prediction - Generation took 3.6063[sec]. (No references given)
2022-09-15 18:45:24,545 - INFO - joeynmt.training - Processing Predictions on Batch 40/105
2022-09-15 18:45:28,329 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:45:28,329 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:45:28,664 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:45:28,935 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:45:29,059 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:45:29,059 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:45:29,094 - INFO - joeynmt.prediction - Predicting 220 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:45:32,719 - INFO - joeynmt.prediction - Generation took 3.6133[sec]. (No references given)
2022-09-15 18:45:32,732 - INFO - joeynmt.training - Processing Predictions on Batch 41/105
2022-09-15 18:45:36,508 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:45:36,508 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:45:36,843 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:45:37,124 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:45:37,250 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:45:37,250 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:45:37,284 - INFO - joeynmt.prediction - Predicting 223 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:45:40,959 - INFO - joeynmt.prediction - Generation took 3.6635[sec]. (No references given)
2022-09-15 18:45:40,973 - INFO - joeynmt.training - Processing Predictions on Batch 42/105
2022-09-15 18:45:44,761 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:45:44,761 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:45:45,096 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:45:45,377 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:45:45,503 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:45:45,503 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:45:45,541 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:45:49,855 - INFO - joeynmt.prediction - Generation took 4.3023[sec]. (No references given)
2022-09-15 18:45:49,869 - INFO - joeynmt.training - Processing Predictions on Batch 43/105
2022-09-15 18:45:53,651 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:45:53,651 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:45:53,996 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:45:54,268 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:45:54,392 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:45:54,392 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:45:54,429 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:45:57,887 - INFO - joeynmt.prediction - Generation took 3.4459[sec]. (No references given)
2022-09-15 18:45:57,901 - INFO - joeynmt.training - Processing Predictions on Batch 44/105
2022-09-15 18:46:01,659 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:46:01,659 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:46:02,000 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:46:02,269 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:46:02,395 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:46:02,396 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:46:02,430 - INFO - joeynmt.prediction - Predicting 224 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:46:06,201 - INFO - joeynmt.prediction - Generation took 3.7608[sec]. (No references given)
2022-09-15 18:46:06,214 - INFO - joeynmt.training - Processing Predictions on Batch 45/105
2022-09-15 18:46:09,966 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:46:09,966 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:46:10,303 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:46:10,572 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:46:10,697 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:46:10,697 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:46:10,732 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:46:14,538 - INFO - joeynmt.prediction - Generation took 3.7948[sec]. (No references given)
2022-09-15 18:46:14,551 - INFO - joeynmt.training - Processing Predictions on Batch 46/105
2022-09-15 18:46:18,302 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:46:18,302 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:46:18,638 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:46:18,910 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:46:19,034 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:46:19,035 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:46:19,070 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:46:23,436 - INFO - joeynmt.prediction - Generation took 4.3546[sec]. (No references given)
2022-09-15 18:46:23,447 - INFO - joeynmt.training - Processing Predictions on Batch 47/105
2022-09-15 18:46:27,204 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:46:27,204 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:46:27,541 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:46:27,805 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:46:27,928 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:46:27,928 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:46:27,964 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:46:31,556 - INFO - joeynmt.prediction - Generation took 3.5802[sec]. (No references given)
2022-09-15 18:46:31,569 - INFO - joeynmt.training - Processing Predictions on Batch 48/105
2022-09-15 18:46:35,305 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:46:35,305 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:46:35,641 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:46:35,908 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:46:36,032 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:46:36,032 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:46:36,069 - INFO - joeynmt.prediction - Predicting 219 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:46:40,530 - INFO - joeynmt.prediction - Generation took 4.4476[sec]. (No references given)
2022-09-15 18:46:40,541 - INFO - joeynmt.training - Processing Predictions on Batch 49/105
2022-09-15 18:46:44,268 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:46:44,268 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:46:44,603 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:46:44,874 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:46:44,998 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:46:44,998 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:46:45,038 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:46:49,753 - INFO - joeynmt.prediction - Generation took 4.7025[sec]. (No references given)
2022-09-15 18:46:49,767 - INFO - joeynmt.training - Processing Predictions on Batch 50/105
2022-09-15 18:46:53,524 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:46:53,524 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:46:53,859 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:46:54,140 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:46:54,264 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:46:54,264 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:46:54,296 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:46:57,968 - INFO - joeynmt.prediction - Generation took 3.6605[sec]. (No references given)
2022-09-15 18:46:57,981 - INFO - joeynmt.training - Processing Predictions on Batch 51/105
2022-09-15 18:47:01,705 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:47:01,705 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:47:02,043 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:47:02,327 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:47:02,452 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:47:02,453 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:47:02,491 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:47:06,236 - INFO - joeynmt.prediction - Generation took 3.7329[sec]. (No references given)
2022-09-15 18:47:06,250 - INFO - joeynmt.training - Processing Predictions on Batch 52/105
2022-09-15 18:47:09,941 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:47:09,942 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:47:10,279 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:47:10,544 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:47:10,669 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:47:10,669 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:47:10,701 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:47:14,351 - INFO - joeynmt.prediction - Generation took 3.6383[sec]. (No references given)
2022-09-15 18:47:14,362 - INFO - joeynmt.training - Processing Predictions on Batch 53/105
2022-09-15 18:47:18,076 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:47:18,076 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:47:18,413 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:47:18,675 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:47:18,799 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:47:18,799 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:47:18,832 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:47:22,306 - INFO - joeynmt.prediction - Generation took 3.4625[sec]. (No references given)
2022-09-15 18:47:22,318 - INFO - joeynmt.training - Processing Predictions on Batch 54/105
2022-09-15 18:47:26,023 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:47:26,023 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:47:26,359 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:47:26,628 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:47:26,752 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:47:26,752 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:47:26,787 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:47:30,645 - INFO - joeynmt.prediction - Generation took 3.8454[sec]. (No references given)
2022-09-15 18:47:30,659 - INFO - joeynmt.training - Processing Predictions on Batch 55/105
2022-09-15 18:47:34,394 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:47:34,395 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:47:34,731 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:47:35,010 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:47:35,134 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:47:35,134 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:47:35,172 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:47:39,199 - INFO - joeynmt.prediction - Generation took 4.0151[sec]. (No references given)
2022-09-15 18:47:39,213 - INFO - joeynmt.training - Processing Predictions on Batch 56/105
2022-09-15 18:47:42,941 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:47:42,941 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:47:43,286 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:47:43,560 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:47:43,683 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:47:43,684 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:47:43,720 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:47:47,933 - INFO - joeynmt.prediction - Generation took 4.1996[sec]. (No references given)
2022-09-15 18:47:47,946 - INFO - joeynmt.training - Processing Predictions on Batch 57/105
2022-09-15 18:47:51,644 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:47:51,644 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:47:51,979 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:47:52,252 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:47:52,376 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:47:52,377 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:47:52,411 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:47:56,637 - INFO - joeynmt.prediction - Generation took 4.2119[sec]. (No references given)
2022-09-15 18:47:56,648 - INFO - joeynmt.training - Processing Predictions on Batch 58/105
2022-09-15 18:48:00,351 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:48:00,352 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:48:00,687 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:48:00,955 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:48:01,079 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:48:01,079 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:48:01,115 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:48:05,241 - INFO - joeynmt.prediction - Generation took 4.1149[sec]. (No references given)
2022-09-15 18:48:05,255 - INFO - joeynmt.training - Processing Predictions on Batch 59/105
2022-09-15 18:48:08,941 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:48:08,943 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:48:09,286 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:48:09,561 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:48:09,685 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:48:09,685 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:48:09,717 - INFO - joeynmt.prediction - Predicting 220 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:48:13,634 - INFO - joeynmt.prediction - Generation took 3.9053[sec]. (No references given)
2022-09-15 18:48:13,648 - INFO - joeynmt.training - Processing Predictions on Batch 60/105
2022-09-15 18:48:17,374 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:48:17,374 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:48:17,710 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:48:18,460 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:48:18,584 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:48:18,584 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:48:18,619 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:48:22,811 - INFO - joeynmt.prediction - Generation took 4.1803[sec]. (No references given)
2022-09-15 18:48:22,823 - INFO - joeynmt.training - Processing Predictions on Batch 61/105
2022-09-15 18:48:26,605 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:48:26,605 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:48:26,942 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:48:27,203 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:48:27,328 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:48:27,328 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:48:27,361 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:48:31,213 - INFO - joeynmt.prediction - Generation took 3.8405[sec]. (No references given)
2022-09-15 18:48:31,227 - INFO - joeynmt.training - Processing Predictions on Batch 62/105
2022-09-15 18:48:35,004 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:48:35,004 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:48:35,341 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:48:35,619 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:48:35,744 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:48:35,744 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:48:35,780 - INFO - joeynmt.prediction - Predicting 211 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:48:39,324 - INFO - joeynmt.prediction - Generation took 3.5337[sec]. (No references given)
2022-09-15 18:48:39,338 - INFO - joeynmt.training - Processing Predictions on Batch 63/105
2022-09-15 18:48:43,097 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:48:43,097 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:48:43,436 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:48:43,716 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:48:43,840 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:48:43,840 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:48:43,878 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:48:47,594 - INFO - joeynmt.prediction - Generation took 3.7048[sec]. (No references given)
2022-09-15 18:48:47,608 - INFO - joeynmt.training - Processing Predictions on Batch 64/105
2022-09-15 18:48:51,358 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:48:51,358 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:48:51,695 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:48:51,975 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:48:52,100 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:48:52,100 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:48:52,138 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:48:56,184 - INFO - joeynmt.prediction - Generation took 4.0337[sec]. (No references given)
2022-09-15 18:48:56,198 - INFO - joeynmt.training - Processing Predictions on Batch 65/105
2022-09-15 18:48:59,947 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:48:59,947 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:49:00,295 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:49:00,574 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:49:00,698 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:49:00,698 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:49:00,735 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:49:04,805 - INFO - joeynmt.prediction - Generation took 4.0573[sec]. (No references given)
2022-09-15 18:49:04,819 - INFO - joeynmt.training - Processing Predictions on Batch 66/105
2022-09-15 18:49:08,553 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:49:08,553 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:49:08,898 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:49:09,160 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:49:09,285 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:49:09,285 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:49:09,323 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:49:13,615 - INFO - joeynmt.prediction - Generation took 4.2812[sec]. (No references given)
2022-09-15 18:49:13,627 - INFO - joeynmt.training - Processing Predictions on Batch 67/105
2022-09-15 18:49:17,330 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:49:17,330 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:49:17,667 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:49:17,939 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:49:18,065 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:49:18,066 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:49:18,102 - INFO - joeynmt.prediction - Predicting 224 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:49:21,878 - INFO - joeynmt.prediction - Generation took 3.7646[sec]. (No references given)
2022-09-15 18:49:21,893 - INFO - joeynmt.training - Processing Predictions on Batch 68/105
2022-09-15 18:49:25,570 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:49:25,570 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:49:25,907 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:49:26,187 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:49:26,311 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:49:26,311 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:49:26,346 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:49:30,075 - INFO - joeynmt.prediction - Generation took 3.7163[sec]. (No references given)
2022-09-15 18:49:30,088 - INFO - joeynmt.training - Processing Predictions on Batch 69/105
2022-09-15 18:49:33,775 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:49:33,775 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:49:34,112 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:49:34,392 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:49:34,517 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:49:34,517 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:49:34,553 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:49:38,379 - INFO - joeynmt.prediction - Generation took 3.8141[sec]. (No references given)
2022-09-15 18:49:38,393 - INFO - joeynmt.training - Processing Predictions on Batch 70/105
2022-09-15 18:49:42,088 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:49:42,089 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:49:42,437 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:49:42,717 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:49:42,847 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:49:42,847 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:49:42,882 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:49:46,938 - INFO - joeynmt.prediction - Generation took 4.0445[sec]. (No references given)
2022-09-15 18:49:46,953 - INFO - joeynmt.training - Processing Predictions on Batch 71/105
2022-09-15 18:49:50,631 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:49:50,631 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:49:50,970 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:49:51,253 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:49:51,378 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:49:51,378 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:49:51,417 - INFO - joeynmt.prediction - Predicting 223 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:49:55,446 - INFO - joeynmt.prediction - Generation took 4.0169[sec]. (No references given)
2022-09-15 18:49:55,460 - INFO - joeynmt.training - Processing Predictions on Batch 72/105
2022-09-15 18:49:59,140 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:49:59,140 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:49:59,476 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:49:59,756 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:49:59,880 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:49:59,881 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:49:59,918 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:50:03,752 - INFO - joeynmt.prediction - Generation took 3.8221[sec]. (No references given)
2022-09-15 18:50:03,766 - INFO - joeynmt.training - Processing Predictions on Batch 73/105
2022-09-15 18:50:07,439 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:50:07,439 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:50:07,785 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:50:08,062 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:50:08,186 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:50:08,187 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:50:08,226 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:50:12,077 - INFO - joeynmt.prediction - Generation took 3.8389[sec]. (No references given)
2022-09-15 18:50:12,092 - INFO - joeynmt.training - Processing Predictions on Batch 74/105
2022-09-15 18:50:15,773 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:50:15,773 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:50:16,118 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:50:16,396 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:50:16,520 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:50:16,521 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:50:16,556 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:50:20,637 - INFO - joeynmt.prediction - Generation took 4.0691[sec]. (No references given)
2022-09-15 18:50:20,650 - INFO - joeynmt.training - Processing Predictions on Batch 75/105
2022-09-15 18:50:24,364 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:50:24,364 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:50:24,707 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:50:24,981 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:50:25,105 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:50:25,105 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:50:25,140 - INFO - joeynmt.prediction - Predicting 218 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:50:28,849 - INFO - joeynmt.prediction - Generation took 3.6980[sec]. (No references given)
2022-09-15 18:50:28,863 - INFO - joeynmt.training - Processing Predictions on Batch 76/105
2022-09-15 18:50:32,579 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:50:32,579 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:50:32,916 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:50:33,195 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:50:33,319 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:50:33,320 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:50:33,352 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:50:37,383 - INFO - joeynmt.prediction - Generation took 4.0193[sec]. (No references given)
2022-09-15 18:50:37,396 - INFO - joeynmt.training - Processing Predictions on Batch 77/105
2022-09-15 18:50:41,114 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:50:41,114 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:50:41,453 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:50:41,733 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:50:41,857 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:50:41,858 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:50:41,895 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:50:45,924 - INFO - joeynmt.prediction - Generation took 4.0173[sec]. (No references given)
2022-09-15 18:50:45,938 - INFO - joeynmt.training - Processing Predictions on Batch 78/105
2022-09-15 18:50:49,670 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:50:49,670 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:50:50,015 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:50:50,295 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:50:50,420 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:50:50,420 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:50:50,454 - INFO - joeynmt.prediction - Predicting 216 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:50:54,242 - INFO - joeynmt.prediction - Generation took 3.7759[sec]. (No references given)
2022-09-15 18:50:54,257 - INFO - joeynmt.training - Processing Predictions on Batch 79/105
2022-09-15 18:50:57,975 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:50:57,975 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:50:58,312 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:50:58,591 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:50:58,716 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:50:58,716 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:50:58,752 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:51:02,419 - INFO - joeynmt.prediction - Generation took 3.6548[sec]. (No references given)
2022-09-15 18:51:02,434 - INFO - joeynmt.training - Processing Predictions on Batch 80/105
2022-09-15 18:51:06,126 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:51:06,126 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:51:06,471 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:51:06,749 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:51:06,880 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:51:06,880 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:51:06,914 - INFO - joeynmt.prediction - Predicting 219 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:51:10,926 - INFO - joeynmt.prediction - Generation took 3.9995[sec]. (No references given)
2022-09-15 18:51:10,940 - INFO - joeynmt.training - Processing Predictions on Batch 81/105
2022-09-15 18:51:14,632 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:51:14,632 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:51:14,968 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:51:15,232 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:51:15,356 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:51:15,356 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:51:15,392 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:51:19,173 - INFO - joeynmt.prediction - Generation took 3.7694[sec]. (No references given)
2022-09-15 18:51:19,186 - INFO - joeynmt.training - Processing Predictions on Batch 82/105
2022-09-15 18:51:22,850 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:51:22,850 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:51:23,186 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:51:23,461 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:51:23,584 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:51:23,584 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:51:23,622 - INFO - joeynmt.prediction - Predicting 223 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:51:27,392 - INFO - joeynmt.prediction - Generation took 3.7581[sec]. (No references given)
2022-09-15 18:51:27,407 - INFO - joeynmt.training - Processing Predictions on Batch 83/105
2022-09-15 18:51:31,071 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:51:31,072 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:51:31,408 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:51:31,688 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:51:31,811 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:51:31,812 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:51:31,847 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:51:36,400 - INFO - joeynmt.prediction - Generation took 4.5413[sec]. (No references given)
2022-09-15 18:51:36,414 - INFO - joeynmt.training - Processing Predictions on Batch 84/105
2022-09-15 18:51:40,096 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:51:40,096 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:51:40,433 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:51:40,691 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:51:40,814 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:51:40,814 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:51:40,849 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:51:44,503 - INFO - joeynmt.prediction - Generation took 3.6416[sec]. (No references given)
2022-09-15 18:51:44,516 - INFO - joeynmt.training - Processing Predictions on Batch 85/105
2022-09-15 18:51:48,181 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:51:48,181 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:51:48,516 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:51:48,783 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:51:48,907 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:51:48,907 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:51:48,942 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:51:52,960 - INFO - joeynmt.prediction - Generation took 4.0064[sec]. (No references given)
2022-09-15 18:51:52,971 - INFO - joeynmt.training - Processing Predictions on Batch 86/105
2022-09-15 18:51:56,632 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:51:56,632 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:51:56,967 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:51:57,224 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:51:57,346 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:51:57,347 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:51:57,382 - INFO - joeynmt.prediction - Predicting 216 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:52:01,009 - INFO - joeynmt.prediction - Generation took 3.6157[sec]. (No references given)
2022-09-15 18:52:01,021 - INFO - joeynmt.training - Processing Predictions on Batch 87/105
2022-09-15 18:52:04,674 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:52:04,674 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:52:05,009 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:52:05,268 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:52:05,392 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:52:05,392 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:52:05,430 - INFO - joeynmt.prediction - Predicting 223 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:52:09,403 - INFO - joeynmt.prediction - Generation took 3.9618[sec]. (No references given)
2022-09-15 18:52:09,414 - INFO - joeynmt.training - Processing Predictions on Batch 88/105
2022-09-15 18:52:13,096 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:52:13,096 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:52:13,431 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:52:13,699 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:52:13,822 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:52:13,822 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:52:13,859 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:52:18,221 - INFO - joeynmt.prediction - Generation took 4.3496[sec]. (No references given)
2022-09-15 18:52:18,234 - INFO - joeynmt.training - Processing Predictions on Batch 89/105
2022-09-15 18:52:21,948 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:52:21,948 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:52:22,283 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:52:22,563 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:52:22,686 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:52:22,687 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:52:22,726 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:52:26,688 - INFO - joeynmt.prediction - Generation took 3.9501[sec]. (No references given)
2022-09-15 18:52:26,702 - INFO - joeynmt.training - Processing Predictions on Batch 90/105
2022-09-15 18:52:30,421 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:52:30,421 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:52:30,757 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:52:31,034 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:52:31,158 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:52:31,158 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:52:31,192 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:52:35,089 - INFO - joeynmt.prediction - Generation took 3.8859[sec]. (No references given)
2022-09-15 18:52:35,102 - INFO - joeynmt.training - Processing Predictions on Batch 91/105
2022-09-15 18:52:38,811 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:52:38,812 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:52:39,147 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:52:39,416 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:52:39,540 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:52:39,541 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:52:39,579 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:52:43,518 - INFO - joeynmt.prediction - Generation took 3.9261[sec]. (No references given)
2022-09-15 18:52:43,531 - INFO - joeynmt.training - Processing Predictions on Batch 92/105
2022-09-15 18:52:47,260 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:52:47,260 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:52:47,597 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:52:47,878 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:52:48,003 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:52:48,003 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:52:48,035 - INFO - joeynmt.prediction - Predicting 218 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:52:51,363 - INFO - joeynmt.prediction - Generation took 3.3175[sec]. (No references given)
2022-09-15 18:52:51,377 - INFO - joeynmt.training - Processing Predictions on Batch 93/105
2022-09-15 18:52:55,086 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:52:55,087 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:52:55,424 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:52:55,705 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:52:55,829 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:52:55,830 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:52:55,864 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:52:59,489 - INFO - joeynmt.prediction - Generation took 3.6134[sec]. (No references given)
2022-09-15 18:52:59,502 - INFO - joeynmt.training - Processing Predictions on Batch 94/105
2022-09-15 18:53:03,198 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:53:03,199 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:53:03,535 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:53:03,816 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:53:03,941 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:53:03,941 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:53:03,975 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:53:07,918 - INFO - joeynmt.prediction - Generation took 3.9322[sec]. (No references given)
2022-09-15 18:53:07,932 - INFO - joeynmt.training - Processing Predictions on Batch 95/105
2022-09-15 18:53:11,626 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:53:11,626 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:53:11,963 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:53:12,229 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:53:12,354 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:53:12,355 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:53:12,389 - INFO - joeynmt.prediction - Predicting 215 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:53:15,954 - INFO - joeynmt.prediction - Generation took 3.5529[sec]. (No references given)
2022-09-15 18:53:15,966 - INFO - joeynmt.training - Processing Predictions on Batch 96/105
2022-09-15 18:53:19,651 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:53:19,651 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:53:19,988 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:53:20,245 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:53:20,371 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:53:20,371 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:53:20,407 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:53:24,089 - INFO - joeynmt.prediction - Generation took 3.6712[sec]. (No references given)
2022-09-15 18:53:24,101 - INFO - joeynmt.training - Processing Predictions on Batch 97/105
2022-09-15 18:53:27,783 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:53:27,784 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:53:28,120 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:53:28,395 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:53:28,519 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:53:28,520 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:53:28,556 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:53:32,598 - INFO - joeynmt.prediction - Generation took 4.0307[sec]. (No references given)
2022-09-15 18:53:32,613 - INFO - joeynmt.training - Processing Predictions on Batch 98/105
2022-09-15 18:53:36,299 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:53:36,299 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:53:36,636 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:53:36,917 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:53:37,041 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:53:37,042 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:53:37,081 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:53:41,142 - INFO - joeynmt.prediction - Generation took 4.0491[sec]. (No references given)
2022-09-15 18:53:41,156 - INFO - joeynmt.training - Processing Predictions on Batch 99/105
2022-09-15 18:53:44,843 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:53:44,843 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:53:45,180 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:53:45,461 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:53:45,586 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:53:45,586 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:53:45,623 - INFO - joeynmt.prediction - Predicting 214 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:53:49,484 - INFO - joeynmt.prediction - Generation took 3.8495[sec]. (No references given)
2022-09-15 18:53:49,497 - INFO - joeynmt.training - Processing Predictions on Batch 100/105
2022-09-15 18:53:53,197 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:53:53,198 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:53:53,535 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:53:53,816 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:53:53,941 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:53:53,941 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:53:53,977 - INFO - joeynmt.prediction - Predicting 224 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:53:57,815 - INFO - joeynmt.prediction - Generation took 3.8264[sec]. (No references given)
2022-09-15 18:53:57,829 - INFO - joeynmt.training - Processing Predictions on Batch 101/105
2022-09-15 18:54:01,530 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:54:01,531 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:54:01,867 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:54:02,149 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:54:02,277 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:54:02,277 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:54:02,314 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:54:06,521 - INFO - joeynmt.prediction - Generation took 4.1957[sec]. (No references given)
2022-09-15 18:54:06,535 - INFO - joeynmt.training - Processing Predictions on Batch 102/105
2022-09-15 18:54:10,236 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:54:10,237 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:54:10,574 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:54:10,841 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:54:10,966 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:54:10,966 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:54:11,000 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:54:14,683 - INFO - joeynmt.prediction - Generation took 3.6704[sec]. (No references given)
2022-09-15 18:54:14,694 - INFO - joeynmt.training - Processing Predictions on Batch 103/105
2022-09-15 18:54:18,394 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:54:18,394 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:54:18,730 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:54:19,001 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:54:19,126 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:54:19,126 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:54:19,163 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:54:23,659 - INFO - joeynmt.prediction - Generation took 4.4846[sec]. (No references given)
2022-09-15 18:54:23,673 - INFO - joeynmt.training - Processing Predictions on Batch 104/105
2022-09-15 18:54:27,380 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 18:54:27,380 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 18:54:27,716 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 18:54:27,983 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt.
2022-09-15 18:54:28,107 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:54:28,108 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 18:54:28,124 - INFO - joeynmt.prediction - Predicting 108 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 18:54:30,734 - INFO - joeynmt.prediction - Generation took 2.6034[sec]. (No references given)
2022-09-15 18:54:30,884 - INFO - joeynmt.training - Final Query Indices picked: [318531, 343357, 272273, 110155, 67642, 102820, 196211, 43013, 345123, 274915] length: 10000
2022-09-15 18:54:30,884 - INFO - joeynmt.training - Query the samples, one at a time (interactive/file)
2022-09-15 18:54:34,239 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 4096
	effective batch size (w. parallel & accumulation): 4096
2022-09-15 18:54:34,240 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 19:03:57,297 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 19:03:57,299 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.42, loss:   2.58, ppl:  13.21, acc:   0.51, generation: 552.6442[sec], evaluation: 9.8136[sec]
2022-09-15 19:03:57,744 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/215000.ckpt
2022-09-15 19:03:57,773 - INFO - joeynmt.training - Example #0
2022-09-15 19:03:57,773 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 19:03:57,773 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 19:03:57,773 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 19:03:57,790 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 19:03:57,790 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 19:03:57,790 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 19:03:57,790 - INFO - joeynmt.training - Example #1
2022-09-15 19:03:57,790 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 19:03:57,790 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 19:03:57,790 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 19:03:57,805 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 19:03:57,805 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 19:03:57,805 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 19:03:57,805 - INFO - joeynmt.training - Example #2
2022-09-15 19:03:57,806 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 19:03:57,806 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 19:03:57,806 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 19:03:57,820 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 19:03:57,820 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 19:03:57,820 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 19:03:57,821 - INFO - joeynmt.training - Example #3
2022-09-15 19:03:57,821 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 19:03:57,821 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 19:03:57,821 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 19:03:57,835 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 19:03:57,836 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 19:03:57,836 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 19:03:57,897 - INFO - joeynmt.training - EPOCH 1
2022-09-15 19:04:02,564 - INFO - joeynmt.training - Epoch   1, Step:   217500, Batch Loss:     2.697171, Batch Acc: 0.012117, Tokens per Sec:     8775, Lr: 0.000035
2022-09-15 19:04:14,932 - INFO - joeynmt.training - Epoch   1, Step:   217600, Batch Loss:     2.536703, Batch Acc: 0.004567, Tokens per Sec:     9189, Lr: 0.000035
2022-09-15 19:04:27,204 - INFO - joeynmt.training - Epoch   1, Step:   217700, Batch Loss:     2.574714, Batch Acc: 0.004479, Tokens per Sec:     9333, Lr: 0.000035
2022-09-15 19:04:39,407 - INFO - joeynmt.training - Epoch   1, Step:   217800, Batch Loss:     2.599575, Batch Acc: 0.004726, Tokens per Sec:     9225, Lr: 0.000035
2022-09-15 19:04:51,574 - INFO - joeynmt.training - Epoch   1, Step:   217900, Batch Loss:     2.762811, Batch Acc: 0.005122, Tokens per Sec:     9243, Lr: 0.000035
2022-09-15 19:05:03,791 - INFO - joeynmt.training - Epoch   1, Step:   218000, Batch Loss:     2.853010, Batch Acc: 0.004541, Tokens per Sec:     9356, Lr: 0.000035
2022-09-15 19:05:03,791 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 19:14:16,357 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 19:14:16,359 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.45, loss:   2.58, ppl:  13.22, acc:   0.51, generation: 542.2806[sec], evaluation: 9.7800[sec]
2022-09-15 19:14:16,803 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/216000.ckpt
2022-09-15 19:14:16,832 - INFO - joeynmt.training - Example #0
2022-09-15 19:14:16,832 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 19:14:16,832 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 19:14:16,832 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 19:14:16,849 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 19:14:16,849 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 19:14:16,849 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 19:14:16,849 - INFO - joeynmt.training - Example #1
2022-09-15 19:14:16,849 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 19:14:16,849 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 19:14:16,849 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 19:14:16,864 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 19:14:16,864 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 19:14:16,864 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 19:14:16,865 - INFO - joeynmt.training - Example #2
2022-09-15 19:14:16,865 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 19:14:16,865 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 19:14:16,865 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 19:14:16,880 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 19:14:16,880 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 19:14:16,880 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 19:14:16,880 - INFO - joeynmt.training - Example #3
2022-09-15 19:14:16,880 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 19:14:16,880 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 19:14:16,880 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 19:14:16,895 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 19:14:16,895 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 19:14:16,895 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 19:14:29,159 - INFO - joeynmt.training - Epoch   1, Step:   218100, Batch Loss:     2.862102, Batch Acc: 0.004387, Tokens per Sec:     8979, Lr: 0.000035
2022-09-15 19:14:41,362 - INFO - joeynmt.training - Epoch   1, Step:   218200, Batch Loss:     2.714462, Batch Acc: 0.004758, Tokens per Sec:     9387, Lr: 0.000035
2022-09-15 19:14:53,550 - INFO - joeynmt.training - Epoch   1, Step:   218300, Batch Loss:     2.709895, Batch Acc: 0.004678, Tokens per Sec:     9471, Lr: 0.000035
2022-09-15 19:15:05,699 - INFO - joeynmt.training - Epoch   1, Step:   218400, Batch Loss:     2.450595, Batch Acc: 0.005374, Tokens per Sec:     9435, Lr: 0.000035
2022-09-15 19:15:17,822 - INFO - joeynmt.training - Epoch   1, Step:   218500, Batch Loss:     2.556159, Batch Acc: 0.005009, Tokens per Sec:     9420, Lr: 0.000035
2022-09-15 19:15:29,914 - INFO - joeynmt.training - Epoch   1, Step:   218600, Batch Loss:     2.507168, Batch Acc: 0.005398, Tokens per Sec:     9376, Lr: 0.000035
2022-09-15 19:15:42,015 - INFO - joeynmt.training - Epoch   1, Step:   218700, Batch Loss:     2.989274, Batch Acc: 0.003803, Tokens per Sec:     9388, Lr: 0.000035
2022-09-15 19:15:54,135 - INFO - joeynmt.training - Epoch   1, Step:   218800, Batch Loss:     2.637407, Batch Acc: 0.004962, Tokens per Sec:     9528, Lr: 0.000035
2022-09-15 19:16:06,229 - INFO - joeynmt.training - Epoch   1, Step:   218900, Batch Loss:     2.878926, Batch Acc: 0.003892, Tokens per Sec:     9497, Lr: 0.000035
2022-09-15 19:16:18,265 - INFO - joeynmt.training - Epoch   1, Step:   219000, Batch Loss:     2.733027, Batch Acc: 0.005447, Tokens per Sec:     9351, Lr: 0.000035
2022-09-15 19:16:18,265 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 19:25:41,982 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 19:25:41,984 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.43, loss:   2.58, ppl:  13.17, acc:   0.51, generation: 553.3300[sec], evaluation: 9.8729[sec]
2022-09-15 19:25:42,424 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/218000.ckpt
2022-09-15 19:25:42,452 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/218000.ckpt
2022-09-15 19:25:42,452 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/218000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/218000.ckpt')
2022-09-15 19:25:42,453 - INFO - joeynmt.training - Example #0
2022-09-15 19:25:42,453 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 19:25:42,453 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 19:25:42,453 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 19:25:42,471 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 19:25:42,471 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 19:25:42,471 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 19:25:42,471 - INFO - joeynmt.training - Example #1
2022-09-15 19:25:42,471 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 19:25:42,471 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 19:25:42,471 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 19:25:42,487 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 19:25:42,487 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 19:25:42,487 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 19:25:42,487 - INFO - joeynmt.training - Example #2
2022-09-15 19:25:42,487 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 19:25:42,487 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 19:25:42,487 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 19:25:42,502 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 19:25:42,502 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 19:25:42,502 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 19:25:42,502 - INFO - joeynmt.training - Example #3
2022-09-15 19:25:42,503 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 19:25:42,503 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 19:25:42,503 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 19:25:42,518 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 19:25:42,518 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 19:25:42,518 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 19:25:54,629 - INFO - joeynmt.training - Epoch   1, Step:   219100, Batch Loss:     2.703768, Batch Acc: 0.003729, Tokens per Sec:     8932, Lr: 0.000035
2022-09-15 19:26:06,699 - INFO - joeynmt.training - Epoch   1, Step:   219200, Batch Loss:     2.777974, Batch Acc: 0.004688, Tokens per Sec:     9368, Lr: 0.000035
2022-09-15 19:26:18,767 - INFO - joeynmt.training - Epoch   1, Step:   219300, Batch Loss:     2.650210, Batch Acc: 0.004218, Tokens per Sec:     9430, Lr: 0.000035
2022-09-15 19:26:30,741 - INFO - joeynmt.training - Epoch   1, Step:   219400, Batch Loss:     2.728526, Batch Acc: 0.004233, Tokens per Sec:     9391, Lr: 0.000035
2022-09-15 19:26:42,756 - INFO - joeynmt.training - Epoch   1, Step:   219500, Batch Loss:     2.721028, Batch Acc: 0.004804, Tokens per Sec:     9530, Lr: 0.000035
2022-09-15 19:26:54,739 - INFO - joeynmt.training - Epoch   1, Step:   219600, Batch Loss:     2.470124, Batch Acc: 0.005364, Tokens per Sec:     9489, Lr: 0.000035
2022-09-15 19:27:06,760 - INFO - joeynmt.training - Epoch   1, Step:   219700, Batch Loss:     2.519211, Batch Acc: 0.004640, Tokens per Sec:     9450, Lr: 0.000035
2022-09-15 19:27:18,842 - INFO - joeynmt.training - Epoch   1, Step:   219800, Batch Loss:     2.869352, Batch Acc: 0.004227, Tokens per Sec:     9537, Lr: 0.000035
2022-09-15 19:27:30,924 - INFO - joeynmt.training - Epoch   1, Step:   219900, Batch Loss:     2.711759, Batch Acc: 0.004828, Tokens per Sec:     9480, Lr: 0.000035
2022-09-15 19:27:42,953 - INFO - joeynmt.training - Epoch   1, Step:   220000, Batch Loss:     2.648513, Batch Acc: 0.004683, Tokens per Sec:     9427, Lr: 0.000035
2022-09-15 19:27:42,953 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 19:36:59,804 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 19:36:59,806 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.49, loss:   2.57, ppl:  13.04, acc:   0.51, generation: 546.4302[sec], evaluation: 9.5453[sec]
2022-09-15 19:36:59,812 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 19:37:00,249 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217464.ckpt
2022-09-15 19:37:00,278 - INFO - joeynmt.training - Example #0
2022-09-15 19:37:00,278 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 19:37:00,278 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 19:37:00,278 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 19:37:00,296 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 19:37:00,296 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 19:37:00,296 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 19:37:00,296 - INFO - joeynmt.training - Example #1
2022-09-15 19:37:00,296 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 19:37:00,296 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 19:37:00,296 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 19:37:00,311 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 19:37:00,311 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 19:37:00,311 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 19:37:00,311 - INFO - joeynmt.training - Example #2
2022-09-15 19:37:00,312 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 19:37:00,312 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 19:37:00,312 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 19:37:00,327 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 19:37:00,327 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 19:37:00,327 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 19:37:00,327 - INFO - joeynmt.training - Example #3
2022-09-15 19:37:00,327 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 19:37:00,327 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 19:37:00,327 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 19:37:00,342 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 19:37:00,342 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 19:37:00,343 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 19:37:12,444 - INFO - joeynmt.training - Epoch   1, Step:   220100, Batch Loss:     2.836931, Batch Acc: 0.004181, Tokens per Sec:     8954, Lr: 0.000035
2022-09-15 19:37:24,517 - INFO - joeynmt.training - Epoch   1, Step:   220200, Batch Loss:     2.879847, Batch Acc: 0.004540, Tokens per Sec:     9742, Lr: 0.000035
2022-09-15 19:37:36,564 - INFO - joeynmt.training - Epoch   1, Step:   220300, Batch Loss:     2.466806, Batch Acc: 0.004198, Tokens per Sec:     9490, Lr: 0.000035
2022-09-15 19:37:48,565 - INFO - joeynmt.training - Epoch   1, Step:   220400, Batch Loss:     2.646399, Batch Acc: 0.004392, Tokens per Sec:     9524, Lr: 0.000035
2022-09-15 19:38:00,598 - INFO - joeynmt.training - Epoch   1, Step:   220500, Batch Loss:     2.770054, Batch Acc: 0.004108, Tokens per Sec:     9589, Lr: 0.000035
2022-09-15 19:38:12,652 - INFO - joeynmt.training - Epoch   1, Step:   220600, Batch Loss:     2.743945, Batch Acc: 0.004275, Tokens per Sec:     9450, Lr: 0.000035
2022-09-15 19:38:24,687 - INFO - joeynmt.training - Epoch   1, Step:   220700, Batch Loss:     2.595157, Batch Acc: 0.004712, Tokens per Sec:     9541, Lr: 0.000035
2022-09-15 19:38:36,708 - INFO - joeynmt.training - Epoch   1, Step:   220800, Batch Loss:     2.502258, Batch Acc: 0.005159, Tokens per Sec:     9563, Lr: 0.000035
2022-09-15 19:38:48,715 - INFO - joeynmt.training - Epoch   1, Step:   220900, Batch Loss:     2.657725, Batch Acc: 0.004918, Tokens per Sec:     9416, Lr: 0.000035
2022-09-15 19:39:00,735 - INFO - joeynmt.training - Epoch   1, Step:   221000, Batch Loss:     2.850920, Batch Acc: 0.004596, Tokens per Sec:     9521, Lr: 0.000035
2022-09-15 19:39:00,736 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 19:48:08,606 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 19:48:08,607 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.49, loss:   2.57, ppl:  13.00, acc:   0.51, generation: 537.3958[sec], evaluation: 9.9715[sec]
2022-09-15 19:48:08,613 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 19:48:09,049 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/219000.ckpt
2022-09-15 19:48:09,078 - INFO - joeynmt.training - Example #0
2022-09-15 19:48:09,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 19:48:09,078 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 19:48:09,078 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-15 19:48:09,095 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 19:48:09,095 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 19:48:09,095 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 19:48:09,096 - INFO - joeynmt.training - Example #1
2022-09-15 19:48:09,096 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 19:48:09,096 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 19:48:09,096 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 19:48:09,111 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 19:48:09,111 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 19:48:09,111 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 19:48:09,111 - INFO - joeynmt.training - Example #2
2022-09-15 19:48:09,111 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 19:48:09,111 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 19:48:09,111 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'गति', 'और', 'इ@@', 'म', '</s>']
2022-09-15 19:48:09,126 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 19:48:09,126 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 19:48:09,126 - INFO - joeynmt.training - 	Hypothesis: अभिनय , गति और इम
2022-09-15 19:48:09,126 - INFO - joeynmt.training - Example #3
2022-09-15 19:48:09,126 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 19:48:09,126 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 19:48:09,126 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 19:48:09,141 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 19:48:09,141 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 19:48:09,141 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 19:48:21,267 - INFO - joeynmt.training - Epoch   1, Step:   221100, Batch Loss:     2.772786, Batch Acc: 0.004205, Tokens per Sec:     9134, Lr: 0.000035
2022-09-15 19:48:33,307 - INFO - joeynmt.training - Epoch   1, Step:   221200, Batch Loss:     2.782740, Batch Acc: 0.004793, Tokens per Sec:     9444, Lr: 0.000035
2022-09-15 19:48:45,307 - INFO - joeynmt.training - Epoch   1, Step:   221300, Batch Loss:     2.776919, Batch Acc: 0.004523, Tokens per Sec:     9563, Lr: 0.000035
2022-09-15 19:48:57,279 - INFO - joeynmt.training - Epoch   1, Step:   221400, Batch Loss:     2.645029, Batch Acc: 0.004479, Tokens per Sec:     9399, Lr: 0.000035
2022-09-15 19:49:09,290 - INFO - joeynmt.training - Epoch   1, Step:   221500, Batch Loss:     2.868955, Batch Acc: 0.004454, Tokens per Sec:     9479, Lr: 0.000035
2022-09-15 19:49:21,294 - INFO - joeynmt.training - Epoch   1, Step:   221600, Batch Loss:     2.720514, Batch Acc: 0.004722, Tokens per Sec:     9510, Lr: 0.000035
2022-09-15 19:49:33,291 - INFO - joeynmt.training - Epoch   1, Step:   221700, Batch Loss:     2.661049, Batch Acc: 0.004258, Tokens per Sec:     9435, Lr: 0.000035
2022-09-15 19:49:45,316 - INFO - joeynmt.training - Epoch   1, Step:   221800, Batch Loss:     2.753759, Batch Acc: 0.004517, Tokens per Sec:     9684, Lr: 0.000035
2022-09-15 19:49:57,339 - INFO - joeynmt.training - Epoch   1, Step:   221900, Batch Loss:     2.766577, Batch Acc: 0.005201, Tokens per Sec:     9581, Lr: 0.000035
2022-09-15 19:50:09,395 - INFO - joeynmt.training - Epoch   1, Step:   222000, Batch Loss:     2.505704, Batch Acc: 0.004969, Tokens per Sec:     9515, Lr: 0.000035
2022-09-15 19:50:09,395 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 19:59:36,925 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 19:59:36,926 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.62, loss:   2.56, ppl:  13.00, acc:   0.51, generation: 557.5886[sec], evaluation: 9.4288[sec]
2022-09-15 19:59:36,932 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 19:59:37,366 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/217000.ckpt
2022-09-15 19:59:37,395 - INFO - joeynmt.training - Example #0
2022-09-15 19:59:37,395 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 19:59:37,395 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 19:59:37,395 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 19:59:37,412 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 19:59:37,412 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 19:59:37,412 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 19:59:37,412 - INFO - joeynmt.training - Example #1
2022-09-15 19:59:37,412 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 19:59:37,412 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 19:59:37,412 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 19:59:37,427 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 19:59:37,427 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 19:59:37,427 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 19:59:37,427 - INFO - joeynmt.training - Example #2
2022-09-15 19:59:37,428 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 19:59:37,428 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 19:59:37,428 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['कार्य', ',', 'गति', 'और', 'इ@@', 'म', '</s>']
2022-09-15 19:59:37,442 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 19:59:37,443 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 19:59:37,443 - INFO - joeynmt.training - 	Hypothesis: कार्य , गति और इम
2022-09-15 19:59:37,443 - INFO - joeynmt.training - Example #3
2022-09-15 19:59:37,443 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 19:59:37,443 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 19:59:37,443 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 19:59:37,458 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 19:59:37,458 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 19:59:37,458 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 19:59:49,508 - INFO - joeynmt.training - Epoch   1, Step:   222100, Batch Loss:     2.787766, Batch Acc: 0.004197, Tokens per Sec:     9171, Lr: 0.000035
2022-09-15 20:00:01,520 - INFO - joeynmt.training - Epoch   1, Step:   222200, Batch Loss:     2.570292, Batch Acc: 0.005252, Tokens per Sec:     9591, Lr: 0.000035
2022-09-15 20:00:13,499 - INFO - joeynmt.training - Epoch   1, Step:   222300, Batch Loss:     2.594132, Batch Acc: 0.004174, Tokens per Sec:     9460, Lr: 0.000035
2022-09-15 20:00:25,829 - INFO - joeynmt.training - Epoch   1, Step:   222400, Batch Loss:     2.809981, Batch Acc: 0.004054, Tokens per Sec:     9404, Lr: 0.000035
2022-09-15 20:00:37,756 - INFO - joeynmt.training - Epoch   1, Step:   222500, Batch Loss:     2.392360, Batch Acc: 0.004525, Tokens per Sec:     9599, Lr: 0.000035
2022-09-15 20:00:49,698 - INFO - joeynmt.training - Epoch   1, Step:   222600, Batch Loss:     2.755170, Batch Acc: 0.004975, Tokens per Sec:     9831, Lr: 0.000035
2022-09-15 20:01:01,615 - INFO - joeynmt.training - Epoch   1, Step:   222700, Batch Loss:     2.432414, Batch Acc: 0.004646, Tokens per Sec:     9537, Lr: 0.000035
2022-09-15 20:01:13,535 - INFO - joeynmt.training - Epoch   1, Step:   222800, Batch Loss:     2.584712, Batch Acc: 0.004478, Tokens per Sec:     9497, Lr: 0.000035
2022-09-15 20:01:25,461 - INFO - joeynmt.training - Epoch   1, Step:   222900, Batch Loss:     2.658604, Batch Acc: 0.004667, Tokens per Sec:     9596, Lr: 0.000035
2022-09-15 20:01:37,354 - INFO - joeynmt.training - Epoch   1, Step:   223000, Batch Loss:     2.738318, Batch Acc: 0.004388, Tokens per Sec:     9639, Lr: 0.000035
2022-09-15 20:01:37,356 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 20:10:45,745 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 20:10:45,746 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.51, loss:   2.56, ppl:  12.97, acc:   0.51, generation: 538.3691[sec], evaluation: 9.5107[sec]
2022-09-15 20:10:45,752 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 20:10:46,186 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/220000.ckpt
2022-09-15 20:10:46,215 - INFO - joeynmt.training - Example #0
2022-09-15 20:10:46,215 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 20:10:46,215 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 20:10:46,215 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 20:10:46,232 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 20:10:46,232 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 20:10:46,232 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 20:10:46,232 - INFO - joeynmt.training - Example #1
2022-09-15 20:10:46,232 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 20:10:46,232 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 20:10:46,233 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 20:10:46,247 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 20:10:46,247 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 20:10:46,247 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 20:10:46,248 - INFO - joeynmt.training - Example #2
2022-09-15 20:10:46,248 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 20:10:46,248 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 20:10:46,248 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 20:10:46,263 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 20:10:46,263 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 20:10:46,263 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 20:10:46,263 - INFO - joeynmt.training - Example #3
2022-09-15 20:10:46,263 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 20:10:46,263 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 20:10:46,263 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 20:10:46,278 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 20:10:46,278 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 20:10:46,278 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 20:10:58,302 - INFO - joeynmt.training - Epoch   1, Step:   223100, Batch Loss:     3.056662, Batch Acc: 0.003163, Tokens per Sec:     9220, Lr: 0.000035
2022-09-15 20:11:10,257 - INFO - joeynmt.training - Epoch   1, Step:   223200, Batch Loss:     2.536258, Batch Acc: 0.004455, Tokens per Sec:     9576, Lr: 0.000035
2022-09-15 20:11:22,214 - INFO - joeynmt.training - Epoch   1, Step:   223300, Batch Loss:     2.560157, Batch Acc: 0.005141, Tokens per Sec:     9695, Lr: 0.000035
2022-09-15 20:11:34,107 - INFO - joeynmt.training - Epoch   1, Step:   223400, Batch Loss:     2.566190, Batch Acc: 0.004270, Tokens per Sec:     9590, Lr: 0.000035
2022-09-15 20:11:46,043 - INFO - joeynmt.training - Epoch   1, Step:   223500, Batch Loss:     2.673657, Batch Acc: 0.004818, Tokens per Sec:     9668, Lr: 0.000035
2022-09-15 20:11:57,976 - INFO - joeynmt.training - Epoch   1, Step:   223600, Batch Loss:     2.607245, Batch Acc: 0.005006, Tokens per Sec:     9543, Lr: 0.000035
2022-09-15 20:12:09,940 - INFO - joeynmt.training - Epoch   1, Step:   223700, Batch Loss:     2.486689, Batch Acc: 0.005096, Tokens per Sec:     9760, Lr: 0.000035
2022-09-15 20:12:21,878 - INFO - joeynmt.training - Epoch   1, Step:   223800, Batch Loss:     2.558151, Batch Acc: 0.003933, Tokens per Sec:     9777, Lr: 0.000035
2022-09-15 20:12:33,781 - INFO - joeynmt.training - Epoch   1, Step:   223900, Batch Loss:     2.799790, Batch Acc: 0.004906, Tokens per Sec:     9693, Lr: 0.000035
2022-09-15 20:12:45,698 - INFO - joeynmt.training - Epoch   1, Step:   224000, Batch Loss:     2.801581, Batch Acc: 0.004626, Tokens per Sec:     9740, Lr: 0.000035
2022-09-15 20:12:45,699 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 20:21:59,999 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 20:22:00,000 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.70, loss:   2.56, ppl:  12.91, acc:   0.51, generation: 543.7864[sec], evaluation: 10.0020[sec]
2022-09-15 20:22:00,006 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 20:22:00,443 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/221000.ckpt
2022-09-15 20:22:00,471 - INFO - joeynmt.training - Example #0
2022-09-15 20:22:00,471 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 20:22:00,471 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 20:22:00,471 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 20:22:00,489 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 20:22:00,489 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 20:22:00,489 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 20:22:00,489 - INFO - joeynmt.training - Example #1
2022-09-15 20:22:00,489 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 20:22:00,489 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 20:22:00,490 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 20:22:00,505 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 20:22:00,505 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 20:22:00,505 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 20:22:00,505 - INFO - joeynmt.training - Example #2
2022-09-15 20:22:00,505 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 20:22:00,505 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 20:22:00,505 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 20:22:00,520 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 20:22:00,521 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 20:22:00,521 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 20:22:00,521 - INFO - joeynmt.training - Example #3
2022-09-15 20:22:00,521 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 20:22:00,521 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 20:22:00,521 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 20:22:00,536 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 20:22:00,536 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 20:22:00,536 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 20:22:12,524 - INFO - joeynmt.training - Epoch   1, Step:   224100, Batch Loss:     2.804202, Batch Acc: 0.003901, Tokens per Sec:     9052, Lr: 0.000035
2022-09-15 20:22:24,525 - INFO - joeynmt.training - Epoch   1, Step:   224200, Batch Loss:     2.753265, Batch Acc: 0.005480, Tokens per Sec:     9642, Lr: 0.000035
2022-09-15 20:22:36,424 - INFO - joeynmt.training - Epoch   1, Step:   224300, Batch Loss:     2.847317, Batch Acc: 0.003947, Tokens per Sec:     9412, Lr: 0.000035
2022-09-15 20:22:48,317 - INFO - joeynmt.training - Epoch   1, Step:   224400, Batch Loss:     2.979963, Batch Acc: 0.004211, Tokens per Sec:     9645, Lr: 0.000035
2022-09-15 20:23:00,273 - INFO - joeynmt.training - Epoch   1, Step:   224500, Batch Loss:     2.617342, Batch Acc: 0.005345, Tokens per Sec:     9625, Lr: 0.000035
2022-09-15 20:23:12,196 - INFO - joeynmt.training - Epoch   1, Step:   224600, Batch Loss:     2.559974, Batch Acc: 0.004410, Tokens per Sec:     9567, Lr: 0.000035
2022-09-15 20:23:24,137 - INFO - joeynmt.training - Epoch   1, Step:   224700, Batch Loss:     2.781312, Batch Acc: 0.003675, Tokens per Sec:     9481, Lr: 0.000035
2022-09-15 20:23:36,015 - INFO - joeynmt.training - Epoch   1, Step:   224800, Batch Loss:     2.700744, Batch Acc: 0.004820, Tokens per Sec:     9624, Lr: 0.000035
2022-09-15 20:23:47,929 - INFO - joeynmt.training - Epoch   1, Step:   224900, Batch Loss:     2.553850, Batch Acc: 0.004450, Tokens per Sec:     9582, Lr: 0.000035
2022-09-15 20:23:59,826 - INFO - joeynmt.training - Epoch   1, Step:   225000, Batch Loss:     2.661897, Batch Acc: 0.005241, Tokens per Sec:     9704, Lr: 0.000035
2022-09-15 20:23:59,826 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 20:33:04,139 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 20:33:04,141 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.68, loss:   2.56, ppl:  12.90, acc:   0.51, generation: 534.3173[sec], evaluation: 9.4882[sec]
2022-09-15 20:33:04,147 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 20:33:04,581 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/222000.ckpt
2022-09-15 20:33:04,609 - INFO - joeynmt.training - Example #0
2022-09-15 20:33:04,609 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 20:33:04,609 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 20:33:04,610 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 20:33:04,627 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 20:33:04,627 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 20:33:04,627 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 20:33:04,627 - INFO - joeynmt.training - Example #1
2022-09-15 20:33:04,627 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 20:33:04,627 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 20:33:04,627 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 20:33:04,642 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 20:33:04,642 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 20:33:04,642 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 20:33:04,642 - INFO - joeynmt.training - Example #2
2022-09-15 20:33:04,643 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 20:33:04,643 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 20:33:04,643 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 20:33:04,657 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 20:33:04,658 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 20:33:04,658 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 20:33:04,658 - INFO - joeynmt.training - Example #3
2022-09-15 20:33:04,658 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 20:33:04,658 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 20:33:04,658 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 20:33:04,673 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 20:33:04,673 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 20:33:04,673 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 20:33:16,690 - INFO - joeynmt.training - Epoch   1, Step:   225100, Batch Loss:     2.802369, Batch Acc: 0.004463, Tokens per Sec:     9234, Lr: 0.000035
2022-09-15 20:33:28,584 - INFO - joeynmt.training - Epoch   1, Step:   225200, Batch Loss:     2.684057, Batch Acc: 0.004234, Tokens per Sec:     9511, Lr: 0.000035
2022-09-15 20:33:40,517 - INFO - joeynmt.training - Epoch   1, Step:   225300, Batch Loss:     2.617140, Batch Acc: 0.004814, Tokens per Sec:     9593, Lr: 0.000035
2022-09-15 20:33:52,461 - INFO - joeynmt.training - Epoch   1, Step:   225400, Batch Loss:     2.784686, Batch Acc: 0.004151, Tokens per Sec:     9803, Lr: 0.000035
2022-09-15 20:34:04,385 - INFO - joeynmt.training - Epoch   1, Step:   225500, Batch Loss:     2.685458, Batch Acc: 0.003947, Tokens per Sec:     9541, Lr: 0.000035
2022-09-15 20:34:16,281 - INFO - joeynmt.training - Epoch   1, Step:   225600, Batch Loss:     2.845737, Batch Acc: 0.004245, Tokens per Sec:     9702, Lr: 0.000035
2022-09-15 20:34:28,177 - INFO - joeynmt.training - Epoch   1, Step:   225700, Batch Loss:     2.762042, Batch Acc: 0.004195, Tokens per Sec:     9478, Lr: 0.000035
2022-09-15 20:34:40,068 - INFO - joeynmt.training - Epoch   1, Step:   225800, Batch Loss:     2.803827, Batch Acc: 0.003836, Tokens per Sec:     9648, Lr: 0.000035
2022-09-15 20:34:52,349 - INFO - joeynmt.training - Epoch   1, Step:   225900, Batch Loss:     2.420882, Batch Acc: 0.004759, Tokens per Sec:     9395, Lr: 0.000035
2022-09-15 20:35:04,585 - INFO - joeynmt.training - Epoch   1, Step:   226000, Batch Loss:     2.882275, Batch Acc: 0.003632, Tokens per Sec:     9429, Lr: 0.000035
2022-09-15 20:35:04,586 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 20:44:36,624 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 20:44:36,626 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.72, loss:   2.55, ppl:  12.80, acc:   0.51, generation: 561.2401[sec], evaluation: 10.2861[sec]
2022-09-15 20:44:36,632 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 20:44:37,079 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/223000.ckpt
2022-09-15 20:44:37,107 - INFO - joeynmt.training - Example #0
2022-09-15 20:44:37,107 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 20:44:37,108 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 20:44:37,108 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 20:44:37,125 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 20:44:37,125 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 20:44:37,125 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 20:44:37,125 - INFO - joeynmt.training - Example #1
2022-09-15 20:44:37,125 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 20:44:37,125 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 20:44:37,125 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 20:44:37,140 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 20:44:37,140 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 20:44:37,140 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 20:44:37,140 - INFO - joeynmt.training - Example #2
2022-09-15 20:44:37,141 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 20:44:37,141 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 20:44:37,141 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 20:44:37,155 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 20:44:37,156 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 20:44:37,156 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 20:44:37,156 - INFO - joeynmt.training - Example #3
2022-09-15 20:44:37,156 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 20:44:37,156 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 20:44:37,156 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 20:44:37,171 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 20:44:37,171 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 20:44:37,171 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 20:44:49,471 - INFO - joeynmt.training - Epoch   1, Step:   226100, Batch Loss:     2.579680, Batch Acc: 0.003966, Tokens per Sec:     8936, Lr: 0.000035
2022-09-15 20:45:01,679 - INFO - joeynmt.training - Epoch   1, Step:   226200, Batch Loss:     2.909146, Batch Acc: 0.003711, Tokens per Sec:     9228, Lr: 0.000035
2022-09-15 20:45:13,922 - INFO - joeynmt.training - Epoch   1, Step:   226300, Batch Loss:     2.665752, Batch Acc: 0.004364, Tokens per Sec:     9227, Lr: 0.000035
2022-09-15 20:45:26,143 - INFO - joeynmt.training - Epoch   1, Step:   226400, Batch Loss:     2.681298, Batch Acc: 0.005068, Tokens per Sec:     9494, Lr: 0.000035
2022-09-15 20:45:38,358 - INFO - joeynmt.training - Epoch   1, Step:   226500, Batch Loss:     2.635115, Batch Acc: 0.004710, Tokens per Sec:     9403, Lr: 0.000035
2022-09-15 20:45:50,627 - INFO - joeynmt.training - Epoch   1, Step:   226600, Batch Loss:     2.882484, Batch Acc: 0.004196, Tokens per Sec:     9383, Lr: 0.000035
2022-09-15 20:46:02,854 - INFO - joeynmt.training - Epoch   1, Step:   226700, Batch Loss:     2.712909, Batch Acc: 0.003771, Tokens per Sec:     9348, Lr: 0.000035
2022-09-15 20:46:15,066 - INFO - joeynmt.training - Epoch   1, Step:   226800, Batch Loss:     2.683190, Batch Acc: 0.004496, Tokens per Sec:     9252, Lr: 0.000035
2022-09-15 20:46:27,296 - INFO - joeynmt.training - Epoch   1, Step:   226900, Batch Loss:     2.493483, Batch Acc: 0.004386, Tokens per Sec:     9490, Lr: 0.000035
2022-09-15 20:46:39,510 - INFO - joeynmt.training - Epoch   1, Step:   227000, Batch Loss:     2.304463, Batch Acc: 0.004217, Tokens per Sec:     9339, Lr: 0.000035
2022-09-15 20:46:39,511 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 20:56:02,603 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 20:56:02,605 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.80, loss:   2.55, ppl:  12.80, acc:   0.51, generation: 552.8546[sec], evaluation: 9.7354[sec]
2022-09-15 20:56:03,062 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/224000.ckpt
2022-09-15 20:56:03,090 - INFO - joeynmt.training - Example #0
2022-09-15 20:56:03,090 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 20:56:03,090 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 20:56:03,090 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 20:56:03,108 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 20:56:03,108 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 20:56:03,108 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 20:56:03,108 - INFO - joeynmt.training - Example #1
2022-09-15 20:56:03,108 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 20:56:03,108 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 20:56:03,108 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 20:56:03,123 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 20:56:03,123 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 20:56:03,124 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 20:56:03,124 - INFO - joeynmt.training - Example #2
2022-09-15 20:56:03,124 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 20:56:03,124 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 20:56:03,124 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 20:56:03,139 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 20:56:03,139 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 20:56:03,139 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 20:56:03,139 - INFO - joeynmt.training - Example #3
2022-09-15 20:56:03,139 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 20:56:03,139 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 20:56:03,139 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 20:56:03,154 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 20:56:03,155 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 20:56:03,155 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 20:56:15,461 - INFO - joeynmt.training - Epoch   1, Step:   227100, Batch Loss:     2.571863, Batch Acc: 0.005427, Tokens per Sec:     8933, Lr: 0.000035
2022-09-15 20:56:27,658 - INFO - joeynmt.training - Epoch   1, Step:   227200, Batch Loss:     2.447176, Batch Acc: 0.005798, Tokens per Sec:     9305, Lr: 0.000035
2022-09-15 20:56:39,903 - INFO - joeynmt.training - Epoch   1, Step:   227300, Batch Loss:     2.437534, Batch Acc: 0.005032, Tokens per Sec:     9380, Lr: 0.000035
2022-09-15 20:56:52,094 - INFO - joeynmt.training - Epoch   1, Step:   227400, Batch Loss:     2.665557, Batch Acc: 0.004696, Tokens per Sec:     9207, Lr: 0.000035
2022-09-15 20:57:04,344 - INFO - joeynmt.training - Epoch   1, Step:   227500, Batch Loss:     2.861506, Batch Acc: 0.004213, Tokens per Sec:     9551, Lr: 0.000035
2022-09-15 20:57:16,604 - INFO - joeynmt.training - Epoch   1, Step:   227600, Batch Loss:     2.449279, Batch Acc: 0.003833, Tokens per Sec:     9365, Lr: 0.000035
2022-09-15 20:57:28,801 - INFO - joeynmt.training - Epoch   1, Step:   227700, Batch Loss:     2.707118, Batch Acc: 0.004088, Tokens per Sec:     9246, Lr: 0.000035
2022-09-15 20:57:40,994 - INFO - joeynmt.training - Epoch   1, Step:   227800, Batch Loss:     2.607710, Batch Acc: 0.004297, Tokens per Sec:     9334, Lr: 0.000035
2022-09-15 20:57:52,989 - INFO - joeynmt.training - Epoch   1, Step:   227900, Batch Loss:     2.409172, Batch Acc: 0.004549, Tokens per Sec:     9585, Lr: 0.000035
2022-09-15 20:58:04,938 - INFO - joeynmt.training - Epoch   1, Step:   228000, Batch Loss:     2.772557, Batch Acc: 0.004753, Tokens per Sec:     9526, Lr: 0.000035
2022-09-15 20:58:04,938 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 21:07:14,025 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 21:07:14,027 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.69, loss:   2.55, ppl:  12.82, acc:   0.51, generation: 538.9672[sec], evaluation: 9.6171[sec]
2022-09-15 21:07:14,480 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/225000.ckpt
2022-09-15 21:07:14,508 - INFO - joeynmt.training - Example #0
2022-09-15 21:07:14,509 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 21:07:14,509 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 21:07:14,509 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-15 21:07:14,526 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 21:07:14,526 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 21:07:14,526 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 21:07:14,526 - INFO - joeynmt.training - Example #1
2022-09-15 21:07:14,527 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 21:07:14,527 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 21:07:14,527 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 21:07:14,542 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 21:07:14,542 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 21:07:14,542 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 21:07:14,542 - INFO - joeynmt.training - Example #2
2022-09-15 21:07:14,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 21:07:14,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 21:07:14,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 21:07:14,557 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 21:07:14,557 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 21:07:14,557 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 21:07:14,557 - INFO - joeynmt.training - Example #3
2022-09-15 21:07:14,557 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 21:07:14,558 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 21:07:14,558 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 21:07:14,573 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 21:07:14,573 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 21:07:14,573 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 21:07:26,859 - INFO - joeynmt.training - Epoch   1, Step:   228100, Batch Loss:     2.770482, Batch Acc: 0.003711, Tokens per Sec:     9033, Lr: 0.000035
2022-09-15 21:07:39,086 - INFO - joeynmt.training - Epoch   1, Step:   228200, Batch Loss:     2.623579, Batch Acc: 0.004156, Tokens per Sec:     9328, Lr: 0.000035
2022-09-15 21:07:51,335 - INFO - joeynmt.training - Epoch   1, Step:   228300, Batch Loss:     2.671275, Batch Acc: 0.004732, Tokens per Sec:     9454, Lr: 0.000035
2022-09-15 21:08:03,537 - INFO - joeynmt.training - Epoch   1, Step:   228400, Batch Loss:     2.583494, Batch Acc: 0.003923, Tokens per Sec:     9381, Lr: 0.000035
2022-09-15 21:08:15,744 - INFO - joeynmt.training - Epoch   1, Step:   228500, Batch Loss:     2.936419, Batch Acc: 0.003518, Tokens per Sec:     9362, Lr: 0.000035
2022-09-15 21:08:27,964 - INFO - joeynmt.training - Epoch   1, Step:   228600, Batch Loss:     2.645277, Batch Acc: 0.003828, Tokens per Sec:     9363, Lr: 0.000035
2022-09-15 21:08:40,202 - INFO - joeynmt.training - Epoch   1, Step:   228700, Batch Loss:     2.882881, Batch Acc: 0.004240, Tokens per Sec:     9367, Lr: 0.000035
2022-09-15 21:08:52,440 - INFO - joeynmt.training - Epoch   1, Step:   228800, Batch Loss:     2.465222, Batch Acc: 0.003920, Tokens per Sec:     9401, Lr: 0.000035
2022-09-15 21:09:04,662 - INFO - joeynmt.training - Epoch   1, Step:   228900, Batch Loss:     2.742768, Batch Acc: 0.004976, Tokens per Sec:     9389, Lr: 0.000035
2022-09-15 21:09:16,908 - INFO - joeynmt.training - Epoch   1, Step:   229000, Batch Loss:     2.811365, Batch Acc: 0.004592, Tokens per Sec:     9568, Lr: 0.000035
2022-09-15 21:09:16,909 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 21:18:41,804 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 21:18:41,805 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.77, loss:   2.54, ppl:  12.74, acc:   0.51, generation: 554.0838[sec], evaluation: 10.2927[sec]
2022-09-15 21:18:41,812 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 21:18:42,255 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/228000.ckpt
2022-09-15 21:18:42,283 - INFO - joeynmt.training - Example #0
2022-09-15 21:18:42,283 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 21:18:42,283 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 21:18:42,283 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 21:18:42,301 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 21:18:42,301 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 21:18:42,301 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 21:18:42,301 - INFO - joeynmt.training - Example #1
2022-09-15 21:18:42,301 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 21:18:42,302 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 21:18:42,302 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 21:18:42,317 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 21:18:42,317 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 21:18:42,317 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 21:18:42,317 - INFO - joeynmt.training - Example #2
2022-09-15 21:18:42,317 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 21:18:42,317 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 21:18:42,317 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 21:18:42,333 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 21:18:42,333 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 21:18:42,333 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 21:18:42,333 - INFO - joeynmt.training - Example #3
2022-09-15 21:18:42,333 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 21:18:42,333 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 21:18:42,333 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 21:18:42,348 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 21:18:42,348 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 21:18:42,348 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 21:18:54,625 - INFO - joeynmt.training - Epoch   1, Step:   229100, Batch Loss:     2.670770, Batch Acc: 0.004886, Tokens per Sec:     8818, Lr: 0.000035
2022-09-15 21:19:06,844 - INFO - joeynmt.training - Epoch   1, Step:   229200, Batch Loss:     2.361142, Batch Acc: 0.004754, Tokens per Sec:     9330, Lr: 0.000035
2022-09-15 21:19:19,111 - INFO - joeynmt.training - Epoch   1, Step:   229300, Batch Loss:     2.702835, Batch Acc: 0.004667, Tokens per Sec:     9451, Lr: 0.000035
2022-09-15 21:19:31,316 - INFO - joeynmt.training - Epoch   1, Step:   229400, Batch Loss:     2.787383, Batch Acc: 0.003998, Tokens per Sec:     9448, Lr: 0.000035
2022-09-15 21:19:43,502 - INFO - joeynmt.training - Epoch   1, Step:   229500, Batch Loss:     2.652217, Batch Acc: 0.005080, Tokens per Sec:     9288, Lr: 0.000035
2022-09-15 21:19:55,727 - INFO - joeynmt.training - Epoch   1, Step:   229600, Batch Loss:     2.483389, Batch Acc: 0.005553, Tokens per Sec:     9370, Lr: 0.000035
2022-09-15 21:20:07,918 - INFO - joeynmt.training - Epoch   1, Step:   229700, Batch Loss:     2.586143, Batch Acc: 0.004178, Tokens per Sec:     9188, Lr: 0.000035
2022-09-15 21:20:20,130 - INFO - joeynmt.training - Epoch   1, Step:   229800, Batch Loss:     2.580380, Batch Acc: 0.005206, Tokens per Sec:     9439, Lr: 0.000035
2022-09-15 21:20:32,354 - INFO - joeynmt.training - Epoch   1, Step:   229900, Batch Loss:     2.792303, Batch Acc: 0.004196, Tokens per Sec:     9437, Lr: 0.000035
2022-09-15 21:20:44,542 - INFO - joeynmt.training - Epoch   1, Step:   230000, Batch Loss:     2.625422, Batch Acc: 0.004956, Tokens per Sec:     9420, Lr: 0.000035
2022-09-15 21:20:44,542 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 21:29:56,237 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 21:29:56,238 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.82, loss:   2.54, ppl:  12.69, acc:   0.51, generation: 541.4490[sec], evaluation: 9.7427[sec]
2022-09-15 21:29:56,245 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 21:29:56,690 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/227000.ckpt
2022-09-15 21:29:56,718 - INFO - joeynmt.training - Example #0
2022-09-15 21:29:56,719 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 21:29:56,719 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 21:29:56,719 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-15 21:29:56,736 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 21:29:56,736 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 21:29:56,737 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-15 21:29:56,737 - INFO - joeynmt.training - Example #1
2022-09-15 21:29:56,737 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 21:29:56,737 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 21:29:56,737 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 21:29:56,752 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 21:29:56,752 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 21:29:56,752 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 21:29:56,752 - INFO - joeynmt.training - Example #2
2022-09-15 21:29:56,752 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 21:29:56,752 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 21:29:56,752 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 21:29:56,767 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 21:29:56,767 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 21:29:56,768 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 21:29:56,768 - INFO - joeynmt.training - Example #3
2022-09-15 21:29:56,768 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 21:29:56,768 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 21:29:56,768 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 21:29:56,783 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 21:29:56,783 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 21:29:56,783 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 21:30:09,118 - INFO - joeynmt.training - Epoch   1, Step:   230100, Batch Loss:     2.645893, Batch Acc: 0.004995, Tokens per Sec:     8958, Lr: 0.000035
2022-09-15 21:30:21,382 - INFO - joeynmt.training - Epoch   1, Step:   230200, Batch Loss:     2.599345, Batch Acc: 0.004194, Tokens per Sec:     9333, Lr: 0.000035
2022-09-15 21:30:33,581 - INFO - joeynmt.training - Epoch   1, Step:   230300, Batch Loss:     2.521271, Batch Acc: 0.005047, Tokens per Sec:     9323, Lr: 0.000035
2022-09-15 21:30:45,775 - INFO - joeynmt.training - Epoch   1, Step:   230400, Batch Loss:     2.515236, Batch Acc: 0.004040, Tokens per Sec:     9195, Lr: 0.000035
2022-09-15 21:30:57,984 - INFO - joeynmt.training - Epoch   1, Step:   230500, Batch Loss:     2.663272, Batch Acc: 0.004551, Tokens per Sec:     9198, Lr: 0.000035
2022-09-15 21:31:10,232 - INFO - joeynmt.training - Epoch   1, Step:   230600, Batch Loss:     2.648286, Batch Acc: 0.005214, Tokens per Sec:     9395, Lr: 0.000035
2022-09-15 21:31:22,430 - INFO - joeynmt.training - Epoch   1, Step:   230700, Batch Loss:     2.790946, Batch Acc: 0.003790, Tokens per Sec:     9322, Lr: 0.000035
2022-09-15 21:31:34,708 - INFO - joeynmt.training - Epoch   1, Step:   230800, Batch Loss:     2.645778, Batch Acc: 0.004893, Tokens per Sec:     9355, Lr: 0.000035
2022-09-15 21:31:46,974 - INFO - joeynmt.training - Epoch   1, Step:   230900, Batch Loss:     2.624224, Batch Acc: 0.005108, Tokens per Sec:     9257, Lr: 0.000035
2022-09-15 21:31:59,210 - INFO - joeynmt.training - Epoch   1, Step:   231000, Batch Loss:     2.550053, Batch Acc: 0.004611, Tokens per Sec:     9290, Lr: 0.000035
2022-09-15 21:31:59,210 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 21:41:21,052 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 21:41:21,053 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.85, loss:   2.54, ppl:  12.66, acc:   0.51, generation: 551.6562[sec], evaluation: 9.6719[sec]
2022-09-15 21:41:21,059 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 21:41:21,506 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/226000.ckpt
2022-09-15 21:41:21,534 - INFO - joeynmt.training - Example #0
2022-09-15 21:41:21,534 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 21:41:21,534 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 21:41:21,534 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-15 21:41:21,552 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 21:41:21,552 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 21:41:21,552 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 21:41:21,552 - INFO - joeynmt.training - Example #1
2022-09-15 21:41:21,552 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 21:41:21,552 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 21:41:21,552 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 21:41:21,567 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 21:41:21,567 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 21:41:21,567 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 21:41:21,567 - INFO - joeynmt.training - Example #2
2022-09-15 21:41:21,568 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 21:41:21,568 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 21:41:21,568 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 21:41:21,583 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 21:41:21,583 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 21:41:21,583 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 21:41:21,583 - INFO - joeynmt.training - Example #3
2022-09-15 21:41:21,583 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 21:41:21,583 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 21:41:21,583 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 21:41:21,598 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 21:41:21,598 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 21:41:21,598 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 21:41:33,880 - INFO - joeynmt.training - Epoch   1, Step:   231100, Batch Loss:     2.550815, Batch Acc: 0.005642, Tokens per Sec:     8985, Lr: 0.000035
2022-09-15 21:41:46,051 - INFO - joeynmt.training - Epoch   1, Step:   231200, Batch Loss:     2.658967, Batch Acc: 0.004551, Tokens per Sec:     9334, Lr: 0.000035
2022-09-15 21:41:58,317 - INFO - joeynmt.training - Epoch   1, Step:   231300, Batch Loss:     2.661066, Batch Acc: 0.004615, Tokens per Sec:     9329, Lr: 0.000035
2022-09-15 21:42:10,491 - INFO - joeynmt.training - Epoch   1, Step:   231400, Batch Loss:     2.678017, Batch Acc: 0.004775, Tokens per Sec:     9409, Lr: 0.000035
2022-09-15 21:42:22,713 - INFO - joeynmt.training - Epoch   1, Step:   231500, Batch Loss:     2.637925, Batch Acc: 0.004419, Tokens per Sec:     9537, Lr: 0.000035
2022-09-15 21:42:34,941 - INFO - joeynmt.training - Epoch   1, Step:   231600, Batch Loss:     2.831815, Batch Acc: 0.004766, Tokens per Sec:     9351, Lr: 0.000035
2022-09-15 21:42:47,183 - INFO - joeynmt.training - Epoch   1, Step:   231700, Batch Loss:     2.770598, Batch Acc: 0.004409, Tokens per Sec:     9375, Lr: 0.000035
2022-09-15 21:42:59,405 - INFO - joeynmt.training - Epoch   1, Step:   231800, Batch Loss:     2.974834, Batch Acc: 0.003733, Tokens per Sec:     9446, Lr: 0.000035
2022-09-15 21:43:11,664 - INFO - joeynmt.training - Epoch   1, Step:   231900, Batch Loss:     2.673681, Batch Acc: 0.005361, Tokens per Sec:     9205, Lr: 0.000035
2022-09-15 21:43:23,933 - INFO - joeynmt.training - Epoch   1, Step:   232000, Batch Loss:     2.545948, Batch Acc: 0.005012, Tokens per Sec:     9416, Lr: 0.000035
2022-09-15 21:43:23,933 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 21:52:32,216 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 21:52:32,224 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-15 21:52:32,671 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/229000.ckpt
2022-09-15 21:52:32,699 - INFO - joeynmt.training - Example #0
2022-09-15 21:52:32,700 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-15 21:52:32,700 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-15 21:52:32,700 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-15 21:52:32,717 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-15 21:52:32,717 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-15 21:52:32,717 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-15 21:52:32,718 - INFO - joeynmt.training - Example #1
2022-09-15 21:52:32,718 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-15 21:52:32,718 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-15 21:52:32,718 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-15 21:52:32,733 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-15 21:52:32,733 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-15 21:52:32,733 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-15 21:52:32,733 - INFO - joeynmt.training - Example #2
2022-09-15 21:52:32,733 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-15 21:52:32,733 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-15 21:52:32,733 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-15 21:52:32,748 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-15 21:52:32,748 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-15 21:52:32,748 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-15 21:52:32,749 - INFO - joeynmt.training - Example #3
2022-09-15 21:52:32,749 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-15 21:52:32,749 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-15 21:52:32,749 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-15 21:52:32,764 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-15 21:52:32,764 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-15 21:52:32,764 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-15 21:52:40,716 - INFO - joeynmt.training - Epoch   1: total training loss 39134.02
2022-09-15 21:52:40,716 - INFO - joeynmt.training - Training ended after   1 epochs.
2022-09-15 21:52:40,716 - INFO - joeynmt.training - Best validation result (greedy) at step   232000:  12.65 ppl.
2022-09-15 21:52:40,717 - INFO - joeynmt.training - Loading from ckpt file: /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt
2022-09-15 21:52:40,733 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 21:52:40,733 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 21:52:41,070 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 21:52:41,075 - INFO - joeynmt.model - Total params: 19302144
2022-09-15 21:52:41,076 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2022-09-15 21:52:41,360 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 21:52:41,424 - INFO - joeynmt.prediction - Decoding on dev set...
2022-09-15 21:52:41,425 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:19:00,146 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 22:19:00,147 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  23.22, generation: 1568.6360[sec], evaluation: 9.5769[sec]
2022-09-15 22:19:00,208 - INFO - joeynmt.prediction - Translations saved to: /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/00232000.hyps.dev.
2022-09-15 22:19:00,209 - INFO - joeynmt.prediction - Decoding on test set...
2022-09-15 22:19:00,209 - INFO - joeynmt.prediction - Predicting 40858 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:44:30,052 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-15 22:44:30,054 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  22.53, generation: 1518.9525[sec], evaluation: 10.3801[sec]
2022-09-15 22:44:30,123 - INFO - joeynmt.prediction - Translations saved to: /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/00232000.hyps.test.
2022-09-15 22:44:30,138 - INFO - joeynmt.training - ACTIVE LEARNING MODEL END - LEAST CONFIDENCE 1
2022-09-15 22:44:30,139 - INFO - joeynmt.training - ACTIVE LEARNING MODEL START - LEAST CONFIDENCE 2
2022-09-15 22:44:30,141 - INFO - joeynmt.training - Random Indices picked: [121958 146867 131932 365838 259178 119879 110268 207892  54886 137337] length: 26153
2022-09-15 22:44:30,141 - INFO - joeynmt.training - Processing Predictions on Batch 0/103
2022-09-15 22:44:34,546 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:44:34,546 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:44:34,896 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:44:35,194 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:44:35,325 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:44:35,325 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:44:35,326 - INFO - joeynmt.prediction - Predicting 3 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:44:35,855 - INFO - joeynmt.prediction - Generation took 0.5281[sec]. (No references given)
2022-09-15 22:44:35,869 - INFO - joeynmt.training - Processing Predictions on Batch 1/103
2022-09-15 22:44:40,332 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:44:40,332 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:44:40,670 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:44:40,962 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:44:41,094 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:44:41,094 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:44:41,096 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:44:41,738 - INFO - joeynmt.prediction - Generation took 0.6398[sec]. (No references given)
2022-09-15 22:44:41,751 - INFO - joeynmt.training - Processing Predictions on Batch 2/103
2022-09-15 22:44:46,234 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:44:46,234 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:44:46,571 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:44:46,846 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:44:46,977 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:44:46,977 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:44:46,980 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:44:47,697 - INFO - joeynmt.prediction - Generation took 0.7153[sec]. (No references given)
2022-09-15 22:44:47,708 - INFO - joeynmt.training - Processing Predictions on Batch 3/103
2022-09-15 22:44:52,186 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:44:52,186 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:44:52,523 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:44:52,804 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:44:52,935 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:44:52,935 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:44:52,937 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:44:53,628 - INFO - joeynmt.prediction - Generation took 0.6891[sec]. (No references given)
2022-09-15 22:44:53,641 - INFO - joeynmt.training - Processing Predictions on Batch 4/103
2022-09-15 22:44:58,108 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:44:58,108 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:44:58,446 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:44:58,742 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:44:58,873 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:44:58,873 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:44:58,875 - INFO - joeynmt.prediction - Predicting 2 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:44:59,577 - INFO - joeynmt.prediction - Generation took 0.7006[sec]. (No references given)
2022-09-15 22:44:59,590 - INFO - joeynmt.training - Processing Predictions on Batch 5/103
2022-09-15 22:45:04,070 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:45:04,070 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:45:04,408 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:45:04,700 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:45:04,832 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:04,832 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:04,835 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:45:05,562 - INFO - joeynmt.prediction - Generation took 0.7244[sec]. (No references given)
2022-09-15 22:45:05,575 - INFO - joeynmt.training - Processing Predictions on Batch 6/103
2022-09-15 22:45:10,064 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:45:10,065 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:45:10,403 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:45:10,682 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:45:10,815 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:10,816 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:10,817 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:45:11,508 - INFO - joeynmt.prediction - Generation took 0.6890[sec]. (No references given)
2022-09-15 22:45:11,519 - INFO - joeynmt.training - Processing Predictions on Batch 7/103
2022-09-15 22:45:16,052 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:45:16,052 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:45:16,391 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:45:16,672 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:45:16,803 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:16,803 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:16,804 - INFO - joeynmt.prediction - Predicting 2 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:45:17,157 - INFO - joeynmt.prediction - Generation took 0.3515[sec]. (No references given)
2022-09-15 22:45:17,170 - INFO - joeynmt.training - Processing Predictions on Batch 8/103
2022-09-15 22:45:21,724 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:45:21,724 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:45:22,061 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:45:22,355 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:45:22,485 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:22,485 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:22,488 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:45:23,224 - INFO - joeynmt.prediction - Generation took 0.7346[sec]. (No references given)
2022-09-15 22:45:23,237 - INFO - joeynmt.training - Processing Predictions on Batch 9/103
2022-09-15 22:45:27,815 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:45:27,815 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:45:28,152 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:45:28,429 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:45:28,559 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:28,559 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:28,562 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:45:29,190 - INFO - joeynmt.prediction - Generation took 0.6262[sec]. (No references given)
2022-09-15 22:45:29,201 - INFO - joeynmt.training - Processing Predictions on Batch 10/103
2022-09-15 22:45:33,783 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:45:33,783 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:45:34,121 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:45:34,393 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:45:34,524 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:34,525 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:34,528 - INFO - joeynmt.prediction - Predicting 11 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:45:35,646 - INFO - joeynmt.prediction - Generation took 1.1156[sec]. (No references given)
2022-09-15 22:45:35,656 - INFO - joeynmt.training - Processing Predictions on Batch 11/103
2022-09-15 22:45:40,248 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:45:40,248 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:45:40,584 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:45:40,853 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:45:40,984 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:40,984 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:40,987 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:45:41,623 - INFO - joeynmt.prediction - Generation took 0.6345[sec]. (No references given)
2022-09-15 22:45:41,634 - INFO - joeynmt.training - Processing Predictions on Batch 12/103
2022-09-15 22:45:46,239 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:45:46,240 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:45:46,576 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:45:46,848 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:45:46,978 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:46,978 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:46,980 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:45:47,492 - INFO - joeynmt.prediction - Generation took 0.5095[sec]. (No references given)
2022-09-15 22:45:47,502 - INFO - joeynmt.training - Processing Predictions on Batch 13/103
2022-09-15 22:45:52,128 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:45:52,128 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:45:52,465 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:45:52,742 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:45:52,872 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:52,872 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:52,874 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:45:53,484 - INFO - joeynmt.prediction - Generation took 0.6083[sec]. (No references given)
2022-09-15 22:45:53,497 - INFO - joeynmt.training - Processing Predictions on Batch 14/103
2022-09-15 22:45:58,119 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:45:58,119 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:45:58,457 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:45:58,746 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:45:58,877 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:58,877 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:45:58,879 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:46:00,442 - INFO - joeynmt.prediction - Generation took 1.5610[sec]. (No references given)
2022-09-15 22:46:00,455 - INFO - joeynmt.training - Processing Predictions on Batch 15/103
2022-09-15 22:46:05,083 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:46:05,085 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:46:05,422 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:46:05,714 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:46:05,844 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:46:05,844 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:46:05,847 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:46:06,629 - INFO - joeynmt.prediction - Generation took 0.7797[sec]. (No references given)
2022-09-15 22:46:06,641 - INFO - joeynmt.training - Processing Predictions on Batch 16/103
2022-09-15 22:46:11,265 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:46:11,265 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:46:11,605 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:46:11,887 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:46:12,019 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:46:12,019 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:46:12,021 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:46:12,684 - INFO - joeynmt.prediction - Generation took 0.6610[sec]. (No references given)
2022-09-15 22:46:12,694 - INFO - joeynmt.training - Processing Predictions on Batch 17/103
2022-09-15 22:46:17,298 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:46:17,298 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:46:17,635 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:46:17,909 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:46:18,039 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:46:18,039 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:46:18,041 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:46:18,692 - INFO - joeynmt.prediction - Generation took 0.6493[sec]. (No references given)
2022-09-15 22:46:18,703 - INFO - joeynmt.training - Processing Predictions on Batch 18/103
2022-09-15 22:46:23,290 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:46:23,291 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:46:23,628 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:46:23,905 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:46:24,036 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:46:24,037 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:46:24,038 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:46:24,442 - INFO - joeynmt.prediction - Generation took 0.4019[sec]. (No references given)
2022-09-15 22:46:24,454 - INFO - joeynmt.training - Processing Predictions on Batch 19/103
2022-09-15 22:46:29,033 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:46:29,033 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:46:29,369 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:46:29,661 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:46:29,791 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:46:29,792 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:46:29,794 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:46:31,456 - INFO - joeynmt.prediction - Generation took 1.6590[sec]. (No references given)
2022-09-15 22:46:31,468 - INFO - joeynmt.training - Processing Predictions on Batch 20/103
2022-09-15 22:46:36,035 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:46:36,036 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:46:36,372 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:46:36,664 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:46:36,795 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:46:36,795 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:46:36,798 - INFO - joeynmt.prediction - Predicting 12 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:46:38,491 - INFO - joeynmt.prediction - Generation took 1.6916[sec]. (No references given)
2022-09-15 22:46:38,504 - INFO - joeynmt.training - Processing Predictions on Batch 21/103
2022-09-15 22:46:43,054 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:46:43,054 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:46:43,390 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:46:43,667 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:46:43,798 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:46:43,798 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:46:43,799 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:46:44,232 - INFO - joeynmt.prediction - Generation took 0.4304[sec]. (No references given)
2022-09-15 22:46:44,242 - INFO - joeynmt.training - Processing Predictions on Batch 22/103
2022-09-15 22:46:48,786 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:46:48,786 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:46:49,124 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:46:49,396 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:46:49,527 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:46:49,527 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:46:49,529 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:46:51,211 - INFO - joeynmt.prediction - Generation took 1.6802[sec]. (No references given)
2022-09-15 22:46:51,222 - INFO - joeynmt.training - Processing Predictions on Batch 23/103
2022-09-15 22:46:55,741 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:46:55,741 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:46:56,077 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:46:56,355 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:46:56,485 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:46:56,485 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:46:56,486 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:46:56,907 - INFO - joeynmt.prediction - Generation took 0.4193[sec]. (No references given)
2022-09-15 22:46:56,920 - INFO - joeynmt.training - Processing Predictions on Batch 24/103
2022-09-15 22:47:01,448 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:47:01,448 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:47:01,786 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:47:02,079 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:47:02,212 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:02,212 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:02,217 - INFO - joeynmt.prediction - Predicting 17 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:47:03,937 - INFO - joeynmt.prediction - Generation took 1.7176[sec]. (No references given)
2022-09-15 22:47:03,950 - INFO - joeynmt.training - Processing Predictions on Batch 25/103
2022-09-15 22:47:08,465 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:47:08,465 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:47:08,804 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:47:09,095 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:47:09,227 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:09,228 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:09,229 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:47:10,076 - INFO - joeynmt.prediction - Generation took 0.8447[sec]. (No references given)
2022-09-15 22:47:10,089 - INFO - joeynmt.training - Processing Predictions on Batch 26/103
2022-09-15 22:47:14,605 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:47:14,605 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:47:14,942 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:47:15,232 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:47:15,364 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:15,364 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:15,365 - INFO - joeynmt.prediction - Predicting 3 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:47:15,694 - INFO - joeynmt.prediction - Generation took 0.3278[sec]. (No references given)
2022-09-15 22:47:15,706 - INFO - joeynmt.training - Processing Predictions on Batch 27/103
2022-09-15 22:47:20,228 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:47:20,228 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:47:20,564 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:47:21,377 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:47:21,507 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:21,508 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:21,510 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:47:22,034 - INFO - joeynmt.prediction - Generation took 0.5228[sec]. (No references given)
2022-09-15 22:47:22,047 - INFO - joeynmt.training - Processing Predictions on Batch 28/103
2022-09-15 22:47:26,545 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:47:26,545 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:47:26,882 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:47:27,173 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:47:27,306 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:27,306 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:27,309 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:47:27,949 - INFO - joeynmt.prediction - Generation took 0.6378[sec]. (No references given)
2022-09-15 22:47:27,962 - INFO - joeynmt.training - Processing Predictions on Batch 29/103
2022-09-15 22:47:32,459 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:47:32,459 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:47:32,797 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:47:33,072 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:47:33,203 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:33,203 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:33,208 - INFO - joeynmt.prediction - Predicting 12 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:47:34,306 - INFO - joeynmt.prediction - Generation took 1.0961[sec]. (No references given)
2022-09-15 22:47:34,317 - INFO - joeynmt.training - Processing Predictions on Batch 30/103
2022-09-15 22:47:38,833 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:47:38,834 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:47:39,170 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:47:39,446 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:47:39,576 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:39,576 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:39,577 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:47:40,133 - INFO - joeynmt.prediction - Generation took 0.5532[sec]. (No references given)
2022-09-15 22:47:40,143 - INFO - joeynmt.training - Processing Predictions on Batch 31/103
2022-09-15 22:47:44,639 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:47:44,639 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:47:44,975 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:47:45,253 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:47:45,384 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:45,384 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:45,385 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:47:46,113 - INFO - joeynmt.prediction - Generation took 0.7255[sec]. (No references given)
2022-09-15 22:47:46,125 - INFO - joeynmt.training - Processing Predictions on Batch 32/103
2022-09-15 22:47:50,617 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:47:50,617 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:47:50,955 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:47:51,232 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:47:51,363 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:51,363 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:51,367 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:47:52,772 - INFO - joeynmt.prediction - Generation took 1.4024[sec]. (No references given)
2022-09-15 22:47:52,782 - INFO - joeynmt.training - Processing Predictions on Batch 33/103
2022-09-15 22:47:57,290 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:47:57,290 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:47:57,627 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:47:57,901 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:47:58,031 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:58,032 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:47:58,034 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:47:58,637 - INFO - joeynmt.prediction - Generation took 0.6012[sec]. (No references given)
2022-09-15 22:47:58,648 - INFO - joeynmt.training - Processing Predictions on Batch 34/103
2022-09-15 22:48:03,154 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:48:03,154 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:48:03,490 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:48:03,768 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:48:03,898 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:03,898 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:03,900 - INFO - joeynmt.prediction - Predicting 13 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:48:04,384 - INFO - joeynmt.prediction - Generation took 0.4815[sec]. (No references given)
2022-09-15 22:48:04,397 - INFO - joeynmt.training - Processing Predictions on Batch 35/103
2022-09-15 22:48:08,904 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:48:08,905 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:48:09,242 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:48:09,520 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:48:09,651 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:09,651 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:09,652 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:48:10,265 - INFO - joeynmt.prediction - Generation took 0.6114[sec]. (No references given)
2022-09-15 22:48:10,275 - INFO - joeynmt.training - Processing Predictions on Batch 36/103
2022-09-15 22:48:14,779 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:48:14,780 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:48:15,117 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:48:15,399 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:48:15,531 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:15,531 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:15,533 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:48:15,925 - INFO - joeynmt.prediction - Generation took 0.3894[sec]. (No references given)
2022-09-15 22:48:15,936 - INFO - joeynmt.training - Processing Predictions on Batch 37/103
2022-09-15 22:48:20,428 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:48:20,429 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:48:20,765 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:48:21,058 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:48:21,188 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:21,188 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:21,190 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:48:21,961 - INFO - joeynmt.prediction - Generation took 0.7692[sec]. (No references given)
2022-09-15 22:48:21,974 - INFO - joeynmt.training - Processing Predictions on Batch 38/103
2022-09-15 22:48:26,481 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:48:26,482 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:48:26,818 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:48:27,103 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:48:27,232 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:27,232 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:27,235 - INFO - joeynmt.prediction - Predicting 11 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:48:28,842 - INFO - joeynmt.prediction - Generation took 1.6040[sec]. (No references given)
2022-09-15 22:48:28,852 - INFO - joeynmt.training - Processing Predictions on Batch 39/103
2022-09-15 22:48:33,356 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:48:33,356 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:48:33,693 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:48:33,963 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:48:34,094 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:34,094 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:34,097 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:48:35,585 - INFO - joeynmt.prediction - Generation took 1.4855[sec]. (No references given)
2022-09-15 22:48:35,596 - INFO - joeynmt.training - Processing Predictions on Batch 40/103
2022-09-15 22:48:40,105 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:48:40,106 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:48:40,443 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:48:40,719 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:48:40,850 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:40,850 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:40,853 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:48:41,492 - INFO - joeynmt.prediction - Generation took 0.6369[sec]. (No references given)
2022-09-15 22:48:41,505 - INFO - joeynmt.training - Processing Predictions on Batch 41/103
2022-09-15 22:48:46,013 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:48:46,014 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:48:46,350 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:48:46,627 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:48:46,757 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:46,757 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:46,758 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:48:47,536 - INFO - joeynmt.prediction - Generation took 0.7750[sec]. (No references given)
2022-09-15 22:48:47,546 - INFO - joeynmt.training - Processing Predictions on Batch 42/103
2022-09-15 22:48:52,059 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:48:52,059 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:48:52,396 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:48:52,678 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:48:52,808 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:52,808 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:52,810 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:48:53,515 - INFO - joeynmt.prediction - Generation took 0.7032[sec]. (No references given)
2022-09-15 22:48:53,527 - INFO - joeynmt.training - Processing Predictions on Batch 43/103
2022-09-15 22:48:58,053 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:48:58,053 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:48:58,391 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:48:58,669 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:48:58,800 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:58,800 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:48:58,804 - INFO - joeynmt.prediction - Predicting 12 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:48:59,850 - INFO - joeynmt.prediction - Generation took 1.0435[sec]. (No references given)
2022-09-15 22:48:59,860 - INFO - joeynmt.training - Processing Predictions on Batch 44/103
2022-09-15 22:49:04,377 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:49:04,377 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:49:04,715 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:49:04,987 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:49:05,117 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:05,118 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:05,120 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:49:06,672 - INFO - joeynmt.prediction - Generation took 1.5499[sec]. (No references given)
2022-09-15 22:49:06,682 - INFO - joeynmt.training - Processing Predictions on Batch 45/103
2022-09-15 22:49:11,198 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:49:11,199 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:49:11,535 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:49:11,813 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:49:11,941 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:11,942 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:11,945 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:49:12,499 - INFO - joeynmt.prediction - Generation took 0.5525[sec]. (No references given)
2022-09-15 22:49:12,512 - INFO - joeynmt.training - Processing Predictions on Batch 46/103
2022-09-15 22:49:17,019 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:49:17,019 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:49:17,355 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:49:17,632 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:49:17,763 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:17,763 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:17,765 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:49:18,458 - INFO - joeynmt.prediction - Generation took 0.6905[sec]. (No references given)
2022-09-15 22:49:18,469 - INFO - joeynmt.training - Processing Predictions on Batch 47/103
2022-09-15 22:49:22,973 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:49:22,974 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:49:23,312 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:49:23,594 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:49:23,725 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:23,725 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:23,727 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:49:24,252 - INFO - joeynmt.prediction - Generation took 0.5235[sec]. (No references given)
2022-09-15 22:49:24,265 - INFO - joeynmt.training - Processing Predictions on Batch 48/103
2022-09-15 22:49:28,767 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:49:28,767 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:49:29,104 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:49:29,379 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:49:29,509 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:29,509 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:29,512 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:49:30,270 - INFO - joeynmt.prediction - Generation took 0.7553[sec]. (No references given)
2022-09-15 22:49:30,280 - INFO - joeynmt.training - Processing Predictions on Batch 49/103
2022-09-15 22:49:34,777 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:49:34,777 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:49:35,113 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:49:35,396 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:49:35,527 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:35,528 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:35,530 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:49:36,213 - INFO - joeynmt.prediction - Generation took 0.6804[sec]. (No references given)
2022-09-15 22:49:36,225 - INFO - joeynmt.training - Processing Predictions on Batch 50/103
2022-09-15 22:49:40,727 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:49:40,727 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:49:41,064 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:49:41,342 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:49:41,473 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:41,473 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:41,475 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:49:42,373 - INFO - joeynmt.prediction - Generation took 0.8952[sec]. (No references given)
2022-09-15 22:49:42,383 - INFO - joeynmt.training - Processing Predictions on Batch 51/103
2022-09-15 22:49:46,881 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:49:46,881 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:49:47,219 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:49:47,500 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:49:47,630 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:47,630 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:47,631 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:49:48,292 - INFO - joeynmt.prediction - Generation took 0.6588[sec]. (No references given)
2022-09-15 22:49:48,304 - INFO - joeynmt.training - Processing Predictions on Batch 52/103
2022-09-15 22:49:52,804 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:49:52,804 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:49:53,141 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:49:53,430 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:49:53,560 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:53,560 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:53,563 - INFO - joeynmt.prediction - Predicting 11 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:49:54,220 - INFO - joeynmt.prediction - Generation took 0.6548[sec]. (No references given)
2022-09-15 22:49:54,233 - INFO - joeynmt.training - Processing Predictions on Batch 53/103
2022-09-15 22:49:58,733 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:49:58,733 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:49:59,069 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:49:59,366 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:49:59,495 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:59,495 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:49:59,498 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:50:00,138 - INFO - joeynmt.prediction - Generation took 0.6378[sec]. (No references given)
2022-09-15 22:50:00,151 - INFO - joeynmt.training - Processing Predictions on Batch 54/103
2022-09-15 22:50:04,651 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:50:04,651 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:50:04,988 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:50:05,266 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:50:05,396 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:05,396 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:05,399 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:50:06,116 - INFO - joeynmt.prediction - Generation took 0.7147[sec]. (No references given)
2022-09-15 22:50:06,126 - INFO - joeynmt.training - Processing Predictions on Batch 55/103
2022-09-15 22:50:10,619 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:50:10,620 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:50:10,959 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:50:11,237 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:50:11,368 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:11,369 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:11,371 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:50:11,826 - INFO - joeynmt.prediction - Generation took 0.4529[sec]. (No references given)
2022-09-15 22:50:11,836 - INFO - joeynmt.training - Processing Predictions on Batch 56/103
2022-09-15 22:50:16,332 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:50:16,332 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:50:16,669 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:50:16,935 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:50:17,064 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:17,064 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:17,066 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:50:17,771 - INFO - joeynmt.prediction - Generation took 0.7032[sec]. (No references given)
2022-09-15 22:50:17,782 - INFO - joeynmt.training - Processing Predictions on Batch 57/103
2022-09-15 22:50:22,277 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:50:22,277 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:50:22,614 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:50:22,885 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:50:23,013 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:23,013 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:23,014 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:50:23,428 - INFO - joeynmt.prediction - Generation took 0.4121[sec]. (No references given)
2022-09-15 22:50:23,438 - INFO - joeynmt.training - Processing Predictions on Batch 58/103
2022-09-15 22:50:27,925 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:50:27,925 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:50:28,262 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:50:28,540 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:50:28,668 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:28,669 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:28,670 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:50:29,040 - INFO - joeynmt.prediction - Generation took 0.3687[sec]. (No references given)
2022-09-15 22:50:29,052 - INFO - joeynmt.training - Processing Predictions on Batch 59/103
2022-09-15 22:50:33,536 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:50:33,536 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:50:33,874 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:50:34,163 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:50:34,810 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:34,810 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:34,812 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:50:35,355 - INFO - joeynmt.prediction - Generation took 0.5409[sec]. (No references given)
2022-09-15 22:50:35,368 - INFO - joeynmt.training - Processing Predictions on Batch 60/103
2022-09-15 22:50:39,825 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:50:39,825 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:50:40,162 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:50:40,454 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:50:40,583 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:40,583 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:40,587 - INFO - joeynmt.prediction - Predicting 16 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:50:42,305 - INFO - joeynmt.prediction - Generation took 1.7151[sec]. (No references given)
2022-09-15 22:50:42,319 - INFO - joeynmt.training - Processing Predictions on Batch 61/103
2022-09-15 22:50:46,772 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:50:46,772 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:50:47,111 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:50:47,391 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:50:47,521 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:47,522 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:47,524 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:50:48,243 - INFO - joeynmt.prediction - Generation took 0.7168[sec]. (No references given)
2022-09-15 22:50:48,254 - INFO - joeynmt.training - Processing Predictions on Batch 62/103
2022-09-15 22:50:52,718 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:50:52,718 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:50:53,057 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:50:53,334 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:50:53,464 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:53,464 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:53,466 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:50:54,154 - INFO - joeynmt.prediction - Generation took 0.6866[sec]. (No references given)
2022-09-15 22:50:54,165 - INFO - joeynmt.training - Processing Predictions on Batch 63/103
2022-09-15 22:50:58,650 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:50:58,650 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:50:58,989 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:50:59,268 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:50:59,397 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:59,397 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:50:59,400 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:51:00,054 - INFO - joeynmt.prediction - Generation took 0.6520[sec]. (No references given)
2022-09-15 22:51:00,067 - INFO - joeynmt.training - Processing Predictions on Batch 64/103
2022-09-15 22:51:04,570 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:51:04,570 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:51:04,907 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:51:05,184 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:51:05,312 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:05,312 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:05,314 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:51:05,984 - INFO - joeynmt.prediction - Generation took 0.6673[sec]. (No references given)
2022-09-15 22:51:05,994 - INFO - joeynmt.training - Processing Predictions on Batch 65/103
2022-09-15 22:51:10,477 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:51:10,478 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:51:10,815 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:51:11,092 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:51:11,220 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:11,220 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:11,222 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:51:11,720 - INFO - joeynmt.prediction - Generation took 0.4966[sec]. (No references given)
2022-09-15 22:51:11,731 - INFO - joeynmt.training - Processing Predictions on Batch 66/103
2022-09-15 22:51:16,212 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:51:16,212 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:51:16,551 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:51:16,826 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:51:16,956 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:16,956 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:16,958 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:51:18,586 - INFO - joeynmt.prediction - Generation took 1.6267[sec]. (No references given)
2022-09-15 22:51:18,599 - INFO - joeynmt.training - Processing Predictions on Batch 67/103
2022-09-15 22:51:23,089 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:51:23,089 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:51:23,426 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:51:23,718 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:51:23,845 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:23,846 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:23,847 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:51:24,251 - INFO - joeynmt.prediction - Generation took 0.4025[sec]. (No references given)
2022-09-15 22:51:24,263 - INFO - joeynmt.training - Processing Predictions on Batch 68/103
2022-09-15 22:51:28,750 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:51:28,750 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:51:29,086 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:51:29,371 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:51:29,498 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:29,498 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:29,501 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:51:30,212 - INFO - joeynmt.prediction - Generation took 0.7092[sec]. (No references given)
2022-09-15 22:51:30,223 - INFO - joeynmt.training - Processing Predictions on Batch 69/103
2022-09-15 22:51:34,725 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:51:34,725 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:51:35,062 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:51:35,340 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:51:35,469 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:35,470 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:35,471 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:51:36,225 - INFO - joeynmt.prediction - Generation took 0.7510[sec]. (No references given)
2022-09-15 22:51:36,237 - INFO - joeynmt.training - Processing Predictions on Batch 70/103
2022-09-15 22:51:40,740 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:51:40,740 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:51:41,080 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:51:41,371 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:51:41,499 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:41,499 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:41,502 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:51:42,084 - INFO - joeynmt.prediction - Generation took 0.5797[sec]. (No references given)
2022-09-15 22:51:42,096 - INFO - joeynmt.training - Processing Predictions on Batch 71/103
2022-09-15 22:51:46,596 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:51:46,597 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:51:46,934 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:51:47,211 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:51:47,337 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:47,338 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:47,340 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:51:48,062 - INFO - joeynmt.prediction - Generation took 0.7197[sec]. (No references given)
2022-09-15 22:51:48,072 - INFO - joeynmt.training - Processing Predictions on Batch 72/103
2022-09-15 22:51:52,585 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:51:52,585 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:51:52,922 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:51:53,204 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:51:53,331 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:53,331 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:53,333 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:51:53,926 - INFO - joeynmt.prediction - Generation took 0.5916[sec]. (No references given)
2022-09-15 22:51:53,939 - INFO - joeynmt.training - Processing Predictions on Batch 73/103
2022-09-15 22:51:58,459 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:51:58,459 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:51:58,796 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:51:59,074 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:51:59,201 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:59,201 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:51:59,202 - INFO - joeynmt.prediction - Predicting 2 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:51:59,543 - INFO - joeynmt.prediction - Generation took 0.3396[sec]. (No references given)
2022-09-15 22:51:59,553 - INFO - joeynmt.training - Processing Predictions on Batch 74/103
2022-09-15 22:52:04,066 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:52:04,067 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:52:04,405 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:52:04,685 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:52:04,813 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:52:04,813 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:52:04,815 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:52:05,471 - INFO - joeynmt.prediction - Generation took 0.6537[sec]. (No references given)
2022-09-15 22:52:05,483 - INFO - joeynmt.training - Processing Predictions on Batch 75/103
2022-09-15 22:52:09,993 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:52:09,993 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:52:10,332 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:52:10,626 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:52:10,754 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:52:10,754 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:52:10,755 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:52:12,446 - INFO - joeynmt.prediction - Generation took 1.6883[sec]. (No references given)
2022-09-15 22:52:12,459 - INFO - joeynmt.training - Processing Predictions on Batch 76/103
2022-09-15 22:52:16,968 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:52:16,969 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:52:17,306 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:52:17,599 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:52:17,725 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:52:17,725 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:52:17,728 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:52:18,419 - INFO - joeynmt.prediction - Generation took 0.6890[sec]. (No references given)
2022-09-15 22:52:18,432 - INFO - joeynmt.training - Processing Predictions on Batch 77/103
2022-09-15 22:52:22,931 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:52:22,931 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:52:23,268 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:52:23,561 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:52:23,688 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:52:23,688 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:52:23,689 - INFO - joeynmt.prediction - Predicting 3 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:52:24,103 - INFO - joeynmt.prediction - Generation took 0.4128[sec]. (No references given)
2022-09-15 22:52:24,114 - INFO - joeynmt.training - Processing Predictions on Batch 78/103
2022-09-15 22:52:28,605 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:52:28,605 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:52:28,945 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:52:29,223 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:52:29,352 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:52:29,352 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:52:29,355 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:52:30,029 - INFO - joeynmt.prediction - Generation took 0.6713[sec]. (No references given)
2022-09-15 22:52:30,039 - INFO - joeynmt.training - Processing Predictions on Batch 79/103
2022-09-15 22:52:34,530 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:52:34,530 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:52:34,867 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:52:35,148 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:52:35,274 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:52:35,274 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:52:35,277 - INFO - joeynmt.prediction - Predicting 11 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:52:36,041 - INFO - joeynmt.prediction - Generation took 0.7616[sec]. (No references given)
2022-09-15 22:52:36,053 - INFO - joeynmt.training - Processing Predictions on Batch 80/103
2022-09-15 22:52:40,565 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:52:40,565 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:52:40,903 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:52:41,181 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:52:41,309 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:52:41,309 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:52:41,310 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:52:41,902 - INFO - joeynmt.prediction - Generation took 0.5903[sec]. (No references given)
2022-09-15 22:52:41,913 - INFO - joeynmt.training - Processing Predictions on Batch 81/103
2022-09-15 22:52:46,420 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:52:46,420 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:52:46,757 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:52:47,038 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:52:47,167 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:52:47,168 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:52:47,170 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:52:48,835 - INFO - joeynmt.prediction - Generation took 1.6628[sec]. (No references given)
2022-09-15 22:52:48,847 - INFO - joeynmt.training - Processing Predictions on Batch 82/103
2022-09-15 22:52:53,359 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:52:53,359 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:52:53,697 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:52:53,973 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:52:54,101 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:52:54,101 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:52:54,104 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:52:54,766 - INFO - joeynmt.prediction - Generation took 0.6608[sec]. (No references given)
2022-09-15 22:52:54,776 - INFO - joeynmt.training - Processing Predictions on Batch 83/103
2022-09-15 22:52:59,299 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:52:59,299 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:52:59,636 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:52:59,919 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:53:00,046 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:00,046 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:00,047 - INFO - joeynmt.prediction - Predicting 3 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:53:00,638 - INFO - joeynmt.prediction - Generation took 0.5882[sec]. (No references given)
2022-09-15 22:53:00,650 - INFO - joeynmt.training - Processing Predictions on Batch 84/103
2022-09-15 22:53:05,170 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:53:05,170 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:53:05,506 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:53:05,784 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:53:05,911 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:05,911 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:05,913 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:53:07,522 - INFO - joeynmt.prediction - Generation took 1.6062[sec]. (No references given)
2022-09-15 22:53:07,532 - INFO - joeynmt.training - Processing Predictions on Batch 85/103
2022-09-15 22:53:12,049 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:53:12,049 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:53:12,386 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:53:12,669 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:53:12,797 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:12,797 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:12,800 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:53:14,246 - INFO - joeynmt.prediction - Generation took 1.4431[sec]. (No references given)
2022-09-15 22:53:14,258 - INFO - joeynmt.training - Processing Predictions on Batch 86/103
2022-09-15 22:53:18,769 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:53:18,769 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:53:19,107 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:53:19,397 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:53:19,525 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:19,525 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:19,526 - INFO - joeynmt.prediction - Predicting 3 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:53:20,175 - INFO - joeynmt.prediction - Generation took 0.6478[sec]. (No references given)
2022-09-15 22:53:20,188 - INFO - joeynmt.training - Processing Predictions on Batch 87/103
2022-09-15 22:53:24,698 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:53:24,698 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:53:25,036 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:53:25,315 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:53:25,441 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:25,441 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:25,443 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:53:26,075 - INFO - joeynmt.prediction - Generation took 0.6298[sec]. (No references given)
2022-09-15 22:53:26,085 - INFO - joeynmt.training - Processing Predictions on Batch 88/103
2022-09-15 22:53:30,590 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:53:30,590 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:53:30,927 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:53:31,203 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:53:31,330 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:31,330 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:31,332 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:53:31,834 - INFO - joeynmt.prediction - Generation took 0.5005[sec]. (No references given)
2022-09-15 22:53:31,844 - INFO - joeynmt.training - Processing Predictions on Batch 89/103
2022-09-15 22:53:36,372 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:53:36,373 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:53:36,709 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:53:37,503 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:53:37,630 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:37,631 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:37,632 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:53:37,998 - INFO - joeynmt.prediction - Generation took 0.3646[sec]. (No references given)
2022-09-15 22:53:38,009 - INFO - joeynmt.training - Processing Predictions on Batch 90/103
2022-09-15 22:53:42,476 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:53:42,476 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:53:42,813 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:53:43,090 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:53:43,217 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:43,218 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:43,219 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:53:44,837 - INFO - joeynmt.prediction - Generation took 1.6153[sec]. (No references given)
2022-09-15 22:53:44,847 - INFO - joeynmt.training - Processing Predictions on Batch 91/103
2022-09-15 22:53:49,304 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:53:49,306 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:53:49,642 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:53:49,924 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:53:50,052 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:50,053 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:50,055 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:53:51,223 - INFO - joeynmt.prediction - Generation took 1.1653[sec]. (No references given)
2022-09-15 22:53:51,235 - INFO - joeynmt.training - Processing Predictions on Batch 92/103
2022-09-15 22:53:55,696 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:53:55,696 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:53:56,033 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:53:56,322 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:53:56,450 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:56,450 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:53:56,452 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:53:57,054 - INFO - joeynmt.prediction - Generation took 0.5999[sec]. (No references given)
2022-09-15 22:53:57,067 - INFO - joeynmt.training - Processing Predictions on Batch 93/103
2022-09-15 22:54:01,529 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:54:01,529 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:54:01,868 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:54:02,148 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:54:02,276 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:02,276 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:02,279 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:54:03,265 - INFO - joeynmt.prediction - Generation took 0.9846[sec]. (No references given)
2022-09-15 22:54:03,276 - INFO - joeynmt.training - Processing Predictions on Batch 94/103
2022-09-15 22:54:07,732 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:54:07,733 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:54:08,069 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:54:08,352 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:54:08,479 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:08,479 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:08,481 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:54:09,095 - INFO - joeynmt.prediction - Generation took 0.6119[sec]. (No references given)
2022-09-15 22:54:09,108 - INFO - joeynmt.training - Processing Predictions on Batch 95/103
2022-09-15 22:54:13,562 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:54:13,562 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:54:13,898 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:54:14,177 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:54:14,304 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:14,304 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:14,306 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:54:14,778 - INFO - joeynmt.prediction - Generation took 0.4697[sec]. (No references given)
2022-09-15 22:54:14,787 - INFO - joeynmt.training - Processing Predictions on Batch 96/103
2022-09-15 22:54:19,244 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:54:19,244 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:54:19,582 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:54:19,857 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:54:19,985 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:19,985 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:19,987 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:54:20,541 - INFO - joeynmt.prediction - Generation took 0.5527[sec]. (No references given)
2022-09-15 22:54:20,551 - INFO - joeynmt.training - Processing Predictions on Batch 97/103
2022-09-15 22:54:25,003 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:54:25,003 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:54:25,339 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:54:25,607 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:54:25,733 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:25,733 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:25,735 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:54:26,341 - INFO - joeynmt.prediction - Generation took 0.6041[sec]. (No references given)
2022-09-15 22:54:26,351 - INFO - joeynmt.training - Processing Predictions on Batch 98/103
2022-09-15 22:54:30,803 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:54:30,803 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:54:31,141 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:54:31,420 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:54:31,547 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:31,547 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:31,549 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:54:32,227 - INFO - joeynmt.prediction - Generation took 0.6760[sec]. (No references given)
2022-09-15 22:54:32,239 - INFO - joeynmt.training - Processing Predictions on Batch 99/103
2022-09-15 22:54:36,701 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:54:36,701 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:54:37,038 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:54:37,334 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:54:37,462 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:37,462 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:37,463 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:54:38,561 - INFO - joeynmt.prediction - Generation took 1.0961[sec]. (No references given)
2022-09-15 22:54:38,573 - INFO - joeynmt.training - Processing Predictions on Batch 100/103
2022-09-15 22:54:43,036 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:54:43,036 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:54:43,375 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:54:43,666 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:54:43,796 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:43,796 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:43,800 - INFO - joeynmt.prediction - Predicting 13 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:54:44,527 - INFO - joeynmt.prediction - Generation took 0.7242[sec]. (No references given)
2022-09-15 22:54:44,539 - INFO - joeynmt.training - Processing Predictions on Batch 101/103
2022-09-15 22:54:48,998 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:54:48,998 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:54:49,336 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:54:49,631 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:54:49,759 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:49,759 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:49,763 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-15 22:54:50,491 - INFO - joeynmt.prediction - Generation took 0.7253[sec]. (No references given)
2022-09-15 22:54:50,503 - INFO - joeynmt.training - Processing Predictions on Batch 102/103
2022-09-15 22:54:54,973 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-15 22:54:54,973 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-15 22:54:55,310 - INFO - joeynmt.model - Enc-dec model built.
2022-09-15 22:54:55,588 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_r4_test/232000.ckpt.
2022-09-15 22:54:55,716 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-15 22:54:55,716 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:29:25,607 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).
2022-09-16 01:29:25,607 - INFO - joeynmt.helpers -                           cfg.name : transformer_100_enhi_bpe
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers -                     cfg.data.train : /home/ubuntu/joeynmt_kriti/data/datasets_enhi_100/train_tok
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers -                       cfg.data.dev : /home/ubuntu/joeynmt_kriti/data/datasets_enhi_100/val_tok
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers -                      cfg.data.test : /home/ubuntu/joeynmt_kriti/data/datasets_enhi_100/test_tok
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain_ac
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers -            cfg.data.src.max_length : 60
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : /home/ubuntu/joeynmt_kriti/data/datasets_enhi_100/vocab.en
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : /home/ubuntu/joeynmt_kriti/data/datasets_enhi_100/en.bpe.codes
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 16000
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : moses
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : hi
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers -            cfg.data.trg.max_length : 60
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2022-09-16 01:29:25,608 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : /home/ubuntu/joeynmt_kriti/data/datasets_enhi_100/vocab.hi
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : /home/ubuntu/joeynmt_kriti/data/datasets_enhi_100/hi.bpe.codes
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 16000
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers -                 cfg.testing.n_best : 1
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers -             cfg.testing.batch_size : 1024
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers -             cfg.testing.batch_type : token
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers -      cfg.testing.max_output_length : 130
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers -           cfg.testing.eval_metrics : ['bleu']
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers -            cfg.testing.return_prob : none
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers -       cfg.testing.return_attention : False
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.tokenize : 13a
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers - cfg.testing.sacrebleu_cfg.lowercase : False
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers -     cfg.active_learning.query_size : 10000
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers -    cfg.active_learning.interactive : False
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers -      cfg.active_learning.pool_size : 6
2022-09-16 01:29:25,609 - INFO - joeynmt.helpers -     cfg.active_learning.batch_size : 256
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers -    cfg.active_learning.num_workers : 4
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers -    cfg.active_learning.num_queries : 3
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers -     cfg.active_learning.al_percent : 30
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers - cfg.active_learning.query_strategy : least_confidence
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers -          cfg.active_learning.epoch : 1
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers - cfg.active_learning.validation_freq : 1000
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers -            cfg.training.load_model : /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/best.ckpt
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers -       cfg.training.reset_best_ckpt : False
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers -       cfg.training.reset_scheduler : False
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers -       cfg.training.reset_optimizer : False
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers -      cfg.training.reset_iter_state : False
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers - cfg.training.learning_rate_decay_length : 2500
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers -    cfg.training.learning_rate_peak : 0.005
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers -  cfg.training.learning_rate_warmup : 1000
2022-09-16 01:29:25,610 - INFO - joeynmt.helpers -  cfg.training.learning_rate_factor : 0.5
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -              cfg.training.patience : 5
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 1e-08
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.1
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -            cfg.training.batch_size : 4096
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 1
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -                cfg.training.epochs : 1
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 1000
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -             cfg.training.model_dir : /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3]
2022-09-16 01:29:25,611 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 3
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : False
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 1024
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.3
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers -       cfg.model.encoder.layer_norm : pre
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2022-09-16 01:29:25,612 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6
2022-09-16 01:29:25,613 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 4
2022-09-16 01:29:25,613 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2022-09-16 01:29:25,613 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2022-09-16 01:29:25,613 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2
2022-09-16 01:29:25,613 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2022-09-16 01:29:25,613 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 1024
2022-09-16 01:29:25,613 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.3
2022-09-16 01:29:25,613 - INFO - joeynmt.helpers -       cfg.model.decoder.layer_norm : pre
2022-09-16 01:29:25,651 - INFO - joeynmt.data - Building tokenizer...
2022-09-16 01:29:25,756 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:29:25,756 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:29:25,756 - INFO - joeynmt.data - Loading train set...
2022-09-16 01:33:21,485 - INFO - joeynmt.data - Building vocabulary...
2022-09-16 01:33:24,653 - INFO - joeynmt.data - Loading dev set...
2022-09-16 01:33:31,653 - INFO - joeynmt.data - Loading test set...
2022-09-16 01:33:38,516 - INFO - joeynmt.data - Data loaded.
2022-09-16 01:33:38,516 - INFO - joeynmt.helpers - Train dataset: PlaintextDatasetAC(split=train, len=1552563, src_lang=en, trg_lang=hi, has_trg=True, random_subset=-1)
2022-09-16 01:33:38,516 - INFO - joeynmt.helpers - Valid dataset: PlaintextDatasetAC(split=dev, len=40856, src_lang=en, trg_lang=hi, has_trg=True, random_subset=-1)
2022-09-16 01:33:38,517 - INFO - joeynmt.helpers -  Test dataset: PlaintextDatasetAC(split=test, len=40858, src_lang=en, trg_lang=hi, has_trg=True, random_subset=-1)
2022-09-16 01:33:38,517 - INFO - joeynmt.helpers - First training example:
	[SRC] give your application an accessibility work@@ out
	[TRG] अपने अनुप्रयोग को पहुंच@@ नीयता व्यायाम का लाभ दें
2022-09-16 01:33:38,517 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) , (6) . (7) of (8) and (9) to
2022-09-16 01:33:38,517 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) के (5) है (6) । (7) , (8) और (9) में
2022-09-16 01:33:38,517 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 16021
2022-09-16 01:33:38,518 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 16174
2022-09-16 01:33:38,528 - INFO - joeynmt.training - BASELINE MODEL START
2022-09-16 01:33:38,528 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:33:38,528 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:33:38,966 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:33:43,852 - INFO - joeynmt.model - Total params: 19302144
2022-09-16 01:33:43,854 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2022-09-16 01:33:43,854 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm="pre"),
	decoder=TransformerDecoder(num_layers=6, num_heads=4, alpha=1.0, layer_norm="pre"),
	src_embed=Embeddings(embedding_dim=256, vocab_size=16021),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=16174),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.1))
2022-09-16 01:33:46,551 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=[0.9, 0.999])
2022-09-16 01:33:46,552 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=5)
2022-09-16 01:33:46,552 - INFO - joeynmt.training - Loading model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/best.ckpt
2022-09-16 01:33:46,861 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:33:46,956 - INFO - joeynmt.training - BASELINE MODEL END
2022-09-16 01:33:46,957 - INFO - joeynmt.training - Random Indices picked: [121958 146867 131932 365838 259178 119879 110268 207892  54886 137337] length: 27946
2022-09-16 01:33:46,957 - INFO - joeynmt.training - Executing Random Strategy
2022-09-16 01:33:46,958 - INFO - joeynmt.training - Final Query Indices picked: [121958, 146867, 131932, 365838, 259178, 119879, 110268, 207892, 54886, 137337] length: 10000
2022-09-16 01:33:46,958 - INFO - joeynmt.training - Query the samples, one at a time (interactive/file)
2022-09-16 01:33:49,194 - INFO - joeynmt.training - Remaining Pool Data size: 455884
2022-09-16 01:33:49,194 - INFO - joeynmt.training - Active Learning Data ready to train size: 1096795
2022-09-16 01:33:49,194 - INFO - joeynmt.training - ACTIVE LEARNING MODEL END - RANDOM
2022-09-16 01:33:49,195 - INFO - joeynmt.training - ACTIVE LEARNING MODEL START - LEAST CONFIDENCE 0
2022-09-16 01:33:49,196 - INFO - joeynmt.training - Random Indices picked: [ 21058 388370 228845 308840 240035  38872  54971 344227  23306 421426] length: 27353
2022-09-16 01:33:49,196 - INFO - joeynmt.training - Processing Predictions on Batch 0/107
2022-09-16 01:33:52,665 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:33:52,666 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:33:53,029 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:33:53,303 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:33:53,475 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:33:53,475 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:33:53,513 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:33:57,531 - INFO - joeynmt.prediction - Generation took 4.0055[sec]. (No references given)
2022-09-16 01:33:57,538 - INFO - joeynmt.training - Processing Predictions on Batch 1/107
2022-09-16 01:34:00,963 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:34:00,963 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:34:01,306 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:34:01,591 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:34:01,703 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:34:01,704 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:34:01,742 - INFO - joeynmt.prediction - Predicting 248 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:34:05,647 - INFO - joeynmt.prediction - Generation took 3.8937[sec]. (No references given)
2022-09-16 01:34:05,656 - INFO - joeynmt.training - Processing Predictions on Batch 2/107
2022-09-16 01:34:09,169 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:34:09,170 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:34:09,502 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:34:09,823 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:34:09,935 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:34:09,935 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:34:09,972 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:34:13,748 - INFO - joeynmt.prediction - Generation took 3.7642[sec]. (No references given)
2022-09-16 01:34:13,757 - INFO - joeynmt.training - Processing Predictions on Batch 3/107
2022-09-16 01:34:17,341 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:34:17,341 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:34:17,676 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:34:17,939 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:34:18,052 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:34:18,052 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:34:18,091 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:34:21,718 - INFO - joeynmt.prediction - Generation took 3.6158[sec]. (No references given)
2022-09-16 01:34:21,727 - INFO - joeynmt.training - Processing Predictions on Batch 4/107
2022-09-16 01:34:25,353 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:34:25,353 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:34:25,686 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:34:25,947 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:34:26,059 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:34:26,059 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:34:26,095 - INFO - joeynmt.prediction - Predicting 251 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:34:30,112 - INFO - joeynmt.prediction - Generation took 4.0061[sec]. (No references given)
2022-09-16 01:34:30,124 - INFO - joeynmt.training - Processing Predictions on Batch 5/107
2022-09-16 01:34:33,755 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:34:33,755 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:34:34,088 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:34:34,369 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:34:34,482 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:34:34,482 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:34:34,516 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:34:38,233 - INFO - joeynmt.prediction - Generation took 3.7051[sec]. (No references given)
2022-09-16 01:34:38,246 - INFO - joeynmt.training - Processing Predictions on Batch 6/107
2022-09-16 01:34:41,913 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:34:41,913 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:34:42,254 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:34:42,530 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:34:42,642 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:34:42,642 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:34:42,684 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:34:46,461 - INFO - joeynmt.prediction - Generation took 3.7642[sec]. (No references given)
2022-09-16 01:34:46,473 - INFO - joeynmt.training - Processing Predictions on Batch 7/107
2022-09-16 01:34:50,257 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:34:50,257 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:34:50,659 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:34:50,934 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:34:51,046 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:34:51,047 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:34:51,081 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:34:54,681 - INFO - joeynmt.prediction - Generation took 3.5889[sec]. (No references given)
2022-09-16 01:34:54,693 - INFO - joeynmt.training - Processing Predictions on Batch 8/107
2022-09-16 01:34:58,507 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:34:58,507 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:34:58,850 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:34:59,131 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:34:59,245 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:34:59,245 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:34:59,283 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:35:02,962 - INFO - joeynmt.prediction - Generation took 3.6669[sec]. (No references given)
2022-09-16 01:35:02,974 - INFO - joeynmt.training - Processing Predictions on Batch 9/107
2022-09-16 01:35:06,832 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:35:06,832 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:35:07,175 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:35:07,513 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:35:07,625 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:35:07,625 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:35:07,666 - INFO - joeynmt.prediction - Predicting 250 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:35:11,237 - INFO - joeynmt.prediction - Generation took 3.5595[sec]. (No references given)
2022-09-16 01:35:11,249 - INFO - joeynmt.training - Processing Predictions on Batch 10/107
2022-09-16 01:35:15,135 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:35:15,135 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:35:15,476 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:35:15,759 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:35:15,872 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:35:15,872 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:35:15,910 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:35:19,608 - INFO - joeynmt.prediction - Generation took 3.6862[sec]. (No references given)
2022-09-16 01:35:19,621 - INFO - joeynmt.training - Processing Predictions on Batch 11/107
2022-09-16 01:35:23,484 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:35:23,484 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:35:23,827 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:35:24,104 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:35:24,216 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:35:24,216 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:35:24,257 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:35:28,450 - INFO - joeynmt.prediction - Generation took 4.1806[sec]. (No references given)
2022-09-16 01:35:28,459 - INFO - joeynmt.training - Processing Predictions on Batch 12/107
2022-09-16 01:35:32,255 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:35:32,255 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:35:32,588 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:35:32,855 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:35:32,968 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:35:32,968 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:35:33,009 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:35:36,842 - INFO - joeynmt.prediction - Generation took 3.8203[sec]. (No references given)
2022-09-16 01:35:36,854 - INFO - joeynmt.training - Processing Predictions on Batch 13/107
2022-09-16 01:35:40,637 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:35:40,637 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:35:40,981 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:35:41,250 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:35:41,364 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:35:41,364 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:35:41,401 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:35:44,801 - INFO - joeynmt.prediction - Generation took 3.3881[sec]. (No references given)
2022-09-16 01:35:44,811 - INFO - joeynmt.training - Processing Predictions on Batch 14/107
2022-09-16 01:35:48,645 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:35:48,645 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:35:48,978 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:35:49,308 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:35:49,422 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:35:49,422 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:35:49,463 - INFO - joeynmt.prediction - Predicting 248 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:35:53,301 - INFO - joeynmt.prediction - Generation took 3.8262[sec]. (No references given)
2022-09-16 01:35:53,311 - INFO - joeynmt.training - Processing Predictions on Batch 15/107
2022-09-16 01:35:57,152 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:35:57,153 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:35:57,485 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:35:57,748 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:35:57,861 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:35:57,861 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:35:57,898 - INFO - joeynmt.prediction - Predicting 251 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:36:01,556 - INFO - joeynmt.prediction - Generation took 3.6457[sec]. (No references given)
2022-09-16 01:36:01,566 - INFO - joeynmt.training - Processing Predictions on Batch 16/107
2022-09-16 01:36:05,439 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:36:05,440 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:36:05,774 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:36:06,037 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:36:06,151 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:36:06,151 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:36:06,189 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:36:10,288 - INFO - joeynmt.prediction - Generation took 4.0873[sec]. (No references given)
2022-09-16 01:36:10,297 - INFO - joeynmt.training - Processing Predictions on Batch 17/107
2022-09-16 01:36:14,158 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:36:14,158 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:36:14,491 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:36:14,754 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:36:14,867 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:36:14,868 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:36:14,906 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:36:18,516 - INFO - joeynmt.prediction - Generation took 3.5986[sec]. (No references given)
2022-09-16 01:36:18,525 - INFO - joeynmt.training - Processing Predictions on Batch 18/107
2022-09-16 01:36:22,313 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:36:22,313 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:36:22,647 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:36:22,910 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:36:23,024 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:36:23,024 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:36:23,065 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:36:27,498 - INFO - joeynmt.prediction - Generation took 4.4201[sec]. (No references given)
2022-09-16 01:36:27,508 - INFO - joeynmt.training - Processing Predictions on Batch 19/107
2022-09-16 01:36:31,365 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:36:31,366 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:36:31,698 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:36:32,027 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:36:32,139 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:36:32,140 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:36:32,180 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:36:36,101 - INFO - joeynmt.prediction - Generation took 3.9092[sec]. (No references given)
2022-09-16 01:36:36,111 - INFO - joeynmt.training - Processing Predictions on Batch 20/107
2022-09-16 01:36:39,987 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:36:39,988 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:36:40,322 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:36:40,583 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:36:40,697 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:36:40,697 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:36:40,733 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:36:44,608 - INFO - joeynmt.prediction - Generation took 3.8624[sec]. (No references given)
2022-09-16 01:36:44,619 - INFO - joeynmt.training - Processing Predictions on Batch 21/107
2022-09-16 01:36:48,491 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:36:48,491 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:36:48,823 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:36:49,092 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:36:49,205 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:36:49,205 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:36:49,242 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:36:53,037 - INFO - joeynmt.prediction - Generation took 3.7831[sec]. (No references given)
2022-09-16 01:36:53,049 - INFO - joeynmt.training - Processing Predictions on Batch 22/107
2022-09-16 01:36:56,906 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:36:56,906 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:36:57,247 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:36:57,514 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:36:57,627 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:36:57,627 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:36:57,664 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:37:01,384 - INFO - joeynmt.prediction - Generation took 3.7080[sec]. (No references given)
2022-09-16 01:37:01,394 - INFO - joeynmt.training - Processing Predictions on Batch 23/107
2022-09-16 01:37:05,203 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:37:05,203 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:37:05,538 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:37:05,811 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:37:05,925 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:37:05,925 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:37:05,961 - INFO - joeynmt.prediction - Predicting 248 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:37:09,097 - INFO - joeynmt.prediction - Generation took 3.1257[sec]. (No references given)
2022-09-16 01:37:09,107 - INFO - joeynmt.training - Processing Predictions on Batch 24/107
2022-09-16 01:37:12,961 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:37:12,961 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:37:13,294 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:37:13,621 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:37:13,734 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:37:13,734 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:37:13,771 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:37:17,250 - INFO - joeynmt.prediction - Generation took 3.4673[sec]. (No references given)
2022-09-16 01:37:17,262 - INFO - joeynmt.training - Processing Predictions on Batch 25/107
2022-09-16 01:37:21,110 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:37:21,110 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:37:21,451 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:37:21,725 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:37:21,838 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:37:21,838 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:37:21,875 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:37:25,104 - INFO - joeynmt.prediction - Generation took 3.2179[sec]. (No references given)
2022-09-16 01:37:25,114 - INFO - joeynmt.training - Processing Predictions on Batch 26/107
2022-09-16 01:37:28,968 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:37:28,968 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:37:29,302 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:37:29,564 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:37:29,677 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:37:29,677 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:37:29,715 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:37:33,363 - INFO - joeynmt.prediction - Generation took 3.6370[sec]. (No references given)
2022-09-16 01:37:33,373 - INFO - joeynmt.training - Processing Predictions on Batch 27/107
2022-09-16 01:37:37,197 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:37:37,197 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:37:37,531 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:37:37,794 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:37:37,907 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:37:37,907 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:37:37,945 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:37:41,363 - INFO - joeynmt.prediction - Generation took 3.4057[sec]. (No references given)
2022-09-16 01:37:41,372 - INFO - joeynmt.training - Processing Predictions on Batch 28/107
2022-09-16 01:37:45,170 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:37:45,170 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:37:45,504 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:37:45,765 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:37:45,878 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:37:45,878 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:37:45,914 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:37:49,775 - INFO - joeynmt.prediction - Generation took 3.8500[sec]. (No references given)
2022-09-16 01:37:49,785 - INFO - joeynmt.training - Processing Predictions on Batch 29/107
2022-09-16 01:37:53,666 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:37:53,666 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:37:53,999 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:37:54,328 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:37:54,441 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:37:54,442 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:37:54,481 - INFO - joeynmt.prediction - Predicting 251 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:37:58,317 - INFO - joeynmt.prediction - Generation took 3.8248[sec]. (No references given)
2022-09-16 01:37:58,327 - INFO - joeynmt.training - Processing Predictions on Batch 30/107
2022-09-16 01:38:02,200 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:38:02,200 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:38:02,537 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:38:02,798 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:38:02,912 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:38:02,912 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:38:02,953 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:38:06,730 - INFO - joeynmt.prediction - Generation took 3.7650[sec]. (No references given)
2022-09-16 01:38:06,740 - INFO - joeynmt.training - Processing Predictions on Batch 31/107
2022-09-16 01:38:10,604 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:38:10,604 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:38:10,939 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:38:11,209 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:38:11,322 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:38:11,322 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:38:11,359 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:38:15,306 - INFO - joeynmt.prediction - Generation took 3.9349[sec]. (No references given)
2022-09-16 01:38:15,317 - INFO - joeynmt.training - Processing Predictions on Batch 32/107
2022-09-16 01:38:19,173 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:38:19,173 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:38:19,516 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:38:19,783 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:38:19,898 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:38:19,898 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:38:19,937 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:38:23,697 - INFO - joeynmt.prediction - Generation took 3.7481[sec]. (No references given)
2022-09-16 01:38:23,707 - INFO - joeynmt.training - Processing Predictions on Batch 33/107
2022-09-16 01:38:27,520 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:38:27,520 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:38:27,854 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:38:28,121 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:38:28,233 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:38:28,234 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:38:28,273 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:38:31,903 - INFO - joeynmt.prediction - Generation took 3.6173[sec]. (No references given)
2022-09-16 01:38:31,912 - INFO - joeynmt.training - Processing Predictions on Batch 34/107
2022-09-16 01:38:35,763 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:38:35,763 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:38:36,096 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:38:36,359 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:38:36,540 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:38:36,540 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:38:36,580 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:38:40,526 - INFO - joeynmt.prediction - Generation took 3.9340[sec]. (No references given)
2022-09-16 01:38:40,535 - INFO - joeynmt.training - Processing Predictions on Batch 35/107
2022-09-16 01:38:44,371 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:38:44,371 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:38:44,704 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:38:44,961 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:38:45,075 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:38:45,075 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:38:45,113 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:38:48,927 - INFO - joeynmt.prediction - Generation took 3.8022[sec]. (No references given)
2022-09-16 01:38:48,936 - INFO - joeynmt.training - Processing Predictions on Batch 36/107
2022-09-16 01:38:52,755 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:38:52,755 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:38:53,087 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:38:53,355 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:38:53,468 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:38:53,468 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:38:53,506 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:38:57,307 - INFO - joeynmt.prediction - Generation took 3.7900[sec]. (No references given)
2022-09-16 01:38:57,319 - INFO - joeynmt.training - Processing Predictions on Batch 37/107
2022-09-16 01:39:01,177 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:39:01,177 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:39:01,518 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:39:01,800 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:39:01,913 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:39:01,914 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:39:01,955 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:39:05,874 - INFO - joeynmt.prediction - Generation took 3.9065[sec]. (No references given)
2022-09-16 01:39:05,886 - INFO - joeynmt.training - Processing Predictions on Batch 38/107
2022-09-16 01:39:09,694 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:39:09,694 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:39:10,036 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:39:10,303 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:39:10,416 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:39:10,416 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:39:10,454 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:39:13,937 - INFO - joeynmt.prediction - Generation took 3.4712[sec]. (No references given)
2022-09-16 01:39:13,947 - INFO - joeynmt.training - Processing Predictions on Batch 39/107
2022-09-16 01:39:17,805 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:39:17,805 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:39:18,203 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:39:18,472 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:39:18,586 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:39:18,586 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:39:18,629 - INFO - joeynmt.prediction - Predicting 245 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:39:22,542 - INFO - joeynmt.prediction - Generation took 3.9002[sec]. (No references given)
2022-09-16 01:39:22,552 - INFO - joeynmt.training - Processing Predictions on Batch 40/107
2022-09-16 01:39:26,377 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:39:26,377 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:39:26,714 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:39:26,983 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:39:27,096 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:39:27,096 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:39:27,136 - INFO - joeynmt.prediction - Predicting 249 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:39:31,184 - INFO - joeynmt.prediction - Generation took 4.0361[sec]. (No references given)
2022-09-16 01:39:31,196 - INFO - joeynmt.training - Processing Predictions on Batch 41/107
2022-09-16 01:39:35,030 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:39:35,030 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:39:35,371 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:39:35,638 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:39:35,751 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:39:35,752 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:39:35,793 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:39:39,447 - INFO - joeynmt.prediction - Generation took 3.6425[sec]. (No references given)
2022-09-16 01:39:39,457 - INFO - joeynmt.training - Processing Predictions on Batch 42/107
2022-09-16 01:39:43,298 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:39:43,298 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:39:43,635 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:39:43,908 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:39:44,021 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:39:44,023 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:39:44,059 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:39:47,521 - INFO - joeynmt.prediction - Generation took 3.4506[sec]. (No references given)
2022-09-16 01:39:47,533 - INFO - joeynmt.training - Processing Predictions on Batch 43/107
2022-09-16 01:39:51,308 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:39:51,308 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:39:51,650 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:39:51,918 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:39:52,031 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:39:52,031 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:39:52,074 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:39:56,188 - INFO - joeynmt.prediction - Generation took 4.1020[sec]. (No references given)
2022-09-16 01:39:56,198 - INFO - joeynmt.training - Processing Predictions on Batch 44/107
2022-09-16 01:40:00,010 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:40:00,010 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:40:00,345 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:40:00,690 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:40:00,803 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:40:00,803 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:40:00,843 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:40:04,343 - INFO - joeynmt.prediction - Generation took 3.4883[sec]. (No references given)
2022-09-16 01:40:04,353 - INFO - joeynmt.training - Processing Predictions on Batch 45/107
2022-09-16 01:40:08,204 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:40:08,204 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:40:08,537 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:40:08,797 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:40:08,909 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:40:08,910 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:40:08,945 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:40:12,401 - INFO - joeynmt.prediction - Generation took 3.4453[sec]. (No references given)
2022-09-16 01:40:12,414 - INFO - joeynmt.training - Processing Predictions on Batch 46/107
2022-09-16 01:40:16,257 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:40:16,258 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:40:16,599 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:40:16,878 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:40:16,992 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:40:16,992 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:40:17,030 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:40:20,951 - INFO - joeynmt.prediction - Generation took 3.9095[sec]. (No references given)
2022-09-16 01:40:20,963 - INFO - joeynmt.training - Processing Predictions on Batch 47/107
2022-09-16 01:40:24,802 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:40:24,803 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:40:25,212 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:40:25,477 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:40:25,591 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:40:25,591 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:40:25,626 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:40:28,925 - INFO - joeynmt.prediction - Generation took 3.2874[sec]. (No references given)
2022-09-16 01:40:28,935 - INFO - joeynmt.training - Processing Predictions on Batch 48/107
2022-09-16 01:40:32,765 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:40:32,766 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:40:33,099 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:40:33,367 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:40:33,481 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:40:33,481 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:40:33,519 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:40:37,199 - INFO - joeynmt.prediction - Generation took 3.6695[sec]. (No references given)
2022-09-16 01:40:37,209 - INFO - joeynmt.training - Processing Predictions on Batch 49/107
2022-09-16 01:40:41,070 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:40:41,070 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:40:41,406 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:40:41,673 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:40:41,786 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:40:41,787 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:40:41,828 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:40:45,546 - INFO - joeynmt.prediction - Generation took 3.7068[sec]. (No references given)
2022-09-16 01:40:45,558 - INFO - joeynmt.training - Processing Predictions on Batch 50/107
2022-09-16 01:40:49,379 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:40:49,380 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:40:49,721 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:40:50,000 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:40:50,113 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:40:50,114 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:40:50,155 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:40:54,321 - INFO - joeynmt.prediction - Generation took 4.1543[sec]. (No references given)
2022-09-16 01:40:54,333 - INFO - joeynmt.training - Processing Predictions on Batch 51/107
2022-09-16 01:40:58,133 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:40:58,133 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:40:58,474 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:40:58,742 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:40:58,855 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:40:58,855 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:40:58,888 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:41:02,181 - INFO - joeynmt.prediction - Generation took 3.2822[sec]. (No references given)
2022-09-16 01:41:02,191 - INFO - joeynmt.training - Processing Predictions on Batch 52/107
2022-09-16 01:41:06,012 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:41:06,012 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:41:06,348 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:41:06,616 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:41:06,730 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:41:06,730 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:41:06,769 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:41:10,569 - INFO - joeynmt.prediction - Generation took 3.7872[sec]. (No references given)
2022-09-16 01:41:10,579 - INFO - joeynmt.training - Processing Predictions on Batch 53/107
2022-09-16 01:41:14,355 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:41:14,355 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:41:14,688 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:41:14,950 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:41:15,063 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:41:15,063 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:41:15,103 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:41:18,546 - INFO - joeynmt.prediction - Generation took 3.4318[sec]. (No references given)
2022-09-16 01:41:18,556 - INFO - joeynmt.training - Processing Predictions on Batch 54/107
2022-09-16 01:41:22,348 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:41:22,350 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:41:22,684 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:41:22,947 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:41:23,060 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:41:23,060 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:41:23,101 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:41:27,747 - INFO - joeynmt.prediction - Generation took 4.6332[sec]. (No references given)
2022-09-16 01:41:27,757 - INFO - joeynmt.training - Processing Predictions on Batch 55/107
2022-09-16 01:41:31,602 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:41:31,602 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:41:31,936 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:41:32,196 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:41:32,380 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:41:32,380 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:41:32,417 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:41:36,232 - INFO - joeynmt.prediction - Generation took 3.8037[sec]. (No references given)
2022-09-16 01:41:36,242 - INFO - joeynmt.training - Processing Predictions on Batch 56/107
2022-09-16 01:41:40,049 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:41:40,049 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:41:40,383 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:41:40,646 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:41:40,760 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:41:40,760 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:41:40,798 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:41:43,226 - INFO - joeynmt.prediction - Generation took 2.4165[sec]. (No references given)
2022-09-16 01:41:43,236 - INFO - joeynmt.training - Processing Predictions on Batch 57/107
2022-09-16 01:41:47,045 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:41:47,045 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:41:47,382 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:41:47,642 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:41:47,755 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:41:47,756 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:41:47,794 - INFO - joeynmt.prediction - Predicting 247 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:41:52,298 - INFO - joeynmt.prediction - Generation took 4.4910[sec]. (No references given)
2022-09-16 01:41:52,308 - INFO - joeynmt.training - Processing Predictions on Batch 58/107
2022-09-16 01:41:56,143 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:41:56,144 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:41:56,476 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:41:56,816 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:41:56,929 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:41:56,929 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:41:56,969 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:42:00,991 - INFO - joeynmt.prediction - Generation took 4.0102[sec]. (No references given)
2022-09-16 01:42:01,001 - INFO - joeynmt.training - Processing Predictions on Batch 59/107
2022-09-16 01:42:04,840 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:42:04,840 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:42:05,173 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:42:05,438 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:42:05,552 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:42:05,552 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:42:05,586 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:42:09,670 - INFO - joeynmt.prediction - Generation took 4.0728[sec]. (No references given)
2022-09-16 01:42:09,680 - INFO - joeynmt.training - Processing Predictions on Batch 60/107
2022-09-16 01:42:13,528 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:42:13,529 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:42:13,862 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:42:14,122 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:42:14,235 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:42:14,235 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:42:14,275 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:42:18,031 - INFO - joeynmt.prediction - Generation took 3.7434[sec]. (No references given)
2022-09-16 01:42:18,041 - INFO - joeynmt.training - Processing Predictions on Batch 61/107
2022-09-16 01:42:21,905 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:42:21,905 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:42:22,238 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:42:22,573 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:42:22,686 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:42:22,687 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:42:22,728 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:42:26,884 - INFO - joeynmt.prediction - Generation took 4.1437[sec]. (No references given)
2022-09-16 01:42:26,895 - INFO - joeynmt.training - Processing Predictions on Batch 62/107
2022-09-16 01:42:30,750 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:42:30,750 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:42:31,083 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:42:31,346 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:42:31,459 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:42:31,459 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:42:31,496 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:42:35,130 - INFO - joeynmt.prediction - Generation took 3.6227[sec]. (No references given)
2022-09-16 01:42:35,140 - INFO - joeynmt.training - Processing Predictions on Batch 63/107
2022-09-16 01:42:39,010 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:42:39,010 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:42:39,345 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:42:39,614 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:42:39,728 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:42:39,728 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:42:39,768 - INFO - joeynmt.prediction - Predicting 246 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:42:43,699 - INFO - joeynmt.prediction - Generation took 3.9190[sec]. (No references given)
2022-09-16 01:42:43,709 - INFO - joeynmt.training - Processing Predictions on Batch 64/107
2022-09-16 01:42:47,565 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:42:47,566 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:42:47,900 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:42:48,160 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:42:48,346 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:42:48,346 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:42:48,382 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:42:52,317 - INFO - joeynmt.prediction - Generation took 3.9228[sec]. (No references given)
2022-09-16 01:42:52,327 - INFO - joeynmt.training - Processing Predictions on Batch 65/107
2022-09-16 01:42:56,115 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:42:56,115 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:42:56,457 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:42:56,736 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:42:56,849 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:42:56,850 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:42:56,886 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:43:00,748 - INFO - joeynmt.prediction - Generation took 3.8491[sec]. (No references given)
2022-09-16 01:43:00,760 - INFO - joeynmt.training - Processing Predictions on Batch 66/107
2022-09-16 01:43:04,539 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:43:04,539 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:43:04,883 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:43:05,148 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:43:05,261 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:43:05,261 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:43:05,298 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:43:08,843 - INFO - joeynmt.prediction - Generation took 3.5330[sec]. (No references given)
2022-09-16 01:43:08,853 - INFO - joeynmt.training - Processing Predictions on Batch 67/107
2022-09-16 01:43:12,685 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:43:12,685 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:43:13,018 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:43:13,287 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:43:13,473 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:43:13,473 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:43:13,510 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:43:17,444 - INFO - joeynmt.prediction - Generation took 3.9222[sec]. (No references given)
2022-09-16 01:43:17,454 - INFO - joeynmt.training - Processing Predictions on Batch 68/107
2022-09-16 01:43:21,237 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:43:21,237 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:43:21,570 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:43:21,832 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:43:21,946 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:43:21,946 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:43:21,981 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:43:25,598 - INFO - joeynmt.prediction - Generation took 3.6056[sec]. (No references given)
2022-09-16 01:43:25,607 - INFO - joeynmt.training - Processing Predictions on Batch 69/107
2022-09-16 01:43:29,386 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:43:29,386 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:43:29,721 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:43:29,983 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:43:30,096 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:43:30,096 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:43:30,134 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:43:33,523 - INFO - joeynmt.prediction - Generation took 3.3774[sec]. (No references given)
2022-09-16 01:43:33,533 - INFO - joeynmt.training - Processing Predictions on Batch 70/107
2022-09-16 01:43:37,334 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:43:37,334 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:43:37,667 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:43:38,001 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:43:38,113 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:43:38,114 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:43:38,151 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:43:42,044 - INFO - joeynmt.prediction - Generation took 3.8817[sec]. (No references given)
2022-09-16 01:43:42,054 - INFO - joeynmt.training - Processing Predictions on Batch 71/107
2022-09-16 01:43:45,858 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:43:45,858 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:43:46,190 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:43:46,463 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:43:46,576 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:43:46,577 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:43:46,610 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:43:50,254 - INFO - joeynmt.prediction - Generation took 3.6330[sec]. (No references given)
2022-09-16 01:43:50,266 - INFO - joeynmt.training - Processing Predictions on Batch 72/107
2022-09-16 01:43:54,079 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:43:54,080 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:43:54,424 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:43:54,700 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:43:54,813 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:43:54,813 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:43:54,849 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:43:58,345 - INFO - joeynmt.prediction - Generation took 3.4844[sec]. (No references given)
2022-09-16 01:43:58,357 - INFO - joeynmt.training - Processing Predictions on Batch 73/107
2022-09-16 01:44:02,154 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:44:02,155 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:44:02,498 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:44:02,767 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:44:02,953 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:44:02,953 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:44:02,993 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:44:06,542 - INFO - joeynmt.prediction - Generation took 3.5371[sec]. (No references given)
2022-09-16 01:44:06,552 - INFO - joeynmt.training - Processing Predictions on Batch 74/107
2022-09-16 01:44:10,318 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:44:10,318 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:44:10,651 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:44:10,921 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:44:11,034 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:44:11,034 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:44:11,072 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:44:14,962 - INFO - joeynmt.prediction - Generation took 3.8787[sec]. (No references given)
2022-09-16 01:44:14,973 - INFO - joeynmt.training - Processing Predictions on Batch 75/107
2022-09-16 01:44:18,753 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:44:18,753 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:44:19,088 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:44:19,348 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:44:19,461 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:44:19,461 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:44:19,499 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:44:23,431 - INFO - joeynmt.prediction - Generation took 3.9198[sec]. (No references given)
2022-09-16 01:44:23,441 - INFO - joeynmt.training - Processing Predictions on Batch 76/107
2022-09-16 01:44:27,255 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:44:27,255 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:44:27,588 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:44:27,935 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:44:28,048 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:44:28,048 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:44:28,084 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:44:31,890 - INFO - joeynmt.prediction - Generation took 3.7936[sec]. (No references given)
2022-09-16 01:44:31,902 - INFO - joeynmt.training - Processing Predictions on Batch 77/107
2022-09-16 01:44:35,715 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:44:35,715 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:44:36,057 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:44:36,324 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:44:36,437 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:44:36,438 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:44:36,474 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:44:40,152 - INFO - joeynmt.prediction - Generation took 3.6668[sec]. (No references given)
2022-09-16 01:44:40,162 - INFO - joeynmt.training - Processing Predictions on Batch 78/107
2022-09-16 01:44:43,957 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:44:43,957 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:44:44,292 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:44:44,563 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:44:44,677 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:44:44,677 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:44:44,715 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:44:48,055 - INFO - joeynmt.prediction - Generation took 3.3292[sec]. (No references given)
2022-09-16 01:44:48,065 - INFO - joeynmt.training - Processing Predictions on Batch 79/107
2022-09-16 01:44:51,861 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:44:51,862 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:44:52,196 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:44:52,450 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:44:52,637 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:44:52,637 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:44:52,672 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:44:56,466 - INFO - joeynmt.prediction - Generation took 3.7820[sec]. (No references given)
2022-09-16 01:44:56,476 - INFO - joeynmt.training - Processing Predictions on Batch 80/107
2022-09-16 01:45:00,226 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:45:00,226 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:45:00,560 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:45:00,824 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:45:00,937 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:45:00,937 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:45:00,973 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:45:04,486 - INFO - joeynmt.prediction - Generation took 3.5008[sec]. (No references given)
2022-09-16 01:45:04,495 - INFO - joeynmt.training - Processing Predictions on Batch 81/107
2022-09-16 01:45:08,253 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:45:08,253 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:45:08,588 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:45:08,852 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:45:08,965 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:45:08,965 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:45:09,003 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:45:12,583 - INFO - joeynmt.prediction - Generation took 3.5683[sec]. (No references given)
2022-09-16 01:45:12,593 - INFO - joeynmt.training - Processing Predictions on Batch 82/107
2022-09-16 01:45:16,408 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:45:16,408 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:45:16,814 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:45:17,078 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:45:17,191 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:45:17,191 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:45:17,232 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:45:21,090 - INFO - joeynmt.prediction - Generation took 3.8461[sec]. (No references given)
2022-09-16 01:45:21,100 - INFO - joeynmt.training - Processing Predictions on Batch 83/107
2022-09-16 01:45:24,908 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:45:24,908 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:45:25,242 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:45:25,505 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:45:25,618 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:45:25,618 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:45:25,658 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:45:29,278 - INFO - joeynmt.prediction - Generation took 3.6075[sec]. (No references given)
2022-09-16 01:45:29,287 - INFO - joeynmt.training - Processing Predictions on Batch 84/107
2022-09-16 01:45:33,126 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:45:33,126 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:45:33,461 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:45:33,727 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:45:33,840 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:45:33,840 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:45:33,880 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:45:37,338 - INFO - joeynmt.prediction - Generation took 3.4465[sec]. (No references given)
2022-09-16 01:45:37,350 - INFO - joeynmt.training - Processing Predictions on Batch 85/107
2022-09-16 01:45:41,184 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:45:41,185 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:45:41,598 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:45:41,865 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:45:41,978 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:45:41,978 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:45:42,013 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:45:45,719 - INFO - joeynmt.prediction - Generation took 3.6948[sec]. (No references given)
2022-09-16 01:45:45,728 - INFO - joeynmt.training - Processing Predictions on Batch 86/107
2022-09-16 01:45:49,558 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:45:49,558 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:45:49,892 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:45:50,155 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:45:50,268 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:45:50,268 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:45:50,305 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:45:53,593 - INFO - joeynmt.prediction - Generation took 3.2776[sec]. (No references given)
2022-09-16 01:45:53,603 - INFO - joeynmt.training - Processing Predictions on Batch 87/107
2022-09-16 01:45:57,446 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:45:57,447 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:45:57,780 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:45:58,042 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:45:58,155 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:45:58,155 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:45:58,191 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:46:02,092 - INFO - joeynmt.prediction - Generation took 3.8890[sec]. (No references given)
2022-09-16 01:46:02,102 - INFO - joeynmt.training - Processing Predictions on Batch 88/107
2022-09-16 01:46:05,944 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:46:05,944 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:46:06,350 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:46:06,610 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:46:06,723 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:46:06,723 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:46:06,757 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:46:10,214 - INFO - joeynmt.prediction - Generation took 3.4463[sec]. (No references given)
2022-09-16 01:46:10,224 - INFO - joeynmt.training - Processing Predictions on Batch 89/107
2022-09-16 01:46:14,054 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:46:14,055 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:46:14,388 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:46:14,649 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:46:14,762 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:46:14,763 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:46:14,798 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:46:18,350 - INFO - joeynmt.prediction - Generation took 3.5407[sec]. (No references given)
2022-09-16 01:46:18,360 - INFO - joeynmt.training - Processing Predictions on Batch 90/107
2022-09-16 01:46:22,239 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:46:22,239 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:46:22,573 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:46:22,842 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:46:22,954 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:46:22,954 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:46:22,992 - INFO - joeynmt.prediction - Predicting 244 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:46:26,756 - INFO - joeynmt.prediction - Generation took 3.7520[sec]. (No references given)
2022-09-16 01:46:26,766 - INFO - joeynmt.training - Processing Predictions on Batch 91/107
2022-09-16 01:46:30,593 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:46:30,593 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:46:30,927 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:46:31,202 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:46:31,315 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:46:31,315 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:46:31,348 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:46:34,691 - INFO - joeynmt.prediction - Generation took 3.3325[sec]. (No references given)
2022-09-16 01:46:34,703 - INFO - joeynmt.training - Processing Predictions on Batch 92/107
2022-09-16 01:46:38,477 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:46:38,478 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:46:38,819 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:46:39,086 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:46:39,198 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:46:39,198 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:46:39,232 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:46:43,054 - INFO - joeynmt.prediction - Generation took 3.8105[sec]. (No references given)
2022-09-16 01:46:43,063 - INFO - joeynmt.training - Processing Predictions on Batch 93/107
2022-09-16 01:46:46,883 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:46:46,884 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:46:47,216 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:46:47,473 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:46:47,585 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:46:47,585 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:46:47,628 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:46:51,265 - INFO - joeynmt.prediction - Generation took 3.6243[sec]. (No references given)
2022-09-16 01:46:51,273 - INFO - joeynmt.training - Processing Predictions on Batch 94/107
2022-09-16 01:46:55,063 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:46:55,063 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:46:55,396 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:46:55,665 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:46:55,777 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:46:55,777 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:46:55,818 - INFO - joeynmt.prediction - Predicting 242 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:46:59,362 - INFO - joeynmt.prediction - Generation took 3.5331[sec]. (No references given)
2022-09-16 01:46:59,372 - INFO - joeynmt.training - Processing Predictions on Batch 95/107
2022-09-16 01:47:03,159 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:47:03,160 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:47:03,494 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:47:03,762 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:47:03,875 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:47:03,875 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:47:03,912 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:47:07,723 - INFO - joeynmt.prediction - Generation took 3.7985[sec]. (No references given)
2022-09-16 01:47:07,732 - INFO - joeynmt.training - Processing Predictions on Batch 96/107
2022-09-16 01:47:11,535 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:47:11,535 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:47:11,877 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:47:12,152 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:47:12,264 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:47:12,264 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:47:12,296 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:47:15,878 - INFO - joeynmt.prediction - Generation took 3.5711[sec]. (No references given)
2022-09-16 01:47:15,889 - INFO - joeynmt.training - Processing Predictions on Batch 97/107
2022-09-16 01:47:19,667 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:47:19,667 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:47:20,008 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:47:20,291 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:47:20,404 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:47:20,404 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:47:20,441 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:47:24,384 - INFO - joeynmt.prediction - Generation took 3.9323[sec]. (No references given)
2022-09-16 01:47:24,397 - INFO - joeynmt.training - Processing Predictions on Batch 98/107
2022-09-16 01:47:28,161 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:47:28,162 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:47:28,505 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:47:28,786 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:47:28,899 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:47:28,899 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:47:28,935 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:47:32,486 - INFO - joeynmt.prediction - Generation took 3.5389[sec]. (No references given)
2022-09-16 01:47:32,498 - INFO - joeynmt.training - Processing Predictions on Batch 99/107
2022-09-16 01:47:36,328 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:47:36,329 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:47:36,671 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:47:36,946 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:47:37,059 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:47:37,059 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:47:37,093 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:47:40,583 - INFO - joeynmt.prediction - Generation took 3.4791[sec]. (No references given)
2022-09-16 01:47:40,595 - INFO - joeynmt.training - Processing Predictions on Batch 100/107
2022-09-16 01:47:44,447 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:47:44,447 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:47:44,863 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:47:45,146 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:47:45,259 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:47:45,259 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:47:45,294 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:47:49,094 - INFO - joeynmt.prediction - Generation took 3.7882[sec]. (No references given)
2022-09-16 01:47:49,106 - INFO - joeynmt.training - Processing Predictions on Batch 101/107
2022-09-16 01:47:52,925 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:47:52,925 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:47:53,267 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:47:53,543 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:47:53,655 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:47:53,655 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:47:53,693 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:47:57,384 - INFO - joeynmt.prediction - Generation took 3.6792[sec]. (No references given)
2022-09-16 01:47:57,396 - INFO - joeynmt.training - Processing Predictions on Batch 102/107
2022-09-16 01:48:01,236 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:48:01,236 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:48:01,582 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:48:01,869 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:48:01,983 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:48:01,983 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:48:02,022 - INFO - joeynmt.prediction - Predicting 243 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:48:05,649 - INFO - joeynmt.prediction - Generation took 3.6154[sec]. (No references given)
2022-09-16 01:48:05,662 - INFO - joeynmt.training - Processing Predictions on Batch 103/107
2022-09-16 01:48:09,506 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:48:09,506 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:48:09,922 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:48:10,206 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:48:10,318 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:48:10,319 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:48:10,358 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:48:14,061 - INFO - joeynmt.prediction - Generation took 3.6914[sec]. (No references given)
2022-09-16 01:48:14,073 - INFO - joeynmt.training - Processing Predictions on Batch 104/107
2022-09-16 01:48:17,926 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:48:17,926 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:48:18,269 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:48:18,543 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:48:18,656 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:48:18,656 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:48:18,694 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:48:22,611 - INFO - joeynmt.prediction - Generation took 3.9050[sec]. (No references given)
2022-09-16 01:48:22,623 - INFO - joeynmt.training - Processing Predictions on Batch 105/107
2022-09-16 01:48:26,468 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:48:26,468 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:48:26,810 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:48:27,087 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:48:27,200 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:48:27,200 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:48:27,237 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:48:30,829 - INFO - joeynmt.prediction - Generation took 3.5807[sec]. (No references given)
2022-09-16 01:48:30,841 - INFO - joeynmt.training - Processing Predictions on Batch 106/107
2022-09-16 01:48:34,688 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 01:48:34,688 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 01:48:35,031 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 01:48:35,306 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence/232000.ckpt.
2022-09-16 01:48:35,495 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:48:35,495 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 01:48:35,528 - INFO - joeynmt.prediction - Predicting 205 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:48:38,790 - INFO - joeynmt.prediction - Generation took 3.2529[sec]. (No references given)
2022-09-16 01:48:38,926 - INFO - joeynmt.training - Final Query Indices picked: [212789, 102960, 187776, 367244, 80000, 152884, 3532, 85862, 408991, 77952] length: 10000
2022-09-16 01:48:38,926 - INFO - joeynmt.training - Query the samples, one at a time (interactive/file)
2022-09-16 01:48:42,038 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 4096
	effective batch size (w. parallel & accumulation): 4096
2022-09-16 01:48:42,039 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 01:57:14,660 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 01:57:14,661 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.86, loss:   2.54, ppl:  12.65, acc:   0.51, generation: 502.8768[sec], evaluation: 9.2462[sec]
2022-09-16 01:57:15,110 - INFO - joeynmt.training - Example #0
2022-09-16 01:57:15,110 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 01:57:15,110 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 01:57:15,111 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 01:57:15,127 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 01:57:15,127 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 01:57:15,128 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 01:57:15,128 - INFO - joeynmt.training - Example #1
2022-09-16 01:57:15,128 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 01:57:15,128 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 01:57:15,128 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 01:57:15,143 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 01:57:15,143 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 01:57:15,143 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 01:57:15,143 - INFO - joeynmt.training - Example #2
2022-09-16 01:57:15,143 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 01:57:15,143 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 01:57:15,143 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 01:57:15,158 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 01:57:15,158 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 01:57:15,158 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 01:57:15,158 - INFO - joeynmt.training - Example #3
2022-09-16 01:57:15,159 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 01:57:15,159 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 01:57:15,159 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 01:57:15,173 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 01:57:15,173 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 01:57:15,173 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 01:57:15,231 - INFO - joeynmt.training - EPOCH 1
2022-09-16 01:57:27,691 - INFO - joeynmt.training - Epoch   1, Step:   232100, Batch Loss:     2.729488, Batch Acc: 0.004986, Tokens per Sec:     9128, Lr: 0.000035
2022-09-16 01:57:39,871 - INFO - joeynmt.training - Epoch   1, Step:   232200, Batch Loss:     2.678339, Batch Acc: 0.003913, Tokens per Sec:     9484, Lr: 0.000035
2022-09-16 01:57:51,965 - INFO - joeynmt.training - Epoch   1, Step:   232300, Batch Loss:     2.789371, Batch Acc: 0.004095, Tokens per Sec:     9451, Lr: 0.000035
2022-09-16 01:58:03,987 - INFO - joeynmt.training - Epoch   1, Step:   232400, Batch Loss:     2.730526, Batch Acc: 0.003699, Tokens per Sec:     9423, Lr: 0.000035
2022-09-16 01:58:15,989 - INFO - joeynmt.training - Epoch   1, Step:   232500, Batch Loss:     2.867078, Batch Acc: 0.004596, Tokens per Sec:     9499, Lr: 0.000035
2022-09-16 01:58:27,963 - INFO - joeynmt.training - Epoch   1, Step:   232600, Batch Loss:     2.727031, Batch Acc: 0.005308, Tokens per Sec:     9520, Lr: 0.000035
2022-09-16 01:58:39,961 - INFO - joeynmt.training - Epoch   1, Step:   232700, Batch Loss:     2.505309, Batch Acc: 0.004534, Tokens per Sec:     9633, Lr: 0.000035
2022-09-16 01:58:52,147 - INFO - joeynmt.training - Epoch   1, Step:   232800, Batch Loss:     2.596322, Batch Acc: 0.004673, Tokens per Sec:     9588, Lr: 0.000035
2022-09-16 01:59:04,113 - INFO - joeynmt.training - Epoch   1, Step:   232900, Batch Loss:     2.286114, Batch Acc: 0.004847, Tokens per Sec:     9620, Lr: 0.000035
2022-09-16 01:59:16,060 - INFO - joeynmt.training - Epoch   1, Step:   233000, Batch Loss:     2.535733, Batch Acc: 0.004722, Tokens per Sec:     9537, Lr: 0.000035
2022-09-16 01:59:16,061 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 02:07:56,827 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 02:07:56,828 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.83, loss:   2.54, ppl:  12.67, acc:   0.51, generation: 510.8491[sec], evaluation: 9.4186[sec]
2022-09-16 02:07:56,835 - INFO - joeynmt.training - Example #0
2022-09-16 02:07:56,836 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 02:07:56,836 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 02:07:56,836 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-16 02:07:56,853 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 02:07:56,853 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 02:07:56,853 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 02:07:56,853 - INFO - joeynmt.training - Example #1
2022-09-16 02:07:56,854 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 02:07:56,854 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 02:07:56,854 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 02:07:56,868 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 02:07:56,868 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 02:07:56,869 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 02:07:56,869 - INFO - joeynmt.training - Example #2
2022-09-16 02:07:56,869 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 02:07:56,869 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 02:07:56,869 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 02:07:56,884 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 02:07:56,884 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 02:07:56,884 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 02:07:56,884 - INFO - joeynmt.training - Example #3
2022-09-16 02:07:56,884 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 02:07:56,884 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 02:07:56,884 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 02:07:56,899 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 02:07:56,899 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 02:07:56,899 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 02:08:09,245 - INFO - joeynmt.training - Epoch   1, Step:   233100, Batch Loss:     2.813827, Batch Acc: 0.003732, Tokens per Sec:     9218, Lr: 0.000035
2022-09-16 02:08:21,476 - INFO - joeynmt.training - Epoch   1, Step:   233200, Batch Loss:     2.658359, Batch Acc: 0.004284, Tokens per Sec:     9389, Lr: 0.000035
2022-09-16 02:08:33,673 - INFO - joeynmt.training - Epoch   1, Step:   233300, Batch Loss:     2.494045, Batch Acc: 0.004651, Tokens per Sec:     9449, Lr: 0.000035
2022-09-16 02:08:45,858 - INFO - joeynmt.training - Epoch   1, Step:   233400, Batch Loss:     2.571293, Batch Acc: 0.004193, Tokens per Sec:     9453, Lr: 0.000035
2022-09-16 02:08:58,050 - INFO - joeynmt.training - Epoch   1, Step:   233500, Batch Loss:     2.695605, Batch Acc: 0.003856, Tokens per Sec:     9465, Lr: 0.000035
2022-09-16 02:09:10,263 - INFO - joeynmt.training - Epoch   1, Step:   233600, Batch Loss:     2.814043, Batch Acc: 0.004914, Tokens per Sec:     9314, Lr: 0.000035
2022-09-16 02:09:22,527 - INFO - joeynmt.training - Epoch   1, Step:   233700, Batch Loss:     2.497178, Batch Acc: 0.004531, Tokens per Sec:     9449, Lr: 0.000035
2022-09-16 02:09:34,707 - INFO - joeynmt.training - Epoch   1, Step:   233800, Batch Loss:     2.498805, Batch Acc: 0.004192, Tokens per Sec:     9440, Lr: 0.000035
2022-09-16 02:09:46,872 - INFO - joeynmt.training - Epoch   1, Step:   233900, Batch Loss:     2.831218, Batch Acc: 0.004803, Tokens per Sec:     9447, Lr: 0.000035
2022-09-16 02:09:58,962 - INFO - joeynmt.training - Epoch   1, Step:   234000, Batch Loss:     2.518821, Batch Acc: 0.005823, Tokens per Sec:     9376, Lr: 0.000035
2022-09-16 02:09:58,963 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 02:18:31,602 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 02:18:31,603 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.94, loss:   2.54, ppl:  12.64, acc:   0.51, generation: 502.8225[sec], evaluation: 9.3183[sec]
2022-09-16 02:18:31,609 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 02:18:32,057 - INFO - joeynmt.training - Example #0
2022-09-16 02:18:32,057 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 02:18:32,057 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 02:18:32,057 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 02:18:32,074 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 02:18:32,074 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 02:18:32,075 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 02:18:32,075 - INFO - joeynmt.training - Example #1
2022-09-16 02:18:32,075 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 02:18:32,075 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 02:18:32,075 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 02:18:32,090 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 02:18:32,091 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 02:18:32,091 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 02:18:32,091 - INFO - joeynmt.training - Example #2
2022-09-16 02:18:32,091 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 02:18:32,091 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 02:18:32,091 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 02:18:32,106 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 02:18:32,106 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 02:18:32,106 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 02:18:32,106 - INFO - joeynmt.training - Example #3
2022-09-16 02:18:32,106 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 02:18:32,106 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 02:18:32,106 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 02:18:32,121 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 02:18:32,121 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 02:18:32,121 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 02:18:44,050 - INFO - joeynmt.training - Epoch   1, Step:   234100, Batch Loss:     2.849273, Batch Acc: 0.004192, Tokens per Sec:     9108, Lr: 0.000035
2022-09-16 02:18:55,903 - INFO - joeynmt.training - Epoch   1, Step:   234200, Batch Loss:     2.719516, Batch Acc: 0.003936, Tokens per Sec:     9775, Lr: 0.000035
2022-09-16 02:19:07,779 - INFO - joeynmt.training - Epoch   1, Step:   234300, Batch Loss:     2.498071, Batch Acc: 0.004529, Tokens per Sec:     9743, Lr: 0.000035
2022-09-16 02:19:19,635 - INFO - joeynmt.training - Epoch   1, Step:   234400, Batch Loss:     2.841503, Batch Acc: 0.004193, Tokens per Sec:     9715, Lr: 0.000035
2022-09-16 02:19:31,521 - INFO - joeynmt.training - Epoch   1, Step:   234500, Batch Loss:     2.661370, Batch Acc: 0.004775, Tokens per Sec:     9657, Lr: 0.000035
2022-09-16 02:19:43,413 - INFO - joeynmt.training - Epoch   1, Step:   234600, Batch Loss:     2.768165, Batch Acc: 0.003204, Tokens per Sec:     9683, Lr: 0.000035
2022-09-16 02:19:55,200 - INFO - joeynmt.training - Epoch   1, Step:   234700, Batch Loss:     2.510572, Batch Acc: 0.004292, Tokens per Sec:     9588, Lr: 0.000035
2022-09-16 02:20:07,049 - INFO - joeynmt.training - Epoch   1, Step:   234800, Batch Loss:     2.490189, Batch Acc: 0.004369, Tokens per Sec:     9755, Lr: 0.000035
2022-09-16 02:20:18,940 - INFO - joeynmt.training - Epoch   1, Step:   234900, Batch Loss:     2.735864, Batch Acc: 0.004527, Tokens per Sec:     9846, Lr: 0.000035
2022-09-16 02:20:30,755 - INFO - joeynmt.training - Epoch   1, Step:   235000, Batch Loss:     2.661683, Batch Acc: 0.004044, Tokens per Sec:     9545, Lr: 0.000035
2022-09-16 02:20:30,755 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 02:29:15,254 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 02:29:15,255 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.96, loss:   2.54, ppl:  12.63, acc:   0.51, generation: 514.5817[sec], evaluation: 9.3993[sec]
2022-09-16 02:29:15,261 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 02:29:15,692 - INFO - joeynmt.training - Example #0
2022-09-16 02:29:15,693 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 02:29:15,693 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 02:29:15,693 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-16 02:29:15,710 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 02:29:15,710 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 02:29:15,710 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 02:29:15,710 - INFO - joeynmt.training - Example #1
2022-09-16 02:29:15,710 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 02:29:15,710 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 02:29:15,710 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 02:29:15,725 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 02:29:15,725 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 02:29:15,725 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 02:29:15,725 - INFO - joeynmt.training - Example #2
2022-09-16 02:29:15,725 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 02:29:15,725 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 02:29:15,725 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 02:29:15,740 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 02:29:15,740 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 02:29:15,740 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 02:29:15,740 - INFO - joeynmt.training - Example #3
2022-09-16 02:29:15,741 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 02:29:15,741 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 02:29:15,741 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 02:29:15,755 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 02:29:15,755 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 02:29:15,756 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 02:29:27,663 - INFO - joeynmt.training - Epoch   1, Step:   235100, Batch Loss:     2.662990, Batch Acc: 0.004876, Tokens per Sec:     9277, Lr: 0.000035
2022-09-16 02:29:39,563 - INFO - joeynmt.training - Epoch   1, Step:   235200, Batch Loss:     2.542673, Batch Acc: 0.005701, Tokens per Sec:     9685, Lr: 0.000035
2022-09-16 02:29:51,476 - INFO - joeynmt.training - Epoch   1, Step:   235300, Batch Loss:     2.723904, Batch Acc: 0.004262, Tokens per Sec:     9771, Lr: 0.000035
2022-09-16 02:30:03,338 - INFO - joeynmt.training - Epoch   1, Step:   235400, Batch Loss:     2.681489, Batch Acc: 0.004493, Tokens per Sec:     9700, Lr: 0.000035
2022-09-16 02:30:15,209 - INFO - joeynmt.training - Epoch   1, Step:   235500, Batch Loss:     2.698096, Batch Acc: 0.004660, Tokens per Sec:     9599, Lr: 0.000035
2022-09-16 02:30:27,102 - INFO - joeynmt.training - Epoch   1, Step:   235600, Batch Loss:     2.676036, Batch Acc: 0.005089, Tokens per Sec:     9551, Lr: 0.000035
2022-09-16 02:30:39,002 - INFO - joeynmt.training - Epoch   1, Step:   235700, Batch Loss:     2.683056, Batch Acc: 0.004132, Tokens per Sec:     9640, Lr: 0.000035
2022-09-16 02:30:50,811 - INFO - joeynmt.training - Epoch   1, Step:   235800, Batch Loss:     2.804587, Batch Acc: 0.004322, Tokens per Sec:     9718, Lr: 0.000035
2022-09-16 02:31:02,643 - INFO - joeynmt.training - Epoch   1, Step:   235900, Batch Loss:     2.418944, Batch Acc: 0.004734, Tokens per Sec:     9481, Lr: 0.000035
2022-09-16 02:31:14,500 - INFO - joeynmt.training - Epoch   1, Step:   236000, Batch Loss:     2.490302, Batch Acc: 0.005010, Tokens per Sec:     9713, Lr: 0.000035
2022-09-16 02:31:14,501 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 02:39:59,570 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 02:39:59,571 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.96, loss:   2.53, ppl:  12.57, acc:   0.51, generation: 515.1092[sec], evaluation: 9.4588[sec]
2022-09-16 02:39:59,577 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 02:40:00,015 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/232000.ckpt
2022-09-16 02:40:00,043 - INFO - joeynmt.training - Example #0
2022-09-16 02:40:00,043 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 02:40:00,043 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 02:40:00,043 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 02:40:00,060 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 02:40:00,060 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 02:40:00,060 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 02:40:00,061 - INFO - joeynmt.training - Example #1
2022-09-16 02:40:00,061 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 02:40:00,061 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 02:40:00,061 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 02:40:00,076 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 02:40:00,076 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 02:40:00,076 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 02:40:00,076 - INFO - joeynmt.training - Example #2
2022-09-16 02:40:00,076 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 02:40:00,076 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 02:40:00,076 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 02:40:00,091 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 02:40:00,091 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 02:40:00,091 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 02:40:00,091 - INFO - joeynmt.training - Example #3
2022-09-16 02:40:00,091 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 02:40:00,091 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 02:40:00,091 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 02:40:00,106 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 02:40:00,106 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 02:40:00,106 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 02:40:12,096 - INFO - joeynmt.training - Epoch   1, Step:   236100, Batch Loss:     2.729293, Batch Acc: 0.004691, Tokens per Sec:     9297, Lr: 0.000035
2022-09-16 02:40:23,901 - INFO - joeynmt.training - Epoch   1, Step:   236200, Batch Loss:     2.557362, Batch Acc: 0.005215, Tokens per Sec:     9504, Lr: 0.000035
2022-09-16 02:40:35,762 - INFO - joeynmt.training - Epoch   1, Step:   236300, Batch Loss:     2.676271, Batch Acc: 0.005337, Tokens per Sec:     9557, Lr: 0.000035
2022-09-16 02:40:47,618 - INFO - joeynmt.training - Epoch   1, Step:   236400, Batch Loss:     2.624111, Batch Acc: 0.005031, Tokens per Sec:     9624, Lr: 0.000035
2022-09-16 02:40:59,492 - INFO - joeynmt.training - Epoch   1, Step:   236500, Batch Loss:     2.671741, Batch Acc: 0.004407, Tokens per Sec:     9766, Lr: 0.000035
2022-09-16 02:41:11,399 - INFO - joeynmt.training - Epoch   1, Step:   236600, Batch Loss:     2.656073, Batch Acc: 0.004830, Tokens per Sec:     9546, Lr: 0.000035
2022-09-16 02:41:23,286 - INFO - joeynmt.training - Epoch   1, Step:   236700, Batch Loss:     2.582308, Batch Acc: 0.003687, Tokens per Sec:     9745, Lr: 0.000035
2022-09-16 02:41:35,126 - INFO - joeynmt.training - Epoch   1, Step:   236800, Batch Loss:     2.668853, Batch Acc: 0.004759, Tokens per Sec:     9583, Lr: 0.000035
2022-09-16 02:41:47,004 - INFO - joeynmt.training - Epoch   1, Step:   236900, Batch Loss:     2.667392, Batch Acc: 0.005539, Tokens per Sec:     9758, Lr: 0.000035
2022-09-16 02:41:58,816 - INFO - joeynmt.training - Epoch   1, Step:   237000, Batch Loss:     2.847941, Batch Acc: 0.004531, Tokens per Sec:     9641, Lr: 0.000035
2022-09-16 02:41:58,817 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 02:50:53,728 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 02:50:53,730 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.02, loss:   2.52, ppl:  12.47, acc:   0.51, generation: 524.8085[sec], evaluation: 9.5977[sec]
2022-09-16 02:50:53,735 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 02:50:54,177 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/234000.ckpt
2022-09-16 02:50:54,205 - INFO - joeynmt.training - Example #0
2022-09-16 02:50:54,205 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 02:50:54,205 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 02:50:54,205 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 02:50:54,222 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 02:50:54,222 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 02:50:54,222 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 02:50:54,222 - INFO - joeynmt.training - Example #1
2022-09-16 02:50:54,222 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 02:50:54,223 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 02:50:54,223 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 02:50:54,237 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 02:50:54,237 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 02:50:54,237 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 02:50:54,237 - INFO - joeynmt.training - Example #2
2022-09-16 02:50:54,238 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 02:50:54,238 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 02:50:54,238 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 02:50:54,252 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 02:50:54,252 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 02:50:54,253 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 02:50:54,253 - INFO - joeynmt.training - Example #3
2022-09-16 02:50:54,253 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 02:50:54,253 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 02:50:54,253 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 02:50:54,267 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 02:50:54,268 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 02:50:54,268 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 02:51:06,196 - INFO - joeynmt.training - Epoch   1, Step:   237100, Batch Loss:     2.747992, Batch Acc: 0.004345, Tokens per Sec:     9179, Lr: 0.000035
2022-09-16 02:51:18,052 - INFO - joeynmt.training - Epoch   1, Step:   237200, Batch Loss:     2.661003, Batch Acc: 0.003970, Tokens per Sec:     9732, Lr: 0.000035
2022-09-16 02:51:29,910 - INFO - joeynmt.training - Epoch   1, Step:   237300, Batch Loss:     2.520321, Batch Acc: 0.004890, Tokens per Sec:     9572, Lr: 0.000035
2022-09-16 02:51:41,750 - INFO - joeynmt.training - Epoch   1, Step:   237400, Batch Loss:     2.628083, Batch Acc: 0.004356, Tokens per Sec:     9520, Lr: 0.000035
2022-09-16 02:51:53,626 - INFO - joeynmt.training - Epoch   1, Step:   237500, Batch Loss:     2.486845, Batch Acc: 0.004299, Tokens per Sec:     9717, Lr: 0.000035
2022-09-16 02:52:05,508 - INFO - joeynmt.training - Epoch   1, Step:   237600, Batch Loss:     2.701645, Batch Acc: 0.003954, Tokens per Sec:     9664, Lr: 0.000035
2022-09-16 02:52:17,329 - INFO - joeynmt.training - Epoch   1, Step:   237700, Batch Loss:     2.707231, Batch Acc: 0.004747, Tokens per Sec:     9641, Lr: 0.000035
2022-09-16 02:52:29,181 - INFO - joeynmt.training - Epoch   1, Step:   237800, Batch Loss:     2.717611, Batch Acc: 0.004831, Tokens per Sec:     9589, Lr: 0.000035
2022-09-16 02:52:41,018 - INFO - joeynmt.training - Epoch   1, Step:   237900, Batch Loss:     2.868741, Batch Acc: 0.003956, Tokens per Sec:     9631, Lr: 0.000035
2022-09-16 02:52:52,851 - INFO - joeynmt.training - Epoch   1, Step:   238000, Batch Loss:     2.941936, Batch Acc: 0.004098, Tokens per Sec:     9692, Lr: 0.000035
2022-09-16 02:52:52,852 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 03:01:54,509 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 03:01:54,510 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.03, loss:   2.53, ppl:  12.50, acc:   0.51, generation: 531.9484[sec], evaluation: 9.1977[sec]
2022-09-16 03:01:54,952 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/235000.ckpt
2022-09-16 03:01:54,980 - INFO - joeynmt.training - Example #0
2022-09-16 03:01:54,980 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 03:01:54,981 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 03:01:54,981 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 03:01:54,998 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 03:01:54,998 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 03:01:54,998 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 03:01:54,998 - INFO - joeynmt.training - Example #1
2022-09-16 03:01:54,998 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 03:01:54,998 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 03:01:54,998 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 03:01:55,013 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 03:01:55,013 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 03:01:55,013 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 03:01:55,013 - INFO - joeynmt.training - Example #2
2022-09-16 03:01:55,013 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 03:01:55,013 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 03:01:55,014 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 03:01:55,028 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 03:01:55,028 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 03:01:55,028 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 03:01:55,029 - INFO - joeynmt.training - Example #3
2022-09-16 03:01:55,029 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 03:01:55,029 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 03:01:55,029 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 03:01:55,044 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 03:01:55,044 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 03:01:55,044 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 03:02:06,967 - INFO - joeynmt.training - Epoch   1, Step:   238100, Batch Loss:     2.632385, Batch Acc: 0.003823, Tokens per Sec:     9159, Lr: 0.000035
2022-09-16 03:02:18,744 - INFO - joeynmt.training - Epoch   1, Step:   238200, Batch Loss:     2.826868, Batch Acc: 0.003871, Tokens per Sec:     9544, Lr: 0.000035
2022-09-16 03:02:30,562 - INFO - joeynmt.training - Epoch   1, Step:   238300, Batch Loss:     2.693064, Batch Acc: 0.003466, Tokens per Sec:     9423, Lr: 0.000035
2022-09-16 03:02:42,418 - INFO - joeynmt.training - Epoch   1, Step:   238400, Batch Loss:     2.546380, Batch Acc: 0.004278, Tokens per Sec:     9386, Lr: 0.000035
2022-09-16 03:02:54,277 - INFO - joeynmt.training - Epoch   1, Step:   238500, Batch Loss:     2.611542, Batch Acc: 0.004620, Tokens per Sec:     9619, Lr: 0.000035
2022-09-16 03:03:06,138 - INFO - joeynmt.training - Epoch   1, Step:   238600, Batch Loss:     2.660439, Batch Acc: 0.004821, Tokens per Sec:     9847, Lr: 0.000035
2022-09-16 03:03:17,942 - INFO - joeynmt.training - Epoch   1, Step:   238700, Batch Loss:     2.571347, Batch Acc: 0.004959, Tokens per Sec:     9653, Lr: 0.000035
2022-09-16 03:03:30,190 - INFO - joeynmt.training - Epoch   1, Step:   238800, Batch Loss:     2.555219, Batch Acc: 0.004602, Tokens per Sec:     9457, Lr: 0.000035
2022-09-16 03:03:42,025 - INFO - joeynmt.training - Epoch   1, Step:   238900, Batch Loss:     2.553567, Batch Acc: 0.005541, Tokens per Sec:     9623, Lr: 0.000035
2022-09-16 03:03:53,825 - INFO - joeynmt.training - Epoch   1, Step:   239000, Batch Loss:     2.420316, Batch Acc: 0.004004, Tokens per Sec:     9674, Lr: 0.000035
2022-09-16 03:03:53,825 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 03:12:37,088 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 03:12:37,089 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  21.98, loss:   2.52, ppl:  12.43, acc:   0.51, generation: 513.5831[sec], evaluation: 9.1735[sec]
2022-09-16 03:12:37,095 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 03:12:37,528 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/236000.ckpt
2022-09-16 03:12:37,556 - INFO - joeynmt.training - Example #0
2022-09-16 03:12:37,556 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 03:12:37,556 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 03:12:37,556 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 03:12:37,573 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 03:12:37,573 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 03:12:37,573 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 03:12:37,573 - INFO - joeynmt.training - Example #1
2022-09-16 03:12:37,574 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 03:12:37,574 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 03:12:37,574 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 03:12:37,588 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 03:12:37,589 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 03:12:37,589 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 03:12:37,589 - INFO - joeynmt.training - Example #2
2022-09-16 03:12:37,589 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 03:12:37,589 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 03:12:37,589 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 03:12:37,604 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 03:12:37,604 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 03:12:37,604 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 03:12:37,604 - INFO - joeynmt.training - Example #3
2022-09-16 03:12:37,604 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 03:12:37,604 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 03:12:37,604 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 03:12:37,619 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 03:12:37,619 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 03:12:37,619 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 03:12:49,488 - INFO - joeynmt.training - Epoch   1, Step:   239100, Batch Loss:     2.571148, Batch Acc: 0.005344, Tokens per Sec:     9196, Lr: 0.000035
2022-09-16 03:13:01,570 - INFO - joeynmt.training - Epoch   1, Step:   239200, Batch Loss:     2.763784, Batch Acc: 0.004209, Tokens per Sec:     9636, Lr: 0.000035
2022-09-16 03:13:13,671 - INFO - joeynmt.training - Epoch   1, Step:   239300, Batch Loss:     2.952753, Batch Acc: 0.004603, Tokens per Sec:     9532, Lr: 0.000035
2022-09-16 03:13:25,726 - INFO - joeynmt.training - Epoch   1, Step:   239400, Batch Loss:     2.692296, Batch Acc: 0.005041, Tokens per Sec:     9428, Lr: 0.000035
2022-09-16 03:13:37,823 - INFO - joeynmt.training - Epoch   1, Step:   239500, Batch Loss:     2.605416, Batch Acc: 0.004164, Tokens per Sec:     9511, Lr: 0.000035
2022-09-16 03:13:49,728 - INFO - joeynmt.training - Epoch   1, Step:   239600, Batch Loss:     2.630680, Batch Acc: 0.004788, Tokens per Sec:     9455, Lr: 0.000035
2022-09-16 03:14:01,571 - INFO - joeynmt.training - Epoch   1, Step:   239700, Batch Loss:     2.537317, Batch Acc: 0.004022, Tokens per Sec:     9512, Lr: 0.000035
2022-09-16 03:14:13,442 - INFO - joeynmt.training - Epoch   1, Step:   239800, Batch Loss:     2.673465, Batch Acc: 0.004786, Tokens per Sec:     9628, Lr: 0.000035
2022-09-16 03:14:25,324 - INFO - joeynmt.training - Epoch   1, Step:   239900, Batch Loss:     2.607424, Batch Acc: 0.004977, Tokens per Sec:     9638, Lr: 0.000035
2022-09-16 03:14:37,215 - INFO - joeynmt.training - Epoch   1, Step:   240000, Batch Loss:     2.449348, Batch Acc: 0.004206, Tokens per Sec:     9579, Lr: 0.000035
2022-09-16 03:14:37,215 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 03:23:19,378 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 03:23:19,379 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.03, loss:   2.51, ppl:  12.35, acc:   0.51, generation: 511.9290[sec], evaluation: 9.7264[sec]
2022-09-16 03:23:19,385 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 03:23:19,819 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/238000.ckpt
2022-09-16 03:23:19,847 - INFO - joeynmt.training - Example #0
2022-09-16 03:23:19,847 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 03:23:19,847 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 03:23:19,847 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-16 03:23:19,864 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 03:23:19,864 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 03:23:19,864 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 03:23:19,864 - INFO - joeynmt.training - Example #1
2022-09-16 03:23:19,864 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 03:23:19,864 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 03:23:19,864 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 03:23:19,879 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 03:23:19,880 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 03:23:19,880 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 03:23:19,880 - INFO - joeynmt.training - Example #2
2022-09-16 03:23:19,880 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 03:23:19,880 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 03:23:19,880 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 03:23:19,895 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 03:23:19,895 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 03:23:19,895 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 03:23:19,895 - INFO - joeynmt.training - Example #3
2022-09-16 03:23:19,895 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 03:23:19,895 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 03:23:19,895 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 03:23:19,910 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 03:23:19,910 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 03:23:19,910 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 03:23:31,831 - INFO - joeynmt.training - Epoch   1, Step:   240100, Batch Loss:     2.513151, Batch Acc: 0.004012, Tokens per Sec:     9052, Lr: 0.000035
2022-09-16 03:23:43,668 - INFO - joeynmt.training - Epoch   1, Step:   240200, Batch Loss:     2.699172, Batch Acc: 0.003785, Tokens per Sec:     9665, Lr: 0.000035
2022-09-16 03:23:55,531 - INFO - joeynmt.training - Epoch   1, Step:   240300, Batch Loss:     2.474271, Batch Acc: 0.004755, Tokens per Sec:     9663, Lr: 0.000035
2022-09-16 03:24:07,404 - INFO - joeynmt.training - Epoch   1, Step:   240400, Batch Loss:     2.836209, Batch Acc: 0.004515, Tokens per Sec:     9832, Lr: 0.000035
2022-09-16 03:24:19,247 - INFO - joeynmt.training - Epoch   1, Step:   240500, Batch Loss:     2.644982, Batch Acc: 0.003395, Tokens per Sec:     9724, Lr: 0.000035
2022-09-16 03:24:31,109 - INFO - joeynmt.training - Epoch   1, Step:   240600, Batch Loss:     2.649275, Batch Acc: 0.004325, Tokens per Sec:     9669, Lr: 0.000035
2022-09-16 03:24:42,946 - INFO - joeynmt.training - Epoch   1, Step:   240700, Batch Loss:     2.794883, Batch Acc: 0.004725, Tokens per Sec:     9638, Lr: 0.000035
2022-09-16 03:24:54,781 - INFO - joeynmt.training - Epoch   1, Step:   240800, Batch Loss:     2.660462, Batch Acc: 0.004777, Tokens per Sec:     9711, Lr: 0.000035
2022-09-16 03:25:06,682 - INFO - joeynmt.training - Epoch   1, Step:   240900, Batch Loss:     2.737550, Batch Acc: 0.004076, Tokens per Sec:     9813, Lr: 0.000035
2022-09-16 03:25:18,568 - INFO - joeynmt.training - Epoch   1, Step:   241000, Batch Loss:     2.844695, Batch Acc: 0.004788, Tokens per Sec:     9665, Lr: 0.000035
2022-09-16 03:25:18,568 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 03:34:10,727 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 03:34:10,728 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.10, loss:   2.52, ppl:  12.38, acc:   0.51, generation: 522.3538[sec], evaluation: 9.2910[sec]
2022-09-16 03:34:11,169 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/237000.ckpt
2022-09-16 03:34:11,197 - INFO - joeynmt.training - Example #0
2022-09-16 03:34:11,197 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 03:34:11,197 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 03:34:11,197 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-16 03:34:11,214 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 03:34:11,215 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 03:34:11,215 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 03:34:11,215 - INFO - joeynmt.training - Example #1
2022-09-16 03:34:11,215 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 03:34:11,215 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 03:34:11,215 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 03:34:11,230 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 03:34:11,230 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 03:34:11,230 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 03:34:11,230 - INFO - joeynmt.training - Example #2
2022-09-16 03:34:11,230 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 03:34:11,230 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 03:34:11,231 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 03:34:11,246 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 03:34:11,246 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 03:34:11,246 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 03:34:11,246 - INFO - joeynmt.training - Example #3
2022-09-16 03:34:11,246 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 03:34:11,246 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 03:34:11,246 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 03:34:11,261 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 03:34:11,261 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 03:34:11,261 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 03:34:23,159 - INFO - joeynmt.training - Epoch   1, Step:   241100, Batch Loss:     2.552368, Batch Acc: 0.004004, Tokens per Sec:     9227, Lr: 0.000035
2022-09-16 03:34:34,999 - INFO - joeynmt.training - Epoch   1, Step:   241200, Batch Loss:     2.685496, Batch Acc: 0.004810, Tokens per Sec:     9693, Lr: 0.000035
2022-09-16 03:34:46,887 - INFO - joeynmt.training - Epoch   1, Step:   241300, Batch Loss:     2.428611, Batch Acc: 0.003918, Tokens per Sec:     9662, Lr: 0.000035
2022-09-16 03:34:58,682 - INFO - joeynmt.training - Epoch   1, Step:   241400, Batch Loss:     2.291205, Batch Acc: 0.004822, Tokens per Sec:     9775, Lr: 0.000035
2022-09-16 03:35:10,492 - INFO - joeynmt.training - Epoch   1, Step:   241500, Batch Loss:     2.846140, Batch Acc: 0.003693, Tokens per Sec:     9517, Lr: 0.000035
2022-09-16 03:35:22,315 - INFO - joeynmt.training - Epoch   1, Step:   241600, Batch Loss:     2.626646, Batch Acc: 0.004791, Tokens per Sec:     9746, Lr: 0.000035
2022-09-16 03:35:34,142 - INFO - joeynmt.training - Epoch   1, Step:   241700, Batch Loss:     2.655816, Batch Acc: 0.005669, Tokens per Sec:     9799, Lr: 0.000035
2022-09-16 03:35:46,000 - INFO - joeynmt.training - Epoch   1, Step:   241800, Batch Loss:     2.728070, Batch Acc: 0.004152, Tokens per Sec:     9750, Lr: 0.000035
2022-09-16 03:35:57,821 - INFO - joeynmt.training - Epoch   1, Step:   241900, Batch Loss:     2.365386, Batch Acc: 0.004908, Tokens per Sec:     9739, Lr: 0.000035
2022-09-16 03:36:09,654 - INFO - joeynmt.training - Epoch   1, Step:   242000, Batch Loss:     2.886161, Batch Acc: 0.004334, Tokens per Sec:     9789, Lr: 0.000035
2022-09-16 03:36:09,654 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 03:44:58,202 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 03:44:58,203 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.10, loss:   2.51, ppl:  12.34, acc:   0.51, generation: 518.2769[sec], evaluation: 9.7628[sec]
2022-09-16 03:44:58,209 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 03:44:58,645 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/239000.ckpt
2022-09-16 03:44:58,673 - INFO - joeynmt.training - Example #0
2022-09-16 03:44:58,673 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 03:44:58,673 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 03:44:58,673 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-16 03:44:58,690 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 03:44:58,690 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 03:44:58,690 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 03:44:58,690 - INFO - joeynmt.training - Example #1
2022-09-16 03:44:58,691 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 03:44:58,691 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 03:44:58,691 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 03:44:58,706 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 03:44:58,706 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 03:44:58,706 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 03:44:58,706 - INFO - joeynmt.training - Example #2
2022-09-16 03:44:58,706 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 03:44:58,706 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 03:44:58,706 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 03:44:58,721 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 03:44:58,721 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 03:44:58,721 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 03:44:58,721 - INFO - joeynmt.training - Example #3
2022-09-16 03:44:58,721 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 03:44:58,721 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 03:44:58,721 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 03:44:58,736 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 03:44:58,736 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 03:44:58,736 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 03:45:10,632 - INFO - joeynmt.training - Epoch   1, Step:   242100, Batch Loss:     2.661082, Batch Acc: 0.004506, Tokens per Sec:     9093, Lr: 0.000035
2022-09-16 03:45:22,412 - INFO - joeynmt.training - Epoch   1, Step:   242200, Batch Loss:     2.570481, Batch Acc: 0.004665, Tokens per Sec:     9899, Lr: 0.000035
2022-09-16 03:45:34,222 - INFO - joeynmt.training - Epoch   1, Step:   242300, Batch Loss:     2.614609, Batch Acc: 0.004378, Tokens per Sec:     9632, Lr: 0.000035
2022-09-16 03:45:46,041 - INFO - joeynmt.training - Epoch   1, Step:   242400, Batch Loss:     2.875410, Batch Acc: 0.003818, Tokens per Sec:     9683, Lr: 0.000035
2022-09-16 03:45:57,854 - INFO - joeynmt.training - Epoch   1, Step:   242500, Batch Loss:     2.590926, Batch Acc: 0.004207, Tokens per Sec:     9639, Lr: 0.000035
2022-09-16 03:46:09,633 - INFO - joeynmt.training - Epoch   1, Step:   242600, Batch Loss:     2.665024, Batch Acc: 0.004765, Tokens per Sec:     9818, Lr: 0.000035
2022-09-16 03:46:21,394 - INFO - joeynmt.training - Epoch   1, Step:   242700, Batch Loss:     2.518055, Batch Acc: 0.004893, Tokens per Sec:     9766, Lr: 0.000035
2022-09-16 03:46:33,141 - INFO - joeynmt.training - Epoch   1, Step:   242800, Batch Loss:     2.543454, Batch Acc: 0.005997, Tokens per Sec:     9866, Lr: 0.000035
2022-09-16 03:46:44,876 - INFO - joeynmt.training - Epoch   1, Step:   242900, Batch Loss:     2.801288, Batch Acc: 0.004672, Tokens per Sec:     9686, Lr: 0.000035
2022-09-16 03:46:56,615 - INFO - joeynmt.training - Epoch   1, Step:   243000, Batch Loss:     2.520457, Batch Acc: 0.005367, Tokens per Sec:     9730, Lr: 0.000035
2022-09-16 03:46:56,616 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 03:55:18,811 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 03:55:18,812 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.06, loss:   2.52, ppl:  12.42, acc:   0.51, generation: 492.4066[sec], evaluation: 9.2763[sec]
2022-09-16 03:55:18,819 - INFO - joeynmt.training - Example #0
2022-09-16 03:55:18,819 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 03:55:18,820 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 03:55:18,820 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-16 03:55:18,836 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 03:55:18,837 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 03:55:18,837 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 03:55:18,837 - INFO - joeynmt.training - Example #1
2022-09-16 03:55:18,837 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 03:55:18,837 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 03:55:18,837 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 03:55:18,852 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 03:55:18,852 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 03:55:18,852 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 03:55:18,852 - INFO - joeynmt.training - Example #2
2022-09-16 03:55:18,852 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 03:55:18,852 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 03:55:18,853 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 03:55:18,867 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 03:55:18,867 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 03:55:18,867 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 03:55:18,868 - INFO - joeynmt.training - Example #3
2022-09-16 03:55:18,868 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 03:55:18,868 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 03:55:18,868 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 03:55:18,883 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 03:55:18,883 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 03:55:18,883 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 03:55:30,677 - INFO - joeynmt.training - Epoch   1, Step:   243100, Batch Loss:     2.581992, Batch Acc: 0.005067, Tokens per Sec:     9435, Lr: 0.000035
2022-09-16 03:55:42,678 - INFO - joeynmt.training - Epoch   1, Step:   243200, Batch Loss:     2.883289, Batch Acc: 0.003882, Tokens per Sec:     9618, Lr: 0.000035
2022-09-16 03:55:54,696 - INFO - joeynmt.training - Epoch   1, Step:   243300, Batch Loss:     2.636946, Batch Acc: 0.004100, Tokens per Sec:     9538, Lr: 0.000035
2022-09-16 03:56:06,775 - INFO - joeynmt.training - Epoch   1, Step:   243400, Batch Loss:     2.475022, Batch Acc: 0.004524, Tokens per Sec:     9443, Lr: 0.000035
2022-09-16 03:56:18,800 - INFO - joeynmt.training - Epoch   1, Step:   243500, Batch Loss:     2.445079, Batch Acc: 0.003918, Tokens per Sec:     9488, Lr: 0.000035
2022-09-16 03:56:30,845 - INFO - joeynmt.training - Epoch   1, Step:   243600, Batch Loss:     2.468909, Batch Acc: 0.004740, Tokens per Sec:     9371, Lr: 0.000035
2022-09-16 03:56:42,896 - INFO - joeynmt.training - Epoch   1, Step:   243700, Batch Loss:     2.580335, Batch Acc: 0.004039, Tokens per Sec:     9410, Lr: 0.000035
2022-09-16 03:56:54,910 - INFO - joeynmt.training - Epoch   1, Step:   243800, Batch Loss:     2.575410, Batch Acc: 0.004297, Tokens per Sec:     9434, Lr: 0.000035
2022-09-16 03:57:06,968 - INFO - joeynmt.training - Epoch   1, Step:   243900, Batch Loss:     2.598453, Batch Acc: 0.003776, Tokens per Sec:     9489, Lr: 0.000035
2022-09-16 03:57:19,028 - INFO - joeynmt.training - Epoch   1, Step:   244000, Batch Loss:     2.690273, Batch Acc: 0.004192, Tokens per Sec:     9515, Lr: 0.000035
2022-09-16 03:57:19,029 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 04:06:32,237 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 04:06:32,238 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.11, loss:   2.51, ppl:  12.30, acc:   0.52, generation: 543.3247[sec], evaluation: 9.3586[sec]
2022-09-16 04:06:32,244 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 04:06:32,689 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/241000.ckpt
2022-09-16 04:06:32,717 - INFO - joeynmt.training - Example #0
2022-09-16 04:06:32,717 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 04:06:32,717 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 04:06:32,717 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-16 04:06:32,735 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 04:06:32,735 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 04:06:32,735 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 04:06:32,735 - INFO - joeynmt.training - Example #1
2022-09-16 04:06:32,735 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 04:06:32,735 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 04:06:32,735 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 04:06:32,750 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 04:06:32,750 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 04:06:32,751 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 04:06:32,751 - INFO - joeynmt.training - Example #2
2022-09-16 04:06:32,751 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 04:06:32,751 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 04:06:32,751 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 04:06:32,766 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 04:06:32,766 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 04:06:32,766 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 04:06:32,766 - INFO - joeynmt.training - Example #3
2022-09-16 04:06:32,767 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 04:06:32,767 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 04:06:32,767 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 04:06:32,782 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 04:06:32,782 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 04:06:32,782 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 04:06:44,875 - INFO - joeynmt.training - Epoch   1, Step:   244100, Batch Loss:     2.631993, Batch Acc: 0.004507, Tokens per Sec:     8994, Lr: 0.000035
2022-09-16 04:06:56,927 - INFO - joeynmt.training - Epoch   1, Step:   244200, Batch Loss:     2.508167, Batch Acc: 0.004693, Tokens per Sec:     9547, Lr: 0.000035
2022-09-16 04:07:08,990 - INFO - joeynmt.training - Epoch   1, Step:   244300, Batch Loss:     2.709503, Batch Acc: 0.004778, Tokens per Sec:     9646, Lr: 0.000035
2022-09-16 04:07:21,074 - INFO - joeynmt.training - Epoch   1, Step:   244400, Batch Loss:     2.631922, Batch Acc: 0.004237, Tokens per Sec:     9532, Lr: 0.000035
2022-09-16 04:07:33,121 - INFO - joeynmt.training - Epoch   1, Step:   244500, Batch Loss:     2.372328, Batch Acc: 0.004785, Tokens per Sec:     9559, Lr: 0.000035
2022-09-16 04:07:45,184 - INFO - joeynmt.training - Epoch   1, Step:   244600, Batch Loss:     2.665968, Batch Acc: 0.004977, Tokens per Sec:     9677, Lr: 0.000035
2022-09-16 04:07:57,282 - INFO - joeynmt.training - Epoch   1, Step:   244700, Batch Loss:     2.429275, Batch Acc: 0.004575, Tokens per Sec:     9665, Lr: 0.000035
2022-09-16 04:08:09,389 - INFO - joeynmt.training - Epoch   1, Step:   244800, Batch Loss:     2.609001, Batch Acc: 0.004532, Tokens per Sec:     9331, Lr: 0.000035
2022-09-16 04:08:21,667 - INFO - joeynmt.training - Epoch   1, Step:   244900, Batch Loss:     2.696451, Batch Acc: 0.004004, Tokens per Sec:     9338, Lr: 0.000035
2022-09-16 04:08:33,454 - INFO - joeynmt.training - Epoch   1, Step:   245000, Batch Loss:     2.484469, Batch Acc: 0.004837, Tokens per Sec:     9647, Lr: 0.000035
2022-09-16 04:08:33,455 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 04:17:09,282 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 04:17:09,284 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.08, loss:   2.51, ppl:  12.28, acc:   0.52, generation: 505.9972[sec], evaluation: 9.3199[sec]
2022-09-16 04:17:09,289 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 04:17:09,735 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/240000.ckpt
2022-09-16 04:17:09,763 - INFO - joeynmt.training - Example #0
2022-09-16 04:17:09,763 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 04:17:09,763 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 04:17:09,763 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-16 04:17:09,780 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 04:17:09,780 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 04:17:09,780 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 04:17:09,781 - INFO - joeynmt.training - Example #1
2022-09-16 04:17:09,781 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 04:17:09,781 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 04:17:09,781 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 04:17:09,795 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 04:17:09,796 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 04:17:09,796 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 04:17:09,796 - INFO - joeynmt.training - Example #2
2022-09-16 04:17:09,796 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 04:17:09,796 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 04:17:09,796 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'गति', 'और', 'इ@@', 'म', '</s>']
2022-09-16 04:17:09,811 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 04:17:09,811 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 04:17:09,811 - INFO - joeynmt.training - 	Hypothesis: अभिनय , गति और इम
2022-09-16 04:17:09,811 - INFO - joeynmt.training - Example #3
2022-09-16 04:17:09,811 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 04:17:09,811 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 04:17:09,811 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 04:17:09,826 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 04:17:09,826 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 04:17:09,826 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 04:17:21,690 - INFO - joeynmt.training - Epoch   1, Step:   245100, Batch Loss:     2.565839, Batch Acc: 0.004852, Tokens per Sec:     9356, Lr: 0.000035
2022-09-16 04:17:33,472 - INFO - joeynmt.training - Epoch   1, Step:   245200, Batch Loss:     2.590928, Batch Acc: 0.005240, Tokens per Sec:     9767, Lr: 0.000035
2022-09-16 04:17:45,301 - INFO - joeynmt.training - Epoch   1, Step:   245300, Batch Loss:     2.537253, Batch Acc: 0.005205, Tokens per Sec:     9761, Lr: 0.000035
2022-09-16 04:17:57,082 - INFO - joeynmt.training - Epoch   1, Step:   245400, Batch Loss:     2.717219, Batch Acc: 0.003981, Tokens per Sec:     9574, Lr: 0.000035
2022-09-16 04:18:08,907 - INFO - joeynmt.training - Epoch   1, Step:   245500, Batch Loss:     2.675428, Batch Acc: 0.003921, Tokens per Sec:     9814, Lr: 0.000035
2022-09-16 04:18:20,741 - INFO - joeynmt.training - Epoch   1, Step:   245600, Batch Loss:     2.602605, Batch Acc: 0.004350, Tokens per Sec:     9732, Lr: 0.000035
2022-09-16 04:18:32,549 - INFO - joeynmt.training - Epoch   1, Step:   245700, Batch Loss:     2.629860, Batch Acc: 0.004721, Tokens per Sec:     9724, Lr: 0.000035
2022-09-16 04:18:44,325 - INFO - joeynmt.training - Epoch   1, Step:   245800, Batch Loss:     2.629183, Batch Acc: 0.005089, Tokens per Sec:     9828, Lr: 0.000035
2022-09-16 04:18:56,116 - INFO - joeynmt.training - Epoch   1, Step:   245900, Batch Loss:     2.557877, Batch Acc: 0.004423, Tokens per Sec:     9837, Lr: 0.000035
2022-09-16 04:19:07,940 - INFO - joeynmt.training - Epoch   1, Step:   246000, Batch Loss:     2.630361, Batch Acc: 0.005146, Tokens per Sec:     9827, Lr: 0.000035
2022-09-16 04:19:07,941 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 04:27:51,423 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 04:27:51,424 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.19, loss:   2.51, ppl:  12.28, acc:   0.52, generation: 513.6861[sec], evaluation: 9.2890[sec]
2022-09-16 04:27:51,868 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/242000.ckpt
2022-09-16 04:27:51,896 - INFO - joeynmt.training - Example #0
2022-09-16 04:27:51,896 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 04:27:51,896 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 04:27:51,896 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'शामिल', 'किया', 'गया', 'है', '</s>']
2022-09-16 04:27:51,913 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 04:27:51,913 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 04:27:51,913 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को शामिल किया गया है
2022-09-16 04:27:51,913 - INFO - joeynmt.training - Example #1
2022-09-16 04:27:51,913 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 04:27:51,913 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 04:27:51,913 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 04:27:51,928 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 04:27:51,929 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 04:27:51,929 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 04:27:51,929 - INFO - joeynmt.training - Example #2
2022-09-16 04:27:51,929 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 04:27:51,929 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 04:27:51,929 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 04:27:51,944 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 04:27:51,944 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 04:27:51,944 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 04:27:51,944 - INFO - joeynmt.training - Example #3
2022-09-16 04:27:51,944 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 04:27:51,944 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 04:27:51,944 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 04:27:51,959 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 04:27:51,959 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 04:27:51,959 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 04:28:03,829 - INFO - joeynmt.training - Epoch   1, Step:   246100, Batch Loss:     2.850986, Batch Acc: 0.003531, Tokens per Sec:     9295, Lr: 0.000035
2022-09-16 04:28:15,623 - INFO - joeynmt.training - Epoch   1, Step:   246200, Batch Loss:     2.746631, Batch Acc: 0.004124, Tokens per Sec:     9685, Lr: 0.000035
2022-09-16 04:28:27,484 - INFO - joeynmt.training - Epoch   1, Step:   246300, Batch Loss:     2.673350, Batch Acc: 0.004025, Tokens per Sec:     9782, Lr: 0.000035
2022-09-16 04:28:39,289 - INFO - joeynmt.training - Epoch   1, Step:   246400, Batch Loss:     2.517528, Batch Acc: 0.004436, Tokens per Sec:     9569, Lr: 0.000035
2022-09-16 04:28:46,261 - INFO - joeynmt.training - Epoch   1: total training loss 38335.07
2022-09-16 04:28:46,261 - INFO - joeynmt.training - Training ended after   1 epochs.
2022-09-16 04:28:46,261 - INFO - joeynmt.training - Best validation result (greedy) at step   245000:  12.28 ppl.
2022-09-16 04:28:46,262 - INFO - joeynmt.training - Loading from ckpt file: /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt
2022-09-16 04:28:46,278 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 04:28:46,278 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 04:28:46,621 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 04:28:46,625 - INFO - joeynmt.model - Total params: 19302144
2022-09-16 04:28:46,626 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2022-09-16 04:28:46,909 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 04:28:46,974 - INFO - joeynmt.prediction - Decoding on dev set...
2022-09-16 04:28:46,975 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 04:53:27,556 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 04:53:27,557 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  23.51, generation: 1470.1785[sec], evaluation: 9.8963[sec]
2022-09-16 04:53:27,616 - INFO - joeynmt.prediction - Translations saved to: /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/00245000.hyps.dev.
2022-09-16 04:53:27,616 - INFO - joeynmt.prediction - Decoding on test set...
2022-09-16 04:53:27,617 - INFO - joeynmt.prediction - Predicting 40858 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:17:13,125 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 05:17:13,126 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  22.85, generation: 1414.9831[sec], evaluation: 10.0199[sec]
2022-09-16 05:17:13,193 - INFO - joeynmt.prediction - Translations saved to: /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/00245000.hyps.test.
2022-09-16 05:17:13,205 - INFO - joeynmt.training - ACTIVE LEARNING MODEL END - LEAST CONFIDENCE 0
2022-09-16 05:17:13,206 - INFO - joeynmt.training - ACTIVE LEARNING MODEL START - LEAST CONFIDENCE 1
2022-09-16 05:17:13,207 - INFO - joeynmt.training - Random Indices picked: [121958 146867 131932 365838 259178 119879 110268 207892  54886 137337] length: 26753
2022-09-16 05:17:13,207 - INFO - joeynmt.training - Processing Predictions on Batch 0/105
2022-09-16 05:17:16,952 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:17:16,952 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:17:17,286 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:17:17,559 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:17:17,683 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:17:17,684 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:17:17,721 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:17:21,737 - INFO - joeynmt.prediction - Generation took 4.0034[sec]. (No references given)
2022-09-16 05:17:21,750 - INFO - joeynmt.training - Processing Predictions on Batch 1/105
2022-09-16 05:17:25,522 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:17:25,522 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:17:25,865 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:17:26,129 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:17:26,254 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:17:26,254 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:17:26,289 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:17:29,896 - INFO - joeynmt.prediction - Generation took 3.5959[sec]. (No references given)
2022-09-16 05:17:29,909 - INFO - joeynmt.training - Processing Predictions on Batch 2/105
2022-09-16 05:17:33,605 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:17:33,605 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:17:33,941 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:17:34,207 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:17:34,332 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:17:34,332 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:17:34,369 - INFO - joeynmt.prediction - Predicting 240 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:17:37,710 - INFO - joeynmt.prediction - Generation took 3.3296[sec]. (No references given)
2022-09-16 05:17:37,724 - INFO - joeynmt.training - Processing Predictions on Batch 3/105
2022-09-16 05:17:41,341 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:17:41,341 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:17:41,685 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:17:41,948 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:17:42,072 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:17:42,072 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:17:42,110 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:17:45,745 - INFO - joeynmt.prediction - Generation took 3.6238[sec]. (No references given)
2022-09-16 05:17:45,757 - INFO - joeynmt.training - Processing Predictions on Batch 4/105
2022-09-16 05:17:49,395 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:17:49,395 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:17:49,730 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:17:50,001 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:17:50,125 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:17:50,126 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:17:50,160 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:17:53,388 - INFO - joeynmt.prediction - Generation took 3.2176[sec]. (No references given)
2022-09-16 05:17:53,403 - INFO - joeynmt.training - Processing Predictions on Batch 5/105
2022-09-16 05:17:57,097 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:17:57,097 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:17:57,439 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:17:57,710 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:17:57,834 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:17:57,834 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:17:57,874 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:18:01,472 - INFO - joeynmt.prediction - Generation took 3.5849[sec]. (No references given)
2022-09-16 05:18:01,487 - INFO - joeynmt.training - Processing Predictions on Batch 6/105
2022-09-16 05:18:05,225 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:18:05,225 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:18:05,568 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:18:05,837 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:18:05,962 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:18:05,962 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:18:05,998 - INFO - joeynmt.prediction - Predicting 241 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:18:09,712 - INFO - joeynmt.prediction - Generation took 3.7021[sec]. (No references given)
2022-09-16 05:18:09,725 - INFO - joeynmt.training - Processing Predictions on Batch 7/105
2022-09-16 05:18:13,434 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:18:13,434 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:18:13,778 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:18:14,047 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:18:14,171 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:18:14,172 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:18:14,211 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:18:18,564 - INFO - joeynmt.prediction - Generation took 4.3401[sec]. (No references given)
2022-09-16 05:18:18,578 - INFO - joeynmt.training - Processing Predictions on Batch 8/105
2022-09-16 05:18:22,242 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:18:22,242 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:18:22,585 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:18:22,859 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:18:22,984 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:18:22,984 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:18:23,019 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:18:27,139 - INFO - joeynmt.prediction - Generation took 4.1084[sec]. (No references given)
2022-09-16 05:18:27,153 - INFO - joeynmt.training - Processing Predictions on Batch 9/105
2022-09-16 05:18:30,829 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:18:30,829 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:18:31,173 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:18:31,442 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:18:31,566 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:18:31,566 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:18:31,605 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:18:35,342 - INFO - joeynmt.prediction - Generation took 3.7243[sec]. (No references given)
2022-09-16 05:18:35,357 - INFO - joeynmt.training - Processing Predictions on Batch 10/105
2022-09-16 05:18:39,030 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:18:39,030 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:18:39,380 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:18:39,636 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:18:39,760 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:18:39,760 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:18:39,793 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:18:43,235 - INFO - joeynmt.prediction - Generation took 3.4312[sec]. (No references given)
2022-09-16 05:18:43,247 - INFO - joeynmt.training - Processing Predictions on Batch 11/105
2022-09-16 05:18:46,923 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:18:46,923 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:18:47,256 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:18:47,532 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:18:47,658 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:18:47,658 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:18:47,695 - INFO - joeynmt.prediction - Predicting 239 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:18:51,323 - INFO - joeynmt.prediction - Generation took 3.6157[sec]. (No references given)
2022-09-16 05:18:51,338 - INFO - joeynmt.training - Processing Predictions on Batch 12/105
2022-09-16 05:18:55,041 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:18:55,041 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:18:55,375 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:18:55,648 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:18:55,774 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:18:55,775 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:18:55,808 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:18:59,488 - INFO - joeynmt.prediction - Generation took 3.6676[sec]. (No references given)
2022-09-16 05:18:59,502 - INFO - joeynmt.training - Processing Predictions on Batch 13/105
2022-09-16 05:19:03,193 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:19:03,194 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:19:03,538 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:19:03,806 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:19:03,933 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:19:03,933 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:19:03,966 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:19:07,759 - INFO - joeynmt.prediction - Generation took 3.7817[sec]. (No references given)
2022-09-16 05:19:07,773 - INFO - joeynmt.training - Processing Predictions on Batch 14/105
2022-09-16 05:19:11,456 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:19:11,456 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:19:11,800 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:19:12,060 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:19:12,186 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:19:12,186 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:19:12,225 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:19:16,327 - INFO - joeynmt.prediction - Generation took 4.0905[sec]. (No references given)
2022-09-16 05:19:16,339 - INFO - joeynmt.training - Processing Predictions on Batch 15/105
2022-09-16 05:19:20,007 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:19:20,007 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:19:20,342 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:19:20,611 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:19:20,735 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:19:20,735 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:19:20,770 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:19:24,647 - INFO - joeynmt.prediction - Generation took 3.8648[sec]. (No references given)
2022-09-16 05:19:24,662 - INFO - joeynmt.training - Processing Predictions on Batch 16/105
2022-09-16 05:19:28,325 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:19:28,325 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:19:28,676 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:19:28,931 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:19:29,055 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:19:29,055 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:19:29,087 - INFO - joeynmt.prediction - Predicting 224 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:19:32,156 - INFO - joeynmt.prediction - Generation took 3.0585[sec]. (No references given)
2022-09-16 05:19:32,168 - INFO - joeynmt.training - Processing Predictions on Batch 17/105
2022-09-16 05:19:35,821 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:19:35,822 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:19:36,157 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:19:36,426 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:19:36,551 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:19:36,551 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:19:36,588 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:19:40,541 - INFO - joeynmt.prediction - Generation took 3.9406[sec]. (No references given)
2022-09-16 05:19:40,555 - INFO - joeynmt.training - Processing Predictions on Batch 18/105
2022-09-16 05:19:44,211 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:19:44,211 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:19:44,545 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:19:44,817 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:19:44,943 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:19:44,943 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:19:44,980 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:19:49,060 - INFO - joeynmt.prediction - Generation took 4.0676[sec]. (No references given)
2022-09-16 05:19:49,074 - INFO - joeynmt.training - Processing Predictions on Batch 19/105
2022-09-16 05:19:52,746 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:19:52,746 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:19:53,089 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:19:53,361 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:19:53,485 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:19:53,485 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:19:53,523 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:19:57,363 - INFO - joeynmt.prediction - Generation took 3.8279[sec]. (No references given)
2022-09-16 05:19:57,377 - INFO - joeynmt.training - Processing Predictions on Batch 20/105
2022-09-16 05:20:01,037 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:20:01,037 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:20:01,380 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:20:01,654 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:20:01,784 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:20:01,784 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:20:01,827 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:20:06,219 - INFO - joeynmt.prediction - Generation took 4.3774[sec]. (No references given)
2022-09-16 05:20:06,233 - INFO - joeynmt.training - Processing Predictions on Batch 21/105
2022-09-16 05:20:09,893 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:20:09,894 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:20:10,237 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:20:10,508 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:20:10,634 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:20:10,635 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:20:10,666 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:20:15,270 - INFO - joeynmt.prediction - Generation took 4.5918[sec]. (No references given)
2022-09-16 05:20:15,284 - INFO - joeynmt.training - Processing Predictions on Batch 22/105
2022-09-16 05:20:18,998 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:20:18,998 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:20:19,342 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:20:19,613 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:20:19,737 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:20:19,737 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:20:19,775 - INFO - joeynmt.prediction - Predicting 220 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:20:23,581 - INFO - joeynmt.prediction - Generation took 3.7940[sec]. (No references given)
2022-09-16 05:20:23,596 - INFO - joeynmt.training - Processing Predictions on Batch 23/105
2022-09-16 05:20:27,290 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:20:27,290 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:20:27,633 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:20:27,903 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:20:28,027 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:20:28,027 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:20:28,065 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:20:32,009 - INFO - joeynmt.prediction - Generation took 3.9312[sec]. (No references given)
2022-09-16 05:20:32,022 - INFO - joeynmt.training - Processing Predictions on Batch 24/105
2022-09-16 05:20:35,716 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:20:35,717 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:20:36,067 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:20:36,326 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:20:36,450 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:20:36,450 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:20:36,488 - INFO - joeynmt.prediction - Predicting 224 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:20:39,991 - INFO - joeynmt.prediction - Generation took 3.4901[sec]. (No references given)
2022-09-16 05:20:40,002 - INFO - joeynmt.training - Processing Predictions on Batch 25/105
2022-09-16 05:20:43,741 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:20:43,741 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:20:44,074 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:20:44,346 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:20:44,469 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:20:44,470 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:20:44,508 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:20:48,477 - INFO - joeynmt.prediction - Generation took 3.9567[sec]. (No references given)
2022-09-16 05:20:48,491 - INFO - joeynmt.training - Processing Predictions on Batch 26/105
2022-09-16 05:20:52,211 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:20:52,211 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:20:52,554 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:20:52,825 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:20:52,949 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:20:52,950 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:20:52,984 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:20:56,958 - INFO - joeynmt.prediction - Generation took 3.9624[sec]. (No references given)
2022-09-16 05:20:56,972 - INFO - joeynmt.training - Processing Predictions on Batch 27/105
2022-09-16 05:21:00,693 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:21:00,693 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:21:01,036 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:21:01,313 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:21:01,437 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:21:01,437 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:21:01,473 - INFO - joeynmt.prediction - Predicting 234 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:21:06,075 - INFO - joeynmt.prediction - Generation took 4.5905[sec]. (No references given)
2022-09-16 05:21:06,090 - INFO - joeynmt.training - Processing Predictions on Batch 28/105
2022-09-16 05:21:09,788 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:21:09,788 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:21:10,135 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:21:10,404 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:21:10,530 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:21:10,531 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:21:10,567 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:21:14,587 - INFO - joeynmt.prediction - Generation took 4.0080[sec]. (No references given)
2022-09-16 05:21:14,602 - INFO - joeynmt.training - Processing Predictions on Batch 29/105
2022-09-16 05:21:18,309 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:21:18,310 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:21:18,662 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:21:18,923 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:21:19,048 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:21:19,048 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:21:19,084 - INFO - joeynmt.prediction - Predicting 224 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:21:22,704 - INFO - joeynmt.prediction - Generation took 3.6087[sec]. (No references given)
2022-09-16 05:21:22,715 - INFO - joeynmt.training - Processing Predictions on Batch 30/105
2022-09-16 05:21:26,405 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:21:26,405 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:21:26,741 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:21:27,009 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:21:27,134 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:21:27,134 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:21:27,171 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:21:30,403 - INFO - joeynmt.prediction - Generation took 3.2208[sec]. (No references given)
2022-09-16 05:21:30,419 - INFO - joeynmt.training - Processing Predictions on Batch 31/105
2022-09-16 05:21:34,095 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:21:34,095 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:21:34,443 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:21:34,705 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:21:34,829 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:21:34,830 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:21:34,869 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:21:39,077 - INFO - joeynmt.prediction - Generation took 4.1950[sec]. (No references given)
2022-09-16 05:21:39,089 - INFO - joeynmt.training - Processing Predictions on Batch 32/105
2022-09-16 05:21:42,791 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:21:42,791 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:21:43,130 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:21:43,388 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:21:43,512 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:21:43,512 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:21:43,546 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:21:47,120 - INFO - joeynmt.prediction - Generation took 3.5622[sec]. (No references given)
2022-09-16 05:21:47,132 - INFO - joeynmt.training - Processing Predictions on Batch 33/105
2022-09-16 05:21:50,827 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:21:50,827 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:21:51,164 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:21:51,434 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:21:51,559 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:21:51,559 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:21:51,598 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:21:55,464 - INFO - joeynmt.prediction - Generation took 3.8528[sec]. (No references given)
2022-09-16 05:21:55,479 - INFO - joeynmt.training - Processing Predictions on Batch 34/105
2022-09-16 05:21:59,193 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:21:59,193 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:21:59,536 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:21:59,808 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:21:59,932 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:21:59,932 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:21:59,971 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:22:04,213 - INFO - joeynmt.prediction - Generation took 4.2301[sec]. (No references given)
2022-09-16 05:22:04,227 - INFO - joeynmt.training - Processing Predictions on Batch 35/105
2022-09-16 05:22:07,950 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:22:07,950 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:22:08,292 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:22:08,568 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:22:08,693 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:22:08,693 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:22:08,728 - INFO - joeynmt.prediction - Predicting 224 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:22:12,437 - INFO - joeynmt.prediction - Generation took 3.6976[sec]. (No references given)
2022-09-16 05:22:12,451 - INFO - joeynmt.training - Processing Predictions on Batch 36/105
2022-09-16 05:22:16,172 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:22:16,172 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:22:16,517 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:22:16,788 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:22:16,913 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:22:16,913 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:22:16,949 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:22:20,435 - INFO - joeynmt.prediction - Generation took 3.4727[sec]. (No references given)
2022-09-16 05:22:20,449 - INFO - joeynmt.training - Processing Predictions on Batch 37/105
2022-09-16 05:22:24,154 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:22:24,154 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:22:24,499 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:22:24,774 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:22:24,900 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:22:24,900 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:22:24,936 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:22:28,866 - INFO - joeynmt.prediction - Generation took 3.9174[sec]. (No references given)
2022-09-16 05:22:28,881 - INFO - joeynmt.training - Processing Predictions on Batch 38/105
2022-09-16 05:22:32,634 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:22:32,635 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:22:32,979 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:22:33,253 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:22:33,378 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:22:33,378 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:22:33,415 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:22:37,113 - INFO - joeynmt.prediction - Generation took 3.6860[sec]. (No references given)
2022-09-16 05:22:37,127 - INFO - joeynmt.training - Processing Predictions on Batch 39/105
2022-09-16 05:22:40,852 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:22:40,852 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:22:41,196 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:22:41,474 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:22:41,598 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:22:41,598 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:22:41,634 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:22:45,686 - INFO - joeynmt.prediction - Generation took 4.0391[sec]. (No references given)
2022-09-16 05:22:45,700 - INFO - joeynmt.training - Processing Predictions on Batch 40/105
2022-09-16 05:22:49,408 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:22:49,408 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:22:49,751 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:22:50,016 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:22:50,139 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:22:50,139 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:22:50,174 - INFO - joeynmt.prediction - Predicting 218 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:22:53,603 - INFO - joeynmt.prediction - Generation took 3.4180[sec]. (No references given)
2022-09-16 05:22:53,615 - INFO - joeynmt.training - Processing Predictions on Batch 41/105
2022-09-16 05:22:57,337 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:22:57,338 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:22:57,671 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:22:57,939 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:22:58,062 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:22:58,062 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:22:58,098 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:23:02,154 - INFO - joeynmt.prediction - Generation took 4.0446[sec]. (No references given)
2022-09-16 05:23:02,169 - INFO - joeynmt.training - Processing Predictions on Batch 42/105
2022-09-16 05:23:05,879 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:23:05,880 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:23:06,223 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:23:06,488 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:23:06,612 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:23:06,612 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:23:06,648 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:23:10,402 - INFO - joeynmt.prediction - Generation took 3.7424[sec]. (No references given)
2022-09-16 05:23:10,414 - INFO - joeynmt.training - Processing Predictions on Batch 43/105
2022-09-16 05:23:14,129 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:23:14,129 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:23:14,463 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:23:14,732 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:23:14,856 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:23:14,857 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:23:14,896 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:23:18,949 - INFO - joeynmt.prediction - Generation took 4.0407[sec]. (No references given)
2022-09-16 05:23:18,964 - INFO - joeynmt.training - Processing Predictions on Batch 44/105
2022-09-16 05:23:22,715 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:23:22,715 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:23:23,059 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:23:23,323 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:23:23,447 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:23:23,447 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:23:23,484 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:23:27,331 - INFO - joeynmt.prediction - Generation took 3.8348[sec]. (No references given)
2022-09-16 05:23:27,343 - INFO - joeynmt.training - Processing Predictions on Batch 45/105
2022-09-16 05:23:31,065 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:23:31,065 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:23:31,401 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:23:31,668 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:23:31,792 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:23:31,792 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:23:31,825 - INFO - joeynmt.prediction - Predicting 218 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:23:35,057 - INFO - joeynmt.prediction - Generation took 3.2214[sec]. (No references given)
2022-09-16 05:23:35,071 - INFO - joeynmt.training - Processing Predictions on Batch 46/105
2022-09-16 05:23:38,805 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:23:38,805 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:23:39,148 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:23:39,415 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:23:39,540 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:23:39,540 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:23:39,579 - INFO - joeynmt.prediction - Predicting 236 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:23:43,178 - INFO - joeynmt.prediction - Generation took 3.5881[sec]. (No references given)
2022-09-16 05:23:43,192 - INFO - joeynmt.training - Processing Predictions on Batch 47/105
2022-09-16 05:23:46,908 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:23:46,909 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:23:47,250 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:23:47,523 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:23:47,646 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:23:47,646 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:23:47,682 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:23:51,380 - INFO - joeynmt.prediction - Generation took 3.6858[sec]. (No references given)
2022-09-16 05:23:51,394 - INFO - joeynmt.training - Processing Predictions on Batch 48/105
2022-09-16 05:23:55,088 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:23:55,088 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:23:55,431 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:23:55,701 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:23:55,824 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:23:55,824 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:23:55,862 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:24:00,271 - INFO - joeynmt.prediction - Generation took 4.3972[sec]. (No references given)
2022-09-16 05:24:00,285 - INFO - joeynmt.training - Processing Predictions on Batch 49/105
2022-09-16 05:24:04,020 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:24:04,020 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:24:04,363 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:24:04,641 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:24:04,764 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:24:04,765 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:24:04,799 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:24:08,159 - INFO - joeynmt.prediction - Generation took 3.3491[sec]. (No references given)
2022-09-16 05:24:08,174 - INFO - joeynmt.training - Processing Predictions on Batch 50/105
2022-09-16 05:24:11,907 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:24:11,908 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:24:12,250 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:24:12,518 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:24:12,642 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:24:12,642 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:24:12,676 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:24:15,915 - INFO - joeynmt.prediction - Generation took 3.2273[sec]. (No references given)
2022-09-16 05:24:15,928 - INFO - joeynmt.training - Processing Predictions on Batch 51/105
2022-09-16 05:24:19,649 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:24:19,650 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:24:20,000 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:24:20,262 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:24:20,386 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:24:20,387 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:24:20,423 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:24:23,910 - INFO - joeynmt.prediction - Generation took 3.4748[sec]. (No references given)
2022-09-16 05:24:23,921 - INFO - joeynmt.training - Processing Predictions on Batch 52/105
2022-09-16 05:24:27,656 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:24:27,656 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:24:27,992 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:24:28,256 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:24:28,381 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:24:28,381 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:24:28,413 - INFO - joeynmt.prediction - Predicting 223 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:24:32,044 - INFO - joeynmt.prediction - Generation took 3.6201[sec]. (No references given)
2022-09-16 05:24:32,058 - INFO - joeynmt.training - Processing Predictions on Batch 53/105
2022-09-16 05:24:35,775 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:24:35,775 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:24:36,119 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:24:36,388 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:24:36,513 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:24:36,513 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:24:36,545 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:24:39,817 - INFO - joeynmt.prediction - Generation took 3.2616[sec]. (No references given)
2022-09-16 05:24:39,831 - INFO - joeynmt.training - Processing Predictions on Batch 54/105
2022-09-16 05:24:43,542 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:24:43,542 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:24:43,886 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:24:44,160 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:24:44,284 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:24:44,284 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:24:44,323 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:24:48,237 - INFO - joeynmt.prediction - Generation took 3.9017[sec]. (No references given)
2022-09-16 05:24:48,252 - INFO - joeynmt.training - Processing Predictions on Batch 55/105
2022-09-16 05:24:51,955 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:24:51,956 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:24:52,299 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:24:52,561 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:24:52,685 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:24:52,685 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:24:52,724 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:24:56,434 - INFO - joeynmt.prediction - Generation took 3.6983[sec]. (No references given)
2022-09-16 05:24:56,446 - INFO - joeynmt.training - Processing Predictions on Batch 56/105
2022-09-16 05:25:00,149 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:25:00,150 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:25:00,486 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:25:00,756 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:25:00,880 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:25:00,881 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:25:00,918 - INFO - joeynmt.prediction - Predicting 238 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:25:04,334 - INFO - joeynmt.prediction - Generation took 3.4038[sec]. (No references given)
2022-09-16 05:25:04,348 - INFO - joeynmt.training - Processing Predictions on Batch 57/105
2022-09-16 05:25:08,041 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:25:08,041 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:25:08,384 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:25:08,656 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:25:08,780 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:25:08,780 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:25:08,819 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:25:12,246 - INFO - joeynmt.prediction - Generation took 3.4151[sec]. (No references given)
2022-09-16 05:25:12,260 - INFO - joeynmt.training - Processing Predictions on Batch 58/105
2022-09-16 05:25:15,967 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:25:15,967 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:25:16,309 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:25:16,585 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:25:16,708 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:25:16,708 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:25:16,747 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:25:20,527 - INFO - joeynmt.prediction - Generation took 3.7687[sec]. (No references given)
2022-09-16 05:25:20,542 - INFO - joeynmt.training - Processing Predictions on Batch 59/105
2022-09-16 05:25:24,261 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:25:24,262 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:25:24,604 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:25:24,877 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:25:25,001 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:25:25,001 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:25:25,033 - INFO - joeynmt.prediction - Predicting 213 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:25:28,452 - INFO - joeynmt.prediction - Generation took 3.4085[sec]. (No references given)
2022-09-16 05:25:28,466 - INFO - joeynmt.training - Processing Predictions on Batch 60/105
2022-09-16 05:25:32,182 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:25:32,183 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:25:32,525 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:25:32,804 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:25:32,926 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:25:32,927 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:25:32,960 - INFO - joeynmt.prediction - Predicting 237 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:25:36,491 - INFO - joeynmt.prediction - Generation took 3.5192[sec]. (No references given)
2022-09-16 05:25:36,505 - INFO - joeynmt.training - Processing Predictions on Batch 61/105
2022-09-16 05:25:40,223 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:25:40,224 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:25:40,566 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:25:40,838 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:25:40,963 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:25:40,963 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:25:41,000 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:25:44,667 - INFO - joeynmt.prediction - Generation took 3.6532[sec]. (No references given)
2022-09-16 05:25:44,680 - INFO - joeynmt.training - Processing Predictions on Batch 62/105
2022-09-16 05:25:48,388 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:25:48,388 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:25:48,732 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:25:48,996 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:25:49,594 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:25:49,595 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:25:49,629 - INFO - joeynmt.prediction - Predicting 210 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:25:53,039 - INFO - joeynmt.prediction - Generation took 3.3986[sec]. (No references given)
2022-09-16 05:25:53,050 - INFO - joeynmt.training - Processing Predictions on Batch 63/105
2022-09-16 05:25:56,894 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:25:56,894 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:25:57,229 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:25:57,488 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:25:57,612 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:25:57,612 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:25:57,649 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:26:01,205 - INFO - joeynmt.prediction - Generation took 3.5442[sec]. (No references given)
2022-09-16 05:26:01,219 - INFO - joeynmt.training - Processing Predictions on Batch 64/105
2022-09-16 05:26:05,094 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:26:05,094 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:26:05,437 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:26:05,715 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:26:05,838 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:26:05,838 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:26:05,872 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:26:09,367 - INFO - joeynmt.prediction - Generation took 3.4840[sec]. (No references given)
2022-09-16 05:26:09,382 - INFO - joeynmt.training - Processing Predictions on Batch 65/105
2022-09-16 05:26:13,206 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:26:13,207 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:26:13,559 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:26:13,822 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:26:13,946 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:26:13,946 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:26:13,981 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:26:18,026 - INFO - joeynmt.prediction - Generation took 4.0337[sec]. (No references given)
2022-09-16 05:26:18,038 - INFO - joeynmt.training - Processing Predictions on Batch 66/105
2022-09-16 05:26:21,878 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:26:21,878 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:26:22,215 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:26:22,481 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:26:22,605 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:26:22,605 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:26:22,644 - INFO - joeynmt.prediction - Predicting 222 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:26:25,923 - INFO - joeynmt.prediction - Generation took 3.2678[sec]. (No references given)
2022-09-16 05:26:25,937 - INFO - joeynmt.training - Processing Predictions on Batch 67/105
2022-09-16 05:26:29,774 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:26:29,774 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:26:30,108 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:26:30,380 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:26:30,503 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:26:30,503 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:26:30,539 - INFO - joeynmt.prediction - Predicting 223 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:26:33,913 - INFO - joeynmt.prediction - Generation took 3.3629[sec]. (No references given)
2022-09-16 05:26:33,926 - INFO - joeynmt.training - Processing Predictions on Batch 68/105
2022-09-16 05:26:37,733 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:26:37,733 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:26:38,076 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:26:38,346 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:26:38,469 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:26:38,470 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:26:38,504 - INFO - joeynmt.prediction - Predicting 216 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:26:42,192 - INFO - joeynmt.prediction - Generation took 3.6766[sec]. (No references given)
2022-09-16 05:26:42,206 - INFO - joeynmt.training - Processing Predictions on Batch 69/105
2022-09-16 05:26:46,017 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:26:46,017 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:26:46,360 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:26:46,632 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:26:46,754 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:26:46,755 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:26:46,788 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:26:50,500 - INFO - joeynmt.prediction - Generation took 3.7004[sec]. (No references given)
2022-09-16 05:26:50,513 - INFO - joeynmt.training - Processing Predictions on Batch 70/105
2022-09-16 05:26:54,289 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:26:54,289 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:26:54,632 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:26:54,901 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:26:55,024 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:26:55,024 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:26:55,059 - INFO - joeynmt.prediction - Predicting 230 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:26:59,257 - INFO - joeynmt.prediction - Generation took 4.1855[sec]. (No references given)
2022-09-16 05:26:59,270 - INFO - joeynmt.training - Processing Predictions on Batch 71/105
2022-09-16 05:27:03,055 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:27:03,055 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:27:03,398 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:27:03,670 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:27:03,795 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:27:03,795 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:27:03,830 - INFO - joeynmt.prediction - Predicting 220 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:27:07,135 - INFO - joeynmt.prediction - Generation took 3.2930[sec]. (No references given)
2022-09-16 05:27:07,149 - INFO - joeynmt.training - Processing Predictions on Batch 72/105
2022-09-16 05:27:10,934 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:27:10,934 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:27:11,276 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:27:11,548 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:27:11,671 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:27:11,671 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:27:11,708 - INFO - joeynmt.prediction - Predicting 233 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:27:15,431 - INFO - joeynmt.prediction - Generation took 3.7103[sec]. (No references given)
2022-09-16 05:27:15,444 - INFO - joeynmt.training - Processing Predictions on Batch 73/105
2022-09-16 05:27:19,218 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:27:19,219 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:27:19,572 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:27:19,834 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:27:19,958 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:27:19,959 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:27:19,996 - INFO - joeynmt.prediction - Predicting 224 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:27:23,750 - INFO - joeynmt.prediction - Generation took 3.7420[sec]. (No references given)
2022-09-16 05:27:23,761 - INFO - joeynmt.training - Processing Predictions on Batch 74/105
2022-09-16 05:27:27,551 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:27:27,551 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:27:27,885 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:27:28,149 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:27:28,273 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:27:28,273 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:27:28,309 - INFO - joeynmt.prediction - Predicting 228 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:27:32,026 - INFO - joeynmt.prediction - Generation took 3.7043[sec]. (No references given)
2022-09-16 05:27:32,040 - INFO - joeynmt.training - Processing Predictions on Batch 75/105
2022-09-16 05:27:35,813 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:27:35,813 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:27:36,157 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:27:36,422 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:27:36,546 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:27:36,546 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:27:36,580 - INFO - joeynmt.prediction - Predicting 209 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:27:40,149 - INFO - joeynmt.prediction - Generation took 3.5586[sec]. (No references given)
2022-09-16 05:27:40,161 - INFO - joeynmt.training - Processing Predictions on Batch 76/105
2022-09-16 05:27:43,922 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:27:43,922 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:27:44,258 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:27:44,988 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:27:45,112 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:27:45,112 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:27:45,150 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:27:48,675 - INFO - joeynmt.prediction - Generation took 3.5143[sec]. (No references given)
2022-09-16 05:27:48,687 - INFO - joeynmt.training - Processing Predictions on Batch 77/105
2022-09-16 05:27:52,548 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:27:52,548 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:27:52,882 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:27:53,159 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:27:53,281 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:27:53,281 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:27:53,314 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:27:56,703 - INFO - joeynmt.prediction - Generation took 3.3788[sec]. (No references given)
2022-09-16 05:27:56,717 - INFO - joeynmt.training - Processing Predictions on Batch 78/105
2022-09-16 05:28:00,559 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:28:00,559 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:28:00,902 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:28:01,175 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:28:01,297 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:28:01,297 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:28:01,332 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:28:05,560 - INFO - joeynmt.prediction - Generation took 4.2160[sec]. (No references given)
2022-09-16 05:28:05,576 - INFO - joeynmt.training - Processing Predictions on Batch 79/105
2022-09-16 05:28:09,421 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:28:09,421 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:28:09,764 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:28:10,034 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:28:10,156 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:28:10,156 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:28:10,189 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:28:13,403 - INFO - joeynmt.prediction - Generation took 3.2027[sec]. (No references given)
2022-09-16 05:28:13,416 - INFO - joeynmt.training - Processing Predictions on Batch 80/105
2022-09-16 05:28:17,262 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:28:17,262 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:28:17,605 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:28:17,875 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:28:17,998 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:28:17,998 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:28:18,033 - INFO - joeynmt.prediction - Predicting 223 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:28:22,140 - INFO - joeynmt.prediction - Generation took 4.0957[sec]. (No references given)
2022-09-16 05:28:22,154 - INFO - joeynmt.training - Processing Predictions on Batch 81/105
2022-09-16 05:28:26,022 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:28:26,022 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:28:26,364 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:28:26,643 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:28:26,765 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:28:26,765 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:28:26,801 - INFO - joeynmt.prediction - Predicting 235 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:28:30,707 - INFO - joeynmt.prediction - Generation took 3.8933[sec]. (No references given)
2022-09-16 05:28:30,721 - INFO - joeynmt.training - Processing Predictions on Batch 82/105
2022-09-16 05:28:34,575 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:28:34,575 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:28:34,918 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:28:35,193 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:28:35,317 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:28:35,318 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:28:35,353 - INFO - joeynmt.prediction - Predicting 219 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:28:38,946 - INFO - joeynmt.prediction - Generation took 3.5818[sec]. (No references given)
2022-09-16 05:28:38,961 - INFO - joeynmt.training - Processing Predictions on Batch 83/105
2022-09-16 05:28:42,809 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:28:42,809 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:28:43,151 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:28:43,417 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:28:43,541 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:28:43,541 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:28:43,578 - INFO - joeynmt.prediction - Predicting 217 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:28:47,431 - INFO - joeynmt.prediction - Generation took 3.8418[sec]. (No references given)
2022-09-16 05:28:47,443 - INFO - joeynmt.training - Processing Predictions on Batch 84/105
2022-09-16 05:28:51,272 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:28:51,273 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:28:51,607 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:28:51,868 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:28:51,990 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:28:51,991 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:28:52,027 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:28:55,604 - INFO - joeynmt.prediction - Generation took 3.5647[sec]. (No references given)
2022-09-16 05:28:55,618 - INFO - joeynmt.training - Processing Predictions on Batch 85/105
2022-09-16 05:28:59,428 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:28:59,428 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:28:59,771 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:29:00,042 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:29:00,167 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:29:00,167 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:29:00,200 - INFO - joeynmt.prediction - Predicting 215 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:29:03,650 - INFO - joeynmt.prediction - Generation took 3.4384[sec]. (No references given)
2022-09-16 05:29:03,664 - INFO - joeynmt.training - Processing Predictions on Batch 86/105
2022-09-16 05:29:07,465 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:29:07,465 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:29:07,808 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:29:08,086 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:29:08,210 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:29:08,210 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:29:08,246 - INFO - joeynmt.prediction - Predicting 219 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:29:11,764 - INFO - joeynmt.prediction - Generation took 3.5064[sec]. (No references given)
2022-09-16 05:29:11,778 - INFO - joeynmt.training - Processing Predictions on Batch 87/105
2022-09-16 05:29:15,573 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:29:15,573 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:29:15,915 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:29:16,179 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:29:16,303 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:29:16,303 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:29:16,340 - INFO - joeynmt.prediction - Predicting 222 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:29:20,499 - INFO - joeynmt.prediction - Generation took 4.1477[sec]. (No references given)
2022-09-16 05:29:20,511 - INFO - joeynmt.training - Processing Predictions on Batch 88/105
2022-09-16 05:29:24,284 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:29:24,284 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:29:24,618 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:29:24,887 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:29:25,011 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:29:25,011 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:29:25,047 - INFO - joeynmt.prediction - Predicting 213 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:29:28,556 - INFO - joeynmt.prediction - Generation took 3.4970[sec]. (No references given)
2022-09-16 05:29:28,570 - INFO - joeynmt.training - Processing Predictions on Batch 89/105
2022-09-16 05:29:32,326 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:29:32,326 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:29:32,669 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:29:33,415 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:29:33,539 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:29:33,539 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:29:33,579 - INFO - joeynmt.prediction - Predicting 231 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:29:37,343 - INFO - joeynmt.prediction - Generation took 3.7520[sec]. (No references given)
2022-09-16 05:29:37,354 - INFO - joeynmt.training - Processing Predictions on Batch 90/105
2022-09-16 05:29:41,187 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:29:41,188 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:29:41,523 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:29:41,790 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:29:41,913 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:29:41,914 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:29:41,951 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:29:45,641 - INFO - joeynmt.prediction - Generation took 3.6773[sec]. (No references given)
2022-09-16 05:29:45,656 - INFO - joeynmt.training - Processing Predictions on Batch 91/105
2022-09-16 05:29:49,499 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:29:49,500 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:29:49,842 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:29:50,113 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:29:50,236 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:29:50,236 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:29:50,271 - INFO - joeynmt.prediction - Predicting 227 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:29:53,977 - INFO - joeynmt.prediction - Generation took 3.6945[sec]. (No references given)
2022-09-16 05:29:53,991 - INFO - joeynmt.training - Processing Predictions on Batch 92/105
2022-09-16 05:29:57,828 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:29:57,828 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:29:58,170 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:29:58,439 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:29:58,562 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:29:58,562 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:29:58,593 - INFO - joeynmt.prediction - Predicting 221 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:30:02,095 - INFO - joeynmt.prediction - Generation took 3.4907[sec]. (No references given)
2022-09-16 05:30:02,109 - INFO - joeynmt.training - Processing Predictions on Batch 93/105
2022-09-16 05:30:05,945 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:30:05,945 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:30:06,291 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:30:06,565 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:30:06,689 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:30:06,689 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:30:06,724 - INFO - joeynmt.prediction - Predicting 232 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:30:10,345 - INFO - joeynmt.prediction - Generation took 3.6083[sec]. (No references given)
2022-09-16 05:30:10,358 - INFO - joeynmt.training - Processing Predictions on Batch 94/105
2022-09-16 05:30:14,185 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:30:14,185 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:30:14,528 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:30:14,801 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:30:14,925 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:30:14,925 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:30:14,958 - INFO - joeynmt.prediction - Predicting 223 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:30:18,882 - INFO - joeynmt.prediction - Generation took 3.9125[sec]. (No references given)
2022-09-16 05:30:18,896 - INFO - joeynmt.training - Processing Predictions on Batch 95/105
2022-09-16 05:30:22,723 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:30:22,723 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:30:23,069 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:30:23,345 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:30:23,471 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:30:23,471 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:30:23,501 - INFO - joeynmt.prediction - Predicting 218 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:30:26,790 - INFO - joeynmt.prediction - Generation took 3.2777[sec]. (No references given)
2022-09-16 05:30:26,804 - INFO - joeynmt.training - Processing Predictions on Batch 96/105
2022-09-16 05:30:30,637 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:30:30,637 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:30:30,980 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:30:31,253 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:30:31,377 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:30:31,377 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:30:31,408 - INFO - joeynmt.prediction - Predicting 215 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:30:34,808 - INFO - joeynmt.prediction - Generation took 3.3885[sec]. (No references given)
2022-09-16 05:30:34,822 - INFO - joeynmt.training - Processing Predictions on Batch 97/105
2022-09-16 05:30:38,632 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:30:38,632 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:30:38,975 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:30:39,239 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:30:39,363 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:30:39,363 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:30:39,400 - INFO - joeynmt.prediction - Predicting 226 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:30:43,160 - INFO - joeynmt.prediction - Generation took 3.7484[sec]. (No references given)
2022-09-16 05:30:43,171 - INFO - joeynmt.training - Processing Predictions on Batch 98/105
2022-09-16 05:30:46,985 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:30:46,986 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:30:47,319 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:30:47,582 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:30:47,704 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:30:47,704 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:30:47,744 - INFO - joeynmt.prediction - Predicting 229 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:30:51,326 - INFO - joeynmt.prediction - Generation took 3.5707[sec]. (No references given)
2022-09-16 05:30:51,340 - INFO - joeynmt.training - Processing Predictions on Batch 99/105
2022-09-16 05:30:55,130 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:30:55,130 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:30:55,472 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:30:55,743 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:30:55,866 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:30:55,866 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:30:55,899 - INFO - joeynmt.prediction - Predicting 211 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:30:59,380 - INFO - joeynmt.prediction - Generation took 3.4702[sec]. (No references given)
2022-09-16 05:30:59,394 - INFO - joeynmt.training - Processing Predictions on Batch 100/105
2022-09-16 05:31:03,185 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:31:03,185 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:31:03,528 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:31:03,806 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:31:03,928 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:31:03,929 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:31:03,962 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:31:07,399 - INFO - joeynmt.prediction - Generation took 3.4256[sec]. (No references given)
2022-09-16 05:31:07,413 - INFO - joeynmt.training - Processing Predictions on Batch 101/105
2022-09-16 05:31:11,200 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:31:11,200 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:31:11,543 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:31:11,807 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:31:11,930 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:31:11,930 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:31:11,967 - INFO - joeynmt.prediction - Predicting 223 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:31:15,858 - INFO - joeynmt.prediction - Generation took 3.8795[sec]. (No references given)
2022-09-16 05:31:15,870 - INFO - joeynmt.training - Processing Predictions on Batch 102/105
2022-09-16 05:31:19,657 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:31:19,657 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:31:19,991 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:31:20,248 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:31:20,372 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:31:20,372 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:31:20,409 - INFO - joeynmt.prediction - Predicting 222 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:31:25,012 - INFO - joeynmt.prediction - Generation took 4.1065[sec]. (No references given)
2022-09-16 05:31:25,023 - INFO - joeynmt.training - Processing Predictions on Batch 103/105
2022-09-16 05:31:28,755 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:31:28,755 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:31:29,091 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:31:29,355 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:31:29,478 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:31:29,478 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:31:29,516 - INFO - joeynmt.prediction - Predicting 225 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:31:33,168 - INFO - joeynmt.prediction - Generation took 3.6403[sec]. (No references given)
2022-09-16 05:31:33,182 - INFO - joeynmt.training - Processing Predictions on Batch 104/105
2022-09-16 05:31:36,891 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 05:31:36,891 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 05:31:37,234 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 05:31:37,505 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt.
2022-09-16 05:31:37,628 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:31:37,629 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 05:31:37,646 - INFO - joeynmt.prediction - Predicting 106 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:31:39,982 - INFO - joeynmt.prediction - Generation took 2.3295[sec]. (No references given)
2022-09-16 05:31:40,130 - INFO - joeynmt.training - Final Query Indices picked: [51024, 180703, 179428, 223853, 275512, 358266, 371930, 155378, 243942, 8816] length: 10000
2022-09-16 05:31:40,130 - INFO - joeynmt.training - Query the samples, one at a time (interactive/file)
2022-09-16 05:31:43,454 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 4096
	effective batch size (w. parallel & accumulation): 4096
2022-09-16 05:31:43,454 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:40:16,648 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 05:40:16,650 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.19, loss:   2.50, ppl:  12.17, acc:   0.52, generation: 502.9557[sec], evaluation: 9.6461[sec]
2022-09-16 05:40:16,656 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 05:40:17,091 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/244000.ckpt
2022-09-16 05:40:17,119 - INFO - joeynmt.training - Example #0
2022-09-16 05:40:17,119 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 05:40:17,119 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 05:40:17,119 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'शामिल', 'किया', 'गया', 'है', '</s>']
2022-09-16 05:40:17,136 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 05:40:17,137 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 05:40:17,137 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को शामिल किया गया है
2022-09-16 05:40:17,137 - INFO - joeynmt.training - Example #1
2022-09-16 05:40:17,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 05:40:17,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 05:40:17,137 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 05:40:17,152 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 05:40:17,152 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 05:40:17,152 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 05:40:17,152 - INFO - joeynmt.training - Example #2
2022-09-16 05:40:17,152 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 05:40:17,153 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 05:40:17,153 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 05:40:17,168 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 05:40:17,168 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 05:40:17,168 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 05:40:17,168 - INFO - joeynmt.training - Example #3
2022-09-16 05:40:17,168 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 05:40:17,168 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 05:40:17,168 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 05:40:17,183 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 05:40:17,183 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 05:40:17,183 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 05:40:17,244 - INFO - joeynmt.training - EPOCH 1
2022-09-16 05:40:22,428 - INFO - joeynmt.training - Epoch   1, Step:   246500, Batch Loss:     2.685564, Batch Acc: 0.010281, Tokens per Sec:     9028, Lr: 0.000035
2022-09-16 05:40:34,623 - INFO - joeynmt.training - Epoch   1, Step:   246600, Batch Loss:     2.485684, Batch Acc: 0.004908, Tokens per Sec:     9440, Lr: 0.000035
2022-09-16 05:40:46,673 - INFO - joeynmt.training - Epoch   1, Step:   246700, Batch Loss:     2.643685, Batch Acc: 0.004842, Tokens per Sec:     9529, Lr: 0.000035
2022-09-16 05:40:58,716 - INFO - joeynmt.training - Epoch   1, Step:   246800, Batch Loss:     2.540770, Batch Acc: 0.004635, Tokens per Sec:     9459, Lr: 0.000035
2022-09-16 05:41:10,689 - INFO - joeynmt.training - Epoch   1, Step:   246900, Batch Loss:     2.763886, Batch Acc: 0.003328, Tokens per Sec:     9437, Lr: 0.000035
2022-09-16 05:41:22,661 - INFO - joeynmt.training - Epoch   1, Step:   247000, Batch Loss:     2.471522, Batch Acc: 0.003620, Tokens per Sec:     9530, Lr: 0.000035
2022-09-16 05:41:22,661 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 05:50:07,454 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 05:50:07,455 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.28, loss:   2.51, ppl:  12.28, acc:   0.52, generation: 514.6720[sec], evaluation: 9.6079[sec]
2022-09-16 05:50:07,463 - INFO - joeynmt.training - Example #0
2022-09-16 05:50:07,463 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 05:50:07,463 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 05:50:07,463 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'शामिल', 'किया', 'गया', 'है', '</s>']
2022-09-16 05:50:07,480 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 05:50:07,480 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 05:50:07,480 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को शामिल किया गया है
2022-09-16 05:50:07,480 - INFO - joeynmt.training - Example #1
2022-09-16 05:50:07,480 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 05:50:07,480 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 05:50:07,480 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 05:50:07,496 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 05:50:07,496 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 05:50:07,496 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 05:50:07,496 - INFO - joeynmt.training - Example #2
2022-09-16 05:50:07,496 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 05:50:07,496 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 05:50:07,496 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 05:50:07,511 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 05:50:07,511 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 05:50:07,511 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 05:50:07,511 - INFO - joeynmt.training - Example #3
2022-09-16 05:50:07,512 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 05:50:07,512 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 05:50:07,512 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 05:50:07,527 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 05:50:07,527 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 05:50:07,527 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 05:50:19,571 - INFO - joeynmt.training - Epoch   1, Step:   247100, Batch Loss:     2.568674, Batch Acc: 0.004693, Tokens per Sec:     9395, Lr: 0.000035
2022-09-16 05:50:31,514 - INFO - joeynmt.training - Epoch   1, Step:   247200, Batch Loss:     2.710295, Batch Acc: 0.003857, Tokens per Sec:     9552, Lr: 0.000035
2022-09-16 05:50:43,435 - INFO - joeynmt.training - Epoch   1, Step:   247300, Batch Loss:     2.893143, Batch Acc: 0.003593, Tokens per Sec:     9712, Lr: 0.000035
2022-09-16 05:50:55,366 - INFO - joeynmt.training - Epoch   1, Step:   247400, Batch Loss:     2.560834, Batch Acc: 0.005150, Tokens per Sec:     9537, Lr: 0.000035
2022-09-16 05:51:07,317 - INFO - joeynmt.training - Epoch   1, Step:   247500, Batch Loss:     2.585662, Batch Acc: 0.003821, Tokens per Sec:     9570, Lr: 0.000035
2022-09-16 05:51:19,229 - INFO - joeynmt.training - Epoch   1, Step:   247600, Batch Loss:     2.733847, Batch Acc: 0.003992, Tokens per Sec:     9715, Lr: 0.000035
2022-09-16 05:51:31,420 - INFO - joeynmt.training - Epoch   1, Step:   247700, Batch Loss:     2.355537, Batch Acc: 0.004515, Tokens per Sec:     9429, Lr: 0.000035
2022-09-16 05:51:43,278 - INFO - joeynmt.training - Epoch   1, Step:   247800, Batch Loss:     2.514610, Batch Acc: 0.005020, Tokens per Sec:     9491, Lr: 0.000035
2022-09-16 05:51:55,135 - INFO - joeynmt.training - Epoch   1, Step:   247900, Batch Loss:     2.839708, Batch Acc: 0.004470, Tokens per Sec:     9510, Lr: 0.000035
2022-09-16 05:52:07,011 - INFO - joeynmt.training - Epoch   1, Step:   248000, Batch Loss:     2.722634, Batch Acc: 0.004397, Tokens per Sec:     9480, Lr: 0.000035
2022-09-16 05:52:07,011 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:00:48,881 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 06:00:48,882 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.26, loss:   2.50, ppl:  12.19, acc:   0.52, generation: 512.0897[sec], evaluation: 9.2662[sec]
2022-09-16 06:00:49,323 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/246000.ckpt
2022-09-16 06:00:49,351 - INFO - joeynmt.training - Example #0
2022-09-16 06:00:49,352 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 06:00:49,352 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 06:00:49,352 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'शामिल', 'किया', 'गया', 'है', '</s>']
2022-09-16 06:00:49,369 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 06:00:49,369 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 06:00:49,369 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को शामिल किया गया है
2022-09-16 06:00:49,369 - INFO - joeynmt.training - Example #1
2022-09-16 06:00:49,369 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 06:00:49,369 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 06:00:49,369 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 06:00:49,384 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 06:00:49,384 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 06:00:49,384 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 06:00:49,384 - INFO - joeynmt.training - Example #2
2022-09-16 06:00:49,384 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 06:00:49,384 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 06:00:49,385 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 06:00:49,399 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 06:00:49,399 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 06:00:49,399 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 06:00:49,400 - INFO - joeynmt.training - Example #3
2022-09-16 06:00:49,400 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 06:00:49,400 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 06:00:49,400 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 06:00:49,414 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 06:00:49,414 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 06:00:49,414 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 06:01:01,539 - INFO - joeynmt.training - Epoch   1, Step:   248100, Batch Loss:     2.471986, Batch Acc: 0.004309, Tokens per Sec:     9099, Lr: 0.000035
2022-09-16 06:01:14,108 - INFO - joeynmt.training - Epoch   1, Step:   248200, Batch Loss:     2.610709, Batch Acc: 0.003519, Tokens per Sec:     9067, Lr: 0.000035
2022-09-16 06:01:26,308 - INFO - joeynmt.training - Epoch   1, Step:   248300, Batch Loss:     2.542355, Batch Acc: 0.005125, Tokens per Sec:     9292, Lr: 0.000035
2022-09-16 06:01:38,477 - INFO - joeynmt.training - Epoch   1, Step:   248400, Batch Loss:     2.821138, Batch Acc: 0.004851, Tokens per Sec:     9437, Lr: 0.000035
2022-09-16 06:01:50,418 - INFO - joeynmt.training - Epoch   1, Step:   248500, Batch Loss:     2.326585, Batch Acc: 0.005673, Tokens per Sec:     9713, Lr: 0.000035
2022-09-16 06:02:02,580 - INFO - joeynmt.training - Epoch   1, Step:   248600, Batch Loss:     2.432890, Batch Acc: 0.004746, Tokens per Sec:     9338, Lr: 0.000035
2022-09-16 06:02:14,866 - INFO - joeynmt.training - Epoch   1, Step:   248700, Batch Loss:     2.764837, Batch Acc: 0.004184, Tokens per Sec:     9453, Lr: 0.000035
2022-09-16 06:02:27,015 - INFO - joeynmt.training - Epoch   1, Step:   248800, Batch Loss:     2.742802, Batch Acc: 0.004434, Tokens per Sec:     9487, Lr: 0.000035
2022-09-16 06:02:39,176 - INFO - joeynmt.training - Epoch   1, Step:   248900, Batch Loss:     2.546351, Batch Acc: 0.004563, Tokens per Sec:     9515, Lr: 0.000035
2022-09-16 06:02:51,324 - INFO - joeynmt.training - Epoch   1, Step:   249000, Batch Loss:     2.509273, Batch Acc: 0.004249, Tokens per Sec:     9514, Lr: 0.000035
2022-09-16 06:02:51,324 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:11:33,661 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 06:11:33,663 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.35, loss:   2.50, ppl:  12.24, acc:   0.52, generation: 512.6433[sec], evaluation: 9.1864[sec]
2022-09-16 06:11:34,473 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/245000.ckpt
2022-09-16 06:11:34,502 - INFO - joeynmt.training - Example #0
2022-09-16 06:11:34,502 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 06:11:34,502 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 06:11:34,502 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-16 06:11:34,520 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 06:11:34,520 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 06:11:34,520 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 06:11:34,520 - INFO - joeynmt.training - Example #1
2022-09-16 06:11:34,520 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 06:11:34,520 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 06:11:34,520 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 06:11:34,535 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 06:11:34,535 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 06:11:34,535 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 06:11:34,535 - INFO - joeynmt.training - Example #2
2022-09-16 06:11:34,535 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 06:11:34,535 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 06:11:34,535 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 06:11:34,550 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 06:11:34,550 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 06:11:34,550 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 06:11:34,551 - INFO - joeynmt.training - Example #3
2022-09-16 06:11:34,551 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 06:11:34,551 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 06:11:34,551 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 06:11:34,566 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 06:11:34,566 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 06:11:34,566 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 06:11:46,530 - INFO - joeynmt.training - Epoch   1, Step:   249100, Batch Loss:     2.611003, Batch Acc: 0.004920, Tokens per Sec:     8960, Lr: 0.000035
2022-09-16 06:11:58,537 - INFO - joeynmt.training - Epoch   1, Step:   249200, Batch Loss:     2.486953, Batch Acc: 0.004463, Tokens per Sec:     9351, Lr: 0.000035
2022-09-16 06:12:10,462 - INFO - joeynmt.training - Epoch   1, Step:   249300, Batch Loss:     2.763711, Batch Acc: 0.004858, Tokens per Sec:     9529, Lr: 0.000035
2022-09-16 06:12:22,357 - INFO - joeynmt.training - Epoch   1, Step:   249400, Batch Loss:     2.500463, Batch Acc: 0.004484, Tokens per Sec:     9582, Lr: 0.000035
2022-09-16 06:12:34,251 - INFO - joeynmt.training - Epoch   1, Step:   249500, Batch Loss:     2.788921, Batch Acc: 0.004630, Tokens per Sec:     9698, Lr: 0.000035
2022-09-16 06:12:46,212 - INFO - joeynmt.training - Epoch   1, Step:   249600, Batch Loss:     2.556208, Batch Acc: 0.004882, Tokens per Sec:     9504, Lr: 0.000035
2022-09-16 06:12:58,269 - INFO - joeynmt.training - Epoch   1, Step:   249700, Batch Loss:     2.671070, Batch Acc: 0.004504, Tokens per Sec:     9669, Lr: 0.000035
2022-09-16 06:13:10,207 - INFO - joeynmt.training - Epoch   1, Step:   249800, Batch Loss:     2.623341, Batch Acc: 0.003991, Tokens per Sec:     9802, Lr: 0.000035
2022-09-16 06:13:22,105 - INFO - joeynmt.training - Epoch   1, Step:   249900, Batch Loss:     2.849627, Batch Acc: 0.004225, Tokens per Sec:     9528, Lr: 0.000035
2022-09-16 06:13:33,972 - INFO - joeynmt.training - Epoch   1, Step:   250000, Batch Loss:     2.830075, Batch Acc: 0.003675, Tokens per Sec:     9584, Lr: 0.000035
2022-09-16 06:13:33,973 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:22:00,281 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 06:22:00,283 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.30, loss:   2.50, ppl:  12.19, acc:   0.52, generation: 496.5774[sec], evaluation: 9.2227[sec]
2022-09-16 06:22:00,727 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/249000.ckpt
2022-09-16 06:22:00,755 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/249000.ckpt
2022-09-16 06:22:00,755 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/249000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/249000.ckpt')
2022-09-16 06:22:00,756 - INFO - joeynmt.training - Example #0
2022-09-16 06:22:00,756 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 06:22:00,756 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 06:22:00,756 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'शामिल', 'किया', 'गया', 'है', '</s>']
2022-09-16 06:22:00,773 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 06:22:00,773 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 06:22:00,773 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को शामिल किया गया है
2022-09-16 06:22:00,773 - INFO - joeynmt.training - Example #1
2022-09-16 06:22:00,773 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 06:22:00,774 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 06:22:00,774 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'प्ले', 'करें', '</s>']
2022-09-16 06:22:00,789 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 06:22:00,789 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 06:22:00,789 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन प्ले करें
2022-09-16 06:22:00,789 - INFO - joeynmt.training - Example #2
2022-09-16 06:22:00,789 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 06:22:00,789 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 06:22:00,789 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 06:22:00,804 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 06:22:00,804 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 06:22:00,804 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 06:22:00,804 - INFO - joeynmt.training - Example #3
2022-09-16 06:22:00,804 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 06:22:00,804 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 06:22:00,804 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 06:22:00,819 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 06:22:00,819 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 06:22:00,819 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 06:22:12,770 - INFO - joeynmt.training - Epoch   1, Step:   250100, Batch Loss:     2.531248, Batch Acc: 0.004682, Tokens per Sec:     9069, Lr: 0.000035
2022-09-16 06:22:24,602 - INFO - joeynmt.training - Epoch   1, Step:   250200, Batch Loss:     2.651405, Batch Acc: 0.004400, Tokens per Sec:     9759, Lr: 0.000035
2022-09-16 06:22:36,495 - INFO - joeynmt.training - Epoch   1, Step:   250300, Batch Loss:     2.397990, Batch Acc: 0.004787, Tokens per Sec:     9574, Lr: 0.000035
2022-09-16 06:22:48,376 - INFO - joeynmt.training - Epoch   1, Step:   250400, Batch Loss:     2.434280, Batch Acc: 0.005657, Tokens per Sec:     9419, Lr: 0.000035
2022-09-16 06:23:00,197 - INFO - joeynmt.training - Epoch   1, Step:   250500, Batch Loss:     2.539444, Batch Acc: 0.005332, Tokens per Sec:     9631, Lr: 0.000035
2022-09-16 06:23:12,438 - INFO - joeynmt.training - Epoch   1, Step:   250600, Batch Loss:     2.690038, Batch Acc: 0.004708, Tokens per Sec:     9421, Lr: 0.000035
2022-09-16 06:23:24,289 - INFO - joeynmt.training - Epoch   1, Step:   250700, Batch Loss:     2.583759, Batch Acc: 0.004823, Tokens per Sec:     9589, Lr: 0.000035
2022-09-16 06:23:36,144 - INFO - joeynmt.training - Epoch   1, Step:   250800, Batch Loss:     2.525214, Batch Acc: 0.004932, Tokens per Sec:     9663, Lr: 0.000035
2022-09-16 06:23:47,998 - INFO - joeynmt.training - Epoch   1, Step:   250900, Batch Loss:     2.574503, Batch Acc: 0.004903, Tokens per Sec:     9790, Lr: 0.000035
2022-09-16 06:23:59,828 - INFO - joeynmt.training - Epoch   1, Step:   251000, Batch Loss:     2.619263, Batch Acc: 0.003982, Tokens per Sec:     9596, Lr: 0.000035
2022-09-16 06:23:59,828 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:32:20,344 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 06:32:20,345 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.32, loss:   2.50, ppl:  12.23, acc:   0.52, generation: 490.7648[sec], evaluation: 9.2470[sec]
2022-09-16 06:32:20,352 - INFO - joeynmt.training - Example #0
2022-09-16 06:32:20,353 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 06:32:20,353 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 06:32:20,353 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'शामिल', 'किया', 'गया', 'है', '</s>']
2022-09-16 06:32:20,370 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 06:32:20,371 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 06:32:20,371 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को शामिल किया गया है
2022-09-16 06:32:20,371 - INFO - joeynmt.training - Example #1
2022-09-16 06:32:20,371 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 06:32:20,371 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 06:32:20,371 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'प्ले', 'करें', '</s>']
2022-09-16 06:32:20,386 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 06:32:20,386 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 06:32:20,386 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन प्ले करें
2022-09-16 06:32:20,386 - INFO - joeynmt.training - Example #2
2022-09-16 06:32:20,386 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 06:32:20,386 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 06:32:20,386 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 06:32:20,401 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 06:32:20,401 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 06:32:20,401 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 06:32:20,401 - INFO - joeynmt.training - Example #3
2022-09-16 06:32:20,402 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 06:32:20,402 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 06:32:20,402 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 06:32:20,417 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 06:32:20,417 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 06:32:20,417 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 06:32:32,332 - INFO - joeynmt.training - Epoch   1, Step:   251100, Batch Loss:     2.490758, Batch Acc: 0.004813, Tokens per Sec:     9469, Lr: 0.000035
2022-09-16 06:32:44,243 - INFO - joeynmt.training - Epoch   1, Step:   251200, Batch Loss:     2.912513, Batch Acc: 0.003704, Tokens per Sec:     9611, Lr: 0.000035
2022-09-16 06:32:56,076 - INFO - joeynmt.training - Epoch   1, Step:   251300, Batch Loss:     2.518379, Batch Acc: 0.004758, Tokens per Sec:     9574, Lr: 0.000035
2022-09-16 06:33:07,922 - INFO - joeynmt.training - Epoch   1, Step:   251400, Batch Loss:     2.427289, Batch Acc: 0.005154, Tokens per Sec:     9614, Lr: 0.000035
2022-09-16 06:33:19,771 - INFO - joeynmt.training - Epoch   1, Step:   251500, Batch Loss:     2.447939, Batch Acc: 0.003853, Tokens per Sec:     9682, Lr: 0.000035
2022-09-16 06:33:31,615 - INFO - joeynmt.training - Epoch   1, Step:   251600, Batch Loss:     2.729372, Batch Acc: 0.003918, Tokens per Sec:     9761, Lr: 0.000035
2022-09-16 06:33:43,468 - INFO - joeynmt.training - Epoch   1, Step:   251700, Batch Loss:     2.605040, Batch Acc: 0.004456, Tokens per Sec:     9675, Lr: 0.000035
2022-09-16 06:33:55,314 - INFO - joeynmt.training - Epoch   1, Step:   251800, Batch Loss:     2.676779, Batch Acc: 0.004553, Tokens per Sec:     9606, Lr: 0.000035
2022-09-16 06:34:07,129 - INFO - joeynmt.training - Epoch   1, Step:   251900, Batch Loss:     2.421276, Batch Acc: 0.004467, Tokens per Sec:     9814, Lr: 0.000035
2022-09-16 06:34:18,951 - INFO - joeynmt.training - Epoch   1, Step:   252000, Batch Loss:     2.931715, Batch Acc: 0.004212, Tokens per Sec:     9680, Lr: 0.000035
2022-09-16 06:34:18,951 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:42:58,345 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 06:42:58,346 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.34, loss:   2.49, ppl:  12.08, acc:   0.52, generation: 508.9960[sec], evaluation: 9.8917[sec]
2022-09-16 06:42:58,352 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 06:42:58,788 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/250000.ckpt
2022-09-16 06:42:58,816 - INFO - joeynmt.training - Example #0
2022-09-16 06:42:58,817 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 06:42:58,817 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 06:42:58,817 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'शामिल', 'किया', 'गया', 'है', '</s>']
2022-09-16 06:42:58,834 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 06:42:58,834 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 06:42:58,834 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को शामिल किया गया है
2022-09-16 06:42:58,834 - INFO - joeynmt.training - Example #1
2022-09-16 06:42:58,834 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 06:42:58,835 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 06:42:58,835 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 06:42:58,849 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 06:42:58,849 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 06:42:58,849 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 06:42:58,850 - INFO - joeynmt.training - Example #2
2022-09-16 06:42:58,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 06:42:58,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 06:42:58,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 06:42:58,865 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 06:42:58,865 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 06:42:58,865 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 06:42:58,865 - INFO - joeynmt.training - Example #3
2022-09-16 06:42:58,865 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 06:42:58,865 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 06:42:58,865 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 06:42:58,880 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 06:42:58,880 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 06:42:58,880 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 06:43:10,814 - INFO - joeynmt.training - Epoch   1, Step:   252100, Batch Loss:     2.630130, Batch Acc: 0.005146, Tokens per Sec:     9310, Lr: 0.000035
2022-09-16 06:43:22,643 - INFO - joeynmt.training - Epoch   1, Step:   252200, Batch Loss:     2.601206, Batch Acc: 0.005172, Tokens per Sec:     9578, Lr: 0.000035
2022-09-16 06:43:34,504 - INFO - joeynmt.training - Epoch   1, Step:   252300, Batch Loss:     2.657711, Batch Acc: 0.005006, Tokens per Sec:     9465, Lr: 0.000035
2022-09-16 06:43:46,366 - INFO - joeynmt.training - Epoch   1, Step:   252400, Batch Loss:     2.648149, Batch Acc: 0.003951, Tokens per Sec:     9752, Lr: 0.000035
2022-09-16 06:43:58,183 - INFO - joeynmt.training - Epoch   1, Step:   252500, Batch Loss:     2.398103, Batch Acc: 0.004698, Tokens per Sec:     9619, Lr: 0.000035
2022-09-16 06:44:10,100 - INFO - joeynmt.training - Epoch   1, Step:   252600, Batch Loss:     2.739766, Batch Acc: 0.005533, Tokens per Sec:     9706, Lr: 0.000035
2022-09-16 06:44:21,945 - INFO - joeynmt.training - Epoch   1, Step:   252700, Batch Loss:     2.545545, Batch Acc: 0.005161, Tokens per Sec:     9638, Lr: 0.000035
2022-09-16 06:44:33,759 - INFO - joeynmt.training - Epoch   1, Step:   252800, Batch Loss:     2.770055, Batch Acc: 0.004356, Tokens per Sec:     9677, Lr: 0.000035
2022-09-16 06:44:45,892 - INFO - joeynmt.training - Epoch   1, Step:   252900, Batch Loss:     2.568453, Batch Acc: 0.004188, Tokens per Sec:     9427, Lr: 0.000035
2022-09-16 06:44:58,088 - INFO - joeynmt.training - Epoch   1, Step:   253000, Batch Loss:     2.707327, Batch Acc: 0.004697, Tokens per Sec:     9322, Lr: 0.000035
2022-09-16 06:44:58,088 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 06:53:49,975 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 06:53:49,976 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.30, loss:   2.50, ppl:  12.14, acc:   0.52, generation: 521.9580[sec], evaluation: 9.4183[sec]
2022-09-16 06:53:50,426 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/248000.ckpt
2022-09-16 06:53:50,454 - INFO - joeynmt.training - Example #0
2022-09-16 06:53:50,454 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 06:53:50,454 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 06:53:50,454 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'शामिल', 'किया', 'गया', 'है', '</s>']
2022-09-16 06:53:50,472 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 06:53:50,472 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 06:53:50,472 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को शामिल किया गया है
2022-09-16 06:53:50,472 - INFO - joeynmt.training - Example #1
2022-09-16 06:53:50,472 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 06:53:50,472 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 06:53:50,472 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 06:53:50,487 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 06:53:50,487 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 06:53:50,487 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 06:53:50,487 - INFO - joeynmt.training - Example #2
2022-09-16 06:53:50,487 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 06:53:50,487 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 06:53:50,487 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 06:53:50,502 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 06:53:50,502 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 06:53:50,503 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 06:53:50,503 - INFO - joeynmt.training - Example #3
2022-09-16 06:53:50,503 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 06:53:50,503 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 06:53:50,503 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 06:53:50,518 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 06:53:50,518 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 06:53:50,518 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 06:54:02,677 - INFO - joeynmt.training - Epoch   1, Step:   253100, Batch Loss:     2.524065, Batch Acc: 0.004964, Tokens per Sec:     8854, Lr: 0.000035
2022-09-16 06:54:14,766 - INFO - joeynmt.training - Epoch   1, Step:   253200, Batch Loss:     2.584980, Batch Acc: 0.004889, Tokens per Sec:     9509, Lr: 0.000035
2022-09-16 06:54:26,891 - INFO - joeynmt.training - Epoch   1, Step:   253300, Batch Loss:     2.545939, Batch Acc: 0.005025, Tokens per Sec:     9389, Lr: 0.000035
2022-09-16 06:54:39,021 - INFO - joeynmt.training - Epoch   1, Step:   253400, Batch Loss:     2.601483, Batch Acc: 0.003386, Tokens per Sec:     9543, Lr: 0.000035
2022-09-16 06:54:51,110 - INFO - joeynmt.training - Epoch   1, Step:   253500, Batch Loss:     2.589185, Batch Acc: 0.003930, Tokens per Sec:     9409, Lr: 0.000035
2022-09-16 06:55:03,308 - INFO - joeynmt.training - Epoch   1, Step:   253600, Batch Loss:     2.387049, Batch Acc: 0.005027, Tokens per Sec:     9509, Lr: 0.000035
2022-09-16 06:55:15,404 - INFO - joeynmt.training - Epoch   1, Step:   253700, Batch Loss:     2.583243, Batch Acc: 0.004163, Tokens per Sec:     9453, Lr: 0.000035
2022-09-16 06:55:27,481 - INFO - joeynmt.training - Epoch   1, Step:   253800, Batch Loss:     2.689574, Batch Acc: 0.004132, Tokens per Sec:     9499, Lr: 0.000035
2022-09-16 06:55:39,534 - INFO - joeynmt.training - Epoch   1, Step:   253900, Batch Loss:     2.769043, Batch Acc: 0.003983, Tokens per Sec:     9291, Lr: 0.000035
2022-09-16 06:55:51,574 - INFO - joeynmt.training - Epoch   1, Step:   254000, Batch Loss:     2.923472, Batch Acc: 0.004421, Tokens per Sec:     9432, Lr: 0.000035
2022-09-16 06:55:51,574 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 07:04:45,664 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 07:04:45,665 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.40, loss:   2.50, ppl:  12.14, acc:   0.52, generation: 523.5747[sec], evaluation: 9.5283[sec]
2022-09-16 07:04:46,118 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/246459.ckpt
2022-09-16 07:04:46,146 - INFO - joeynmt.training - Example #0
2022-09-16 07:04:46,146 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 07:04:46,146 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 07:04:46,146 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'शामिल', 'किया', 'गया', 'है', '</s>']
2022-09-16 07:04:46,164 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 07:04:46,164 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 07:04:46,164 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को शामिल किया गया है
2022-09-16 07:04:46,164 - INFO - joeynmt.training - Example #1
2022-09-16 07:04:46,164 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 07:04:46,164 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 07:04:46,164 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 07:04:46,179 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 07:04:46,179 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 07:04:46,179 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 07:04:46,179 - INFO - joeynmt.training - Example #2
2022-09-16 07:04:46,179 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 07:04:46,179 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 07:04:46,180 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 07:04:46,194 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 07:04:46,195 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 07:04:46,195 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 07:04:46,195 - INFO - joeynmt.training - Example #3
2022-09-16 07:04:46,195 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 07:04:46,195 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 07:04:46,195 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 07:04:46,210 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 07:04:46,210 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 07:04:46,210 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 07:04:58,424 - INFO - joeynmt.training - Epoch   1, Step:   254100, Batch Loss:     2.474951, Batch Acc: 0.005549, Tokens per Sec:     9115, Lr: 0.000035
2022-09-16 07:05:10,562 - INFO - joeynmt.training - Epoch   1, Step:   254200, Batch Loss:     2.667870, Batch Acc: 0.004519, Tokens per Sec:     9444, Lr: 0.000035
2022-09-16 07:05:22,714 - INFO - joeynmt.training - Epoch   1, Step:   254300, Batch Loss:     2.764041, Batch Acc: 0.005073, Tokens per Sec:     9554, Lr: 0.000035
2022-09-16 07:05:34,826 - INFO - joeynmt.training - Epoch   1, Step:   254400, Batch Loss:     2.560003, Batch Acc: 0.005713, Tokens per Sec:     9395, Lr: 0.000035
2022-09-16 07:05:46,934 - INFO - joeynmt.training - Epoch   1, Step:   254500, Batch Loss:     2.724139, Batch Acc: 0.004545, Tokens per Sec:     9594, Lr: 0.000035
2022-09-16 07:05:59,027 - INFO - joeynmt.training - Epoch   1, Step:   254600, Batch Loss:     2.584122, Batch Acc: 0.004360, Tokens per Sec:     9426, Lr: 0.000035
2022-09-16 07:06:11,191 - INFO - joeynmt.training - Epoch   1, Step:   254700, Batch Loss:     2.569643, Batch Acc: 0.004704, Tokens per Sec:     9316, Lr: 0.000035
2022-09-16 07:06:23,296 - INFO - joeynmt.training - Epoch   1, Step:   254800, Batch Loss:     2.714110, Batch Acc: 0.005610, Tokens per Sec:     9336, Lr: 0.000035
2022-09-16 07:06:35,393 - INFO - joeynmt.training - Epoch   1, Step:   254900, Batch Loss:     2.616431, Batch Acc: 0.004861, Tokens per Sec:     9370, Lr: 0.000035
2022-09-16 07:06:47,502 - INFO - joeynmt.training - Epoch   1, Step:   255000, Batch Loss:     2.747959, Batch Acc: 0.004800, Tokens per Sec:     9446, Lr: 0.000035
2022-09-16 07:06:47,503 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 07:15:38,803 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 07:15:39,247 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/254000.ckpt
2022-09-16 07:15:39,274 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/254000.ckpt
2022-09-16 07:15:39,274 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/254000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/254000.ckpt')
2022-09-16 07:15:39,275 - INFO - joeynmt.training - Example #0
2022-09-16 07:15:39,275 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 07:15:39,275 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 07:15:39,275 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'शामिल', 'किया', 'गया', 'है', '</s>']
2022-09-16 07:15:39,292 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 07:15:39,292 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 07:15:39,293 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को शामिल किया गया है
2022-09-16 07:15:39,293 - INFO - joeynmt.training - Example #1
2022-09-16 07:15:39,293 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 07:15:39,293 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 07:15:39,293 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 07:15:39,308 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 07:15:39,308 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 07:15:39,308 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 07:15:39,308 - INFO - joeynmt.training - Example #2
2022-09-16 07:15:39,308 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 07:15:39,308 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 07:15:39,308 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 07:15:39,323 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 07:15:39,323 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 07:15:39,323 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 07:15:39,323 - INFO - joeynmt.training - Example #3
2022-09-16 07:15:39,323 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 07:15:39,323 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 07:15:39,323 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 07:15:39,338 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 07:15:39,338 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 07:15:39,338 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 07:15:51,252 - INFO - joeynmt.training - Epoch   1, Step:   255100, Batch Loss:     2.526867, Batch Acc: 0.004712, Tokens per Sec:     9125, Lr: 0.000035
2022-09-16 07:16:03,186 - INFO - joeynmt.training - Epoch   1, Step:   255200, Batch Loss:     2.646786, Batch Acc: 0.004487, Tokens per Sec:     9657, Lr: 0.000035
2022-09-16 07:16:15,097 - INFO - joeynmt.training - Epoch   1, Step:   255300, Batch Loss:     2.676845, Batch Acc: 0.004027, Tokens per Sec:     9695, Lr: 0.000035
2022-09-16 07:16:26,977 - INFO - joeynmt.training - Epoch   1, Step:   255400, Batch Loss:     2.845131, Batch Acc: 0.004555, Tokens per Sec:     9647, Lr: 0.000035
2022-09-16 07:16:38,831 - INFO - joeynmt.training - Epoch   1, Step:   255500, Batch Loss:     2.547430, Batch Acc: 0.004585, Tokens per Sec:     9771, Lr: 0.000035
2022-09-16 07:16:50,662 - INFO - joeynmt.training - Epoch   1, Step:   255600, Batch Loss:     2.631371, Batch Acc: 0.004118, Tokens per Sec:     9647, Lr: 0.000035
2022-09-16 07:17:02,532 - INFO - joeynmt.training - Epoch   1, Step:   255700, Batch Loss:     2.481309, Batch Acc: 0.004680, Tokens per Sec:     9685, Lr: 0.000035
2022-09-16 07:17:14,353 - INFO - joeynmt.training - Epoch   1, Step:   255800, Batch Loss:     2.782662, Batch Acc: 0.004847, Tokens per Sec:     9721, Lr: 0.000035
2022-09-16 07:17:26,199 - INFO - joeynmt.training - Epoch   1, Step:   255900, Batch Loss:     2.750755, Batch Acc: 0.004128, Tokens per Sec:     9774, Lr: 0.000035
2022-09-16 07:17:38,021 - INFO - joeynmt.training - Epoch   1, Step:   256000, Batch Loss:     2.459184, Batch Acc: 0.004298, Tokens per Sec:     9644, Lr: 0.000035
2022-09-16 07:17:38,022 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 07:26:05,082 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 07:26:05,084 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.38, loss:   2.49, ppl:  12.05, acc:   0.52, generation: 496.5707[sec], evaluation: 9.9874[sec]
2022-09-16 07:26:05,090 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 07:26:05,525 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/253000.ckpt
2022-09-16 07:26:05,553 - INFO - joeynmt.training - Example #0
2022-09-16 07:26:05,554 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 07:26:05,554 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 07:26:05,554 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 07:26:05,571 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 07:26:05,571 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 07:26:05,571 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 07:26:05,571 - INFO - joeynmt.training - Example #1
2022-09-16 07:26:05,571 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 07:26:05,571 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 07:26:05,571 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'खे@@', 'लें', '</s>']
2022-09-16 07:26:05,586 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 07:26:05,586 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 07:26:05,587 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 07:26:05,587 - INFO - joeynmt.training - Example #2
2022-09-16 07:26:05,587 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 07:26:05,587 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 07:26:05,587 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 07:26:05,602 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 07:26:05,602 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 07:26:05,602 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 07:26:05,602 - INFO - joeynmt.training - Example #3
2022-09-16 07:26:05,602 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 07:26:05,602 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 07:26:05,602 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 07:26:05,617 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 07:26:05,617 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 07:26:05,617 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 07:26:17,472 - INFO - joeynmt.training - Epoch   1, Step:   256100, Batch Loss:     2.487042, Batch Acc: 0.003852, Tokens per Sec:     9309, Lr: 0.000035
2022-09-16 07:26:29,258 - INFO - joeynmt.training - Epoch   1, Step:   256200, Batch Loss:     2.658832, Batch Acc: 0.004380, Tokens per Sec:     9725, Lr: 0.000035
2022-09-16 07:26:41,110 - INFO - joeynmt.training - Epoch   1, Step:   256300, Batch Loss:     2.504024, Batch Acc: 0.005440, Tokens per Sec:     9725, Lr: 0.000035
2022-09-16 07:26:52,882 - INFO - joeynmt.training - Epoch   1, Step:   256400, Batch Loss:     2.498432, Batch Acc: 0.004994, Tokens per Sec:     9781, Lr: 0.000035
2022-09-16 07:27:04,725 - INFO - joeynmt.training - Epoch   1, Step:   256500, Batch Loss:     2.508270, Batch Acc: 0.005038, Tokens per Sec:     9789, Lr: 0.000035
2022-09-16 07:27:16,511 - INFO - joeynmt.training - Epoch   1, Step:   256600, Batch Loss:     2.997645, Batch Acc: 0.004319, Tokens per Sec:     9685, Lr: 0.000035
2022-09-16 07:27:28,297 - INFO - joeynmt.training - Epoch   1, Step:   256700, Batch Loss:     2.510773, Batch Acc: 0.004276, Tokens per Sec:     9803, Lr: 0.000035
2022-09-16 07:27:40,086 - INFO - joeynmt.training - Epoch   1, Step:   256800, Batch Loss:     2.673424, Batch Acc: 0.004591, Tokens per Sec:     9682, Lr: 0.000035
2022-09-16 07:27:51,874 - INFO - joeynmt.training - Epoch   1, Step:   256900, Batch Loss:     2.748477, Batch Acc: 0.003472, Tokens per Sec:     9627, Lr: 0.000035
2022-09-16 07:28:03,652 - INFO - joeynmt.training - Epoch   1, Step:   257000, Batch Loss:     2.399065, Batch Acc: 0.004543, Tokens per Sec:     9775, Lr: 0.000035
2022-09-16 07:28:03,652 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 07:36:28,322 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 07:36:28,323 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.34, loss:   2.49, ppl:  12.06, acc:   0.52, generation: 494.8229[sec], evaluation: 9.3487[sec]
2022-09-16 07:36:28,767 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/255000.ckpt
2022-09-16 07:36:28,795 - INFO - joeynmt.training - Example #0
2022-09-16 07:36:28,795 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 07:36:28,795 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 07:36:28,795 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 07:36:28,812 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 07:36:28,812 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 07:36:28,813 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 07:36:28,813 - INFO - joeynmt.training - Example #1
2022-09-16 07:36:28,813 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 07:36:28,813 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 07:36:28,813 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'खे@@', 'लें', '</s>']
2022-09-16 07:36:28,828 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 07:36:28,828 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 07:36:28,828 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 07:36:28,828 - INFO - joeynmt.training - Example #2
2022-09-16 07:36:28,828 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 07:36:28,828 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 07:36:28,828 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 07:36:28,843 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 07:36:28,843 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 07:36:28,843 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 07:36:28,843 - INFO - joeynmt.training - Example #3
2022-09-16 07:36:28,843 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 07:36:28,843 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 07:36:28,844 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 07:36:28,858 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 07:36:28,858 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 07:36:28,858 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 07:36:40,735 - INFO - joeynmt.training - Epoch   1, Step:   257100, Batch Loss:     2.450673, Batch Acc: 0.005527, Tokens per Sec:     9217, Lr: 0.000035
2022-09-16 07:36:52,541 - INFO - joeynmt.training - Epoch   1, Step:   257200, Batch Loss:     2.710982, Batch Acc: 0.004395, Tokens per Sec:     9810, Lr: 0.000035
2022-09-16 07:37:04,437 - INFO - joeynmt.training - Epoch   1, Step:   257300, Batch Loss:     2.317168, Batch Acc: 0.004723, Tokens per Sec:     9470, Lr: 0.000035
2022-09-16 07:37:16,513 - INFO - joeynmt.training - Epoch   1, Step:   257400, Batch Loss:     2.814478, Batch Acc: 0.004073, Tokens per Sec:     9575, Lr: 0.000035
2022-09-16 07:37:28,543 - INFO - joeynmt.training - Epoch   1, Step:   257500, Batch Loss:     2.511230, Batch Acc: 0.005079, Tokens per Sec:     9509, Lr: 0.000035
2022-09-16 07:37:40,606 - INFO - joeynmt.training - Epoch   1, Step:   257600, Batch Loss:     2.805835, Batch Acc: 0.005087, Tokens per Sec:     9550, Lr: 0.000035
2022-09-16 07:37:52,678 - INFO - joeynmt.training - Epoch   1, Step:   257700, Batch Loss:     2.606152, Batch Acc: 0.004604, Tokens per Sec:     9537, Lr: 0.000035
2022-09-16 07:38:04,730 - INFO - joeynmt.training - Epoch   1, Step:   257800, Batch Loss:     2.895373, Batch Acc: 0.004224, Tokens per Sec:     9351, Lr: 0.000035
2022-09-16 07:38:16,658 - INFO - joeynmt.training - Epoch   1, Step:   257900, Batch Loss:     2.759892, Batch Acc: 0.003897, Tokens per Sec:     9596, Lr: 0.000035
2022-09-16 07:38:28,516 - INFO - joeynmt.training - Epoch   1, Step:   258000, Batch Loss:     2.569949, Batch Acc: 0.004312, Tokens per Sec:     9720, Lr: 0.000035
2022-09-16 07:38:28,516 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 07:47:13,593 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 07:47:13,594 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.33, loss:   2.48, ppl:  11.96, acc:   0.52, generation: 515.2502[sec], evaluation: 9.3187[sec]
2022-09-16 07:47:13,600 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 07:47:14,035 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/252000.ckpt
2022-09-16 07:47:14,063 - INFO - joeynmt.training - Example #0
2022-09-16 07:47:14,064 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 07:47:14,064 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 07:47:14,064 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 07:47:14,081 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 07:47:14,081 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 07:47:14,081 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 07:47:14,081 - INFO - joeynmt.training - Example #1
2022-09-16 07:47:14,081 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 07:47:14,081 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 07:47:14,081 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'खे@@', 'लें', '</s>']
2022-09-16 07:47:14,096 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 07:47:14,096 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 07:47:14,096 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 07:47:14,096 - INFO - joeynmt.training - Example #2
2022-09-16 07:47:14,096 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 07:47:14,096 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 07:47:14,096 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 07:47:14,111 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 07:47:14,111 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 07:47:14,111 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 07:47:14,111 - INFO - joeynmt.training - Example #3
2022-09-16 07:47:14,112 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 07:47:14,112 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 07:47:14,112 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 07:47:14,126 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 07:47:14,126 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 07:47:14,127 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 07:47:25,951 - INFO - joeynmt.training - Epoch   1, Step:   258100, Batch Loss:     2.714406, Batch Acc: 0.003745, Tokens per Sec:     9403, Lr: 0.000035
2022-09-16 07:47:37,716 - INFO - joeynmt.training - Epoch   1, Step:   258200, Batch Loss:     2.477106, Batch Acc: 0.004725, Tokens per Sec:     9769, Lr: 0.000035
2022-09-16 07:47:49,531 - INFO - joeynmt.training - Epoch   1, Step:   258300, Batch Loss:     2.684642, Batch Acc: 0.004082, Tokens per Sec:     9767, Lr: 0.000035
2022-09-16 07:48:01,347 - INFO - joeynmt.training - Epoch   1, Step:   258400, Batch Loss:     2.711725, Batch Acc: 0.003869, Tokens per Sec:     9822, Lr: 0.000035
2022-09-16 07:48:13,130 - INFO - joeynmt.training - Epoch   1, Step:   258500, Batch Loss:     2.469326, Batch Acc: 0.005496, Tokens per Sec:     9652, Lr: 0.000035
2022-09-16 07:48:24,907 - INFO - joeynmt.training - Epoch   1, Step:   258600, Batch Loss:     2.586532, Batch Acc: 0.004746, Tokens per Sec:     9769, Lr: 0.000035
2022-09-16 07:48:36,666 - INFO - joeynmt.training - Epoch   1, Step:   258700, Batch Loss:     2.705929, Batch Acc: 0.004457, Tokens per Sec:     9675, Lr: 0.000035
2022-09-16 07:48:48,458 - INFO - joeynmt.training - Epoch   1, Step:   258800, Batch Loss:     2.679708, Batch Acc: 0.004455, Tokens per Sec:     9727, Lr: 0.000035
2022-09-16 07:49:00,194 - INFO - joeynmt.training - Epoch   1, Step:   258900, Batch Loss:     2.652867, Batch Acc: 0.004169, Tokens per Sec:     9730, Lr: 0.000035
2022-09-16 07:49:12,007 - INFO - joeynmt.training - Epoch   1, Step:   259000, Batch Loss:     2.543641, Batch Acc: 0.005416, Tokens per Sec:     9660, Lr: 0.000035
2022-09-16 07:49:12,007 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 07:57:41,988 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 07:57:41,989 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.35, loss:   2.49, ppl:  12.02, acc:   0.52, generation: 500.2261[sec], evaluation: 9.2538[sec]
2022-09-16 07:57:42,929 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/257000.ckpt
2022-09-16 07:57:42,957 - INFO - joeynmt.training - Example #0
2022-09-16 07:57:42,957 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 07:57:42,957 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 07:57:42,957 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 07:57:42,974 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 07:57:42,974 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 07:57:42,974 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 07:57:42,974 - INFO - joeynmt.training - Example #1
2022-09-16 07:57:42,974 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 07:57:42,974 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 07:57:42,974 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'खे@@', 'लें', '</s>']
2022-09-16 07:57:42,989 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 07:57:42,989 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 07:57:42,989 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 07:57:42,989 - INFO - joeynmt.training - Example #2
2022-09-16 07:57:42,989 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 07:57:42,989 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 07:57:42,990 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 07:57:43,004 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 07:57:43,004 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 07:57:43,004 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 07:57:43,004 - INFO - joeynmt.training - Example #3
2022-09-16 07:57:43,004 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 07:57:43,005 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 07:57:43,005 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 07:57:43,019 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 07:57:43,019 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 07:57:43,019 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 07:57:54,880 - INFO - joeynmt.training - Epoch   1, Step:   259100, Batch Loss:     2.536241, Batch Acc: 0.005215, Tokens per Sec:     8959, Lr: 0.000035
2022-09-16 07:58:06,659 - INFO - joeynmt.training - Epoch   1, Step:   259200, Batch Loss:     2.297226, Batch Acc: 0.004458, Tokens per Sec:     9655, Lr: 0.000035
2022-09-16 07:58:18,417 - INFO - joeynmt.training - Epoch   1, Step:   259300, Batch Loss:     2.760186, Batch Acc: 0.004663, Tokens per Sec:     9759, Lr: 0.000035
2022-09-16 07:58:30,197 - INFO - joeynmt.training - Epoch   1, Step:   259400, Batch Loss:     2.505770, Batch Acc: 0.005846, Tokens per Sec:     9643, Lr: 0.000035
2022-09-16 07:58:41,979 - INFO - joeynmt.training - Epoch   1, Step:   259500, Batch Loss:     2.731440, Batch Acc: 0.003706, Tokens per Sec:     9643, Lr: 0.000035
2022-09-16 07:58:53,743 - INFO - joeynmt.training - Epoch   1, Step:   259600, Batch Loss:     2.636055, Batch Acc: 0.004544, Tokens per Sec:     9746, Lr: 0.000035
2022-09-16 07:59:05,515 - INFO - joeynmt.training - Epoch   1, Step:   259700, Batch Loss:     2.419923, Batch Acc: 0.004867, Tokens per Sec:     9651, Lr: 0.000035
2022-09-16 07:59:17,288 - INFO - joeynmt.training - Epoch   1, Step:   259800, Batch Loss:     2.709599, Batch Acc: 0.004595, Tokens per Sec:     9614, Lr: 0.000035
2022-09-16 07:59:29,050 - INFO - joeynmt.training - Epoch   1, Step:   259900, Batch Loss:     2.660631, Batch Acc: 0.004291, Tokens per Sec:     9689, Lr: 0.000035
2022-09-16 07:59:40,805 - INFO - joeynmt.training - Epoch   1, Step:   260000, Batch Loss:     2.580499, Batch Acc: 0.004262, Tokens per Sec:     9800, Lr: 0.000035
2022-09-16 07:59:40,805 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 08:08:03,081 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 08:08:03,082 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.44, loss:   2.48, ppl:  11.99, acc:   0.52, generation: 492.3568[sec], evaluation: 9.4156[sec]
2022-09-16 08:08:03,527 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/256000.ckpt
2022-09-16 08:08:03,555 - INFO - joeynmt.training - Example #0
2022-09-16 08:08:03,556 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 08:08:03,556 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 08:08:03,556 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 08:08:03,573 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 08:08:03,573 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 08:08:03,573 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 08:08:03,573 - INFO - joeynmt.training - Example #1
2022-09-16 08:08:03,573 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 08:08:03,573 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 08:08:03,573 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'खे@@', 'लें', '</s>']
2022-09-16 08:08:03,588 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 08:08:03,589 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 08:08:03,589 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 08:08:03,589 - INFO - joeynmt.training - Example #2
2022-09-16 08:08:03,589 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 08:08:03,589 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 08:08:03,589 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'माइ@@', 'म', '</s>']
2022-09-16 08:08:03,604 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 08:08:03,604 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 08:08:03,604 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और माइम
2022-09-16 08:08:03,604 - INFO - joeynmt.training - Example #3
2022-09-16 08:08:03,604 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 08:08:03,604 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 08:08:03,604 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 08:08:03,620 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 08:08:03,620 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 08:08:03,620 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 08:08:15,405 - INFO - joeynmt.training - Epoch   1, Step:   260100, Batch Loss:     2.516613, Batch Acc: 0.004930, Tokens per Sec:     9091, Lr: 0.000035
2022-09-16 08:08:27,161 - INFO - joeynmt.training - Epoch   1, Step:   260200, Batch Loss:     2.684179, Batch Acc: 0.005441, Tokens per Sec:     9584, Lr: 0.000035
2022-09-16 08:08:38,902 - INFO - joeynmt.training - Epoch   1, Step:   260300, Batch Loss:     2.675326, Batch Acc: 0.004254, Tokens per Sec:     9671, Lr: 0.000035
2022-09-16 08:08:50,689 - INFO - joeynmt.training - Epoch   1, Step:   260400, Batch Loss:     2.565600, Batch Acc: 0.005051, Tokens per Sec:     9658, Lr: 0.000035
2022-09-16 08:09:02,482 - INFO - joeynmt.training - Epoch   1, Step:   260500, Batch Loss:     2.602577, Batch Acc: 0.003998, Tokens per Sec:     9777, Lr: 0.000035
2022-09-16 08:09:14,278 - INFO - joeynmt.training - Epoch   1, Step:   260600, Batch Loss:     2.812848, Batch Acc: 0.004283, Tokens per Sec:     9799, Lr: 0.000035
2022-09-16 08:09:26,012 - INFO - joeynmt.training - Epoch   1, Step:   260700, Batch Loss:     2.625113, Batch Acc: 0.004414, Tokens per Sec:     9711, Lr: 0.000035
2022-09-16 08:09:37,815 - INFO - joeynmt.training - Epoch   1, Step:   260800, Batch Loss:     2.696604, Batch Acc: 0.005069, Tokens per Sec:     9778, Lr: 0.000035
2022-09-16 08:09:49,901 - INFO - joeynmt.training - Epoch   1, Step:   260900, Batch Loss:     2.884764, Batch Acc: 0.004239, Tokens per Sec:     9409, Lr: 0.000035
2022-09-16 08:10:02,047 - INFO - joeynmt.training - Epoch   1, Step:   261000, Batch Loss:     2.773542, Batch Acc: 0.004276, Tokens per Sec:     9435, Lr: 0.000035
2022-09-16 08:10:02,048 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 08:18:57,377 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 08:18:57,379 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.48, loss:   2.48, ppl:  11.92, acc:   0.52, generation: 525.2712[sec], evaluation: 9.5418[sec]
2022-09-16 08:18:57,385 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 08:18:57,832 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/259000.ckpt
2022-09-16 08:18:57,860 - INFO - joeynmt.training - Example #0
2022-09-16 08:18:57,860 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 08:18:57,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 08:18:57,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 08:18:57,877 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 08:18:57,877 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 08:18:57,877 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 08:18:57,878 - INFO - joeynmt.training - Example #1
2022-09-16 08:18:57,878 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 08:18:57,878 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 08:18:57,878 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 08:18:57,893 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 08:18:57,893 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 08:18:57,893 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 08:18:57,894 - INFO - joeynmt.training - Example #2
2022-09-16 08:18:57,894 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 08:18:57,894 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 08:18:57,894 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 08:18:57,909 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 08:18:57,909 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 08:18:57,909 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 08:18:57,909 - INFO - joeynmt.training - Example #3
2022-09-16 08:18:57,909 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 08:18:57,909 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 08:18:57,909 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 08:18:57,924 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 08:18:57,924 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 08:18:57,925 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 08:19:05,807 - INFO - joeynmt.training - Epoch   1: total training loss 38234.80
2022-09-16 08:19:05,807 - INFO - joeynmt.training - Training ended after   1 epochs.
2022-09-16 08:19:05,808 - INFO - joeynmt.training - Best validation result (greedy) at step   261000:  11.92 ppl.
2022-09-16 08:19:05,808 - INFO - joeynmt.training - Loading from ckpt file: /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt
2022-09-16 08:19:05,824 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 08:19:05,825 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 08:19:06,161 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 08:19:06,165 - INFO - joeynmt.model - Total params: 19302144
2022-09-16 08:19:06,166 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2022-09-16 08:19:06,439 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 08:19:06,505 - INFO - joeynmt.prediction - Decoding on dev set...
2022-09-16 08:19:06,506 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 08:44:57,910 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 08:44:57,911 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  23.85, generation: 1541.4489[sec], evaluation: 9.4570[sec]
2022-09-16 08:44:57,971 - INFO - joeynmt.prediction - Translations saved to: /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/00261000.hyps.dev.
2022-09-16 08:44:57,971 - INFO - joeynmt.prediction - Decoding on test set...
2022-09-16 08:44:57,971 - INFO - joeynmt.prediction - Predicting 40858 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:09:09,628 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 09:09:09,629 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  23.09, generation: 1441.2226[sec], evaluation: 9.9338[sec]
2022-09-16 09:09:09,698 - INFO - joeynmt.prediction - Translations saved to: /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/00261000.hyps.test.
2022-09-16 09:09:09,710 - INFO - joeynmt.training - ACTIVE LEARNING MODEL END - LEAST CONFIDENCE 1
2022-09-16 09:09:09,711 - INFO - joeynmt.training - ACTIVE LEARNING MODEL START - LEAST CONFIDENCE 2
2022-09-16 09:09:09,712 - INFO - joeynmt.training - Random Indices picked: [121958 146867 131932 365838 259178 119879 110268 207892  54886 137337] length: 26153
2022-09-16 09:09:09,713 - INFO - joeynmt.training - Processing Predictions on Batch 0/103
2022-09-16 09:09:13,940 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:09:13,940 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:09:14,280 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:09:14,551 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:09:14,679 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:09:14,679 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:09:14,682 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:09:15,312 - INFO - joeynmt.prediction - Generation took 0.6286[sec]. (No references given)
2022-09-16 09:09:15,325 - INFO - joeynmt.training - Processing Predictions on Batch 1/103
2022-09-16 09:09:19,588 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:09:19,589 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:09:19,923 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:09:20,687 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:09:20,815 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:09:20,815 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:09:20,816 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:09:21,364 - INFO - joeynmt.prediction - Generation took 0.5457[sec]. (No references given)
2022-09-16 09:09:21,374 - INFO - joeynmt.training - Processing Predictions on Batch 2/103
2022-09-16 09:09:25,686 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:09:25,687 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:09:26,022 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:09:26,273 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:09:26,401 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:09:26,402 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:09:26,402 - INFO - joeynmt.prediction - Predicting 2 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:09:26,640 - INFO - joeynmt.prediction - Generation took 0.2366[sec]. (No references given)
2022-09-16 09:09:26,650 - INFO - joeynmt.training - Processing Predictions on Batch 3/103
2022-09-16 09:09:30,958 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:09:30,958 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:09:31,294 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:09:31,547 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:09:31,674 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:09:31,675 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:09:31,676 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:09:32,318 - INFO - joeynmt.prediction - Generation took 0.6394[sec]. (No references given)
2022-09-16 09:09:32,328 - INFO - joeynmt.training - Processing Predictions on Batch 4/103
2022-09-16 09:09:36,644 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:09:36,644 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:09:36,978 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:09:37,233 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:09:37,359 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:09:37,359 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:09:37,362 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:09:37,985 - INFO - joeynmt.prediction - Generation took 0.6209[sec]. (No references given)
2022-09-16 09:09:37,995 - INFO - joeynmt.training - Processing Predictions on Batch 5/103
2022-09-16 09:09:42,312 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:09:42,312 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:09:42,646 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:09:42,899 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:09:43,026 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:09:43,026 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:09:43,029 - INFO - joeynmt.prediction - Predicting 12 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:09:43,632 - INFO - joeynmt.prediction - Generation took 0.6006[sec]. (No references given)
2022-09-16 09:09:43,642 - INFO - joeynmt.training - Processing Predictions on Batch 6/103
2022-09-16 09:09:48,024 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:09:48,024 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:09:48,359 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:09:48,610 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:09:48,737 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:09:48,737 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:09:48,739 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:09:49,302 - INFO - joeynmt.prediction - Generation took 0.5618[sec]. (No references given)
2022-09-16 09:09:49,313 - INFO - joeynmt.training - Processing Predictions on Batch 7/103
2022-09-16 09:09:53,712 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:09:53,712 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:09:54,047 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:09:54,300 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:09:54,425 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:09:54,425 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:09:54,427 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:09:55,133 - INFO - joeynmt.prediction - Generation took 0.7039[sec]. (No references given)
2022-09-16 09:09:55,143 - INFO - joeynmt.training - Processing Predictions on Batch 8/103
2022-09-16 09:09:59,553 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:09:59,553 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:09:59,887 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:10:00,142 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:10:00,270 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:00,270 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:00,274 - INFO - joeynmt.prediction - Predicting 11 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:10:01,473 - INFO - joeynmt.prediction - Generation took 1.1973[sec]. (No references given)
2022-09-16 09:10:01,484 - INFO - joeynmt.training - Processing Predictions on Batch 9/103
2022-09-16 09:10:05,922 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:10:05,923 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:10:06,256 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:10:06,510 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:10:06,638 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:06,638 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:06,640 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:10:07,198 - INFO - joeynmt.prediction - Generation took 0.5562[sec]. (No references given)
2022-09-16 09:10:07,209 - INFO - joeynmt.training - Processing Predictions on Batch 10/103
2022-09-16 09:10:11,666 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:10:11,666 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:10:12,002 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:10:12,254 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:10:12,381 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:12,383 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:12,386 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:10:13,920 - INFO - joeynmt.prediction - Generation took 1.5318[sec]. (No references given)
2022-09-16 09:10:13,930 - INFO - joeynmt.training - Processing Predictions on Batch 11/103
2022-09-16 09:10:18,393 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:10:18,393 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:10:18,727 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:10:18,981 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:10:19,107 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:19,107 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:19,108 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:10:19,739 - INFO - joeynmt.prediction - Generation took 0.6291[sec]. (No references given)
2022-09-16 09:10:19,749 - INFO - joeynmt.training - Processing Predictions on Batch 12/103
2022-09-16 09:10:24,229 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:10:24,229 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:10:24,564 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:10:24,817 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:10:24,943 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:24,944 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:24,945 - INFO - joeynmt.prediction - Predicting 2 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:10:25,323 - INFO - joeynmt.prediction - Generation took 0.3768[sec]. (No references given)
2022-09-16 09:10:25,333 - INFO - joeynmt.training - Processing Predictions on Batch 13/103
2022-09-16 09:10:29,809 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:10:29,810 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:10:30,147 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:10:30,401 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:10:30,528 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:30,528 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:30,529 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:10:31,250 - INFO - joeynmt.prediction - Generation took 0.7186[sec]. (No references given)
2022-09-16 09:10:31,261 - INFO - joeynmt.training - Processing Predictions on Batch 14/103
2022-09-16 09:10:35,739 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:10:35,739 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:10:36,075 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:10:36,326 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:10:36,454 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:36,454 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:36,455 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:10:37,084 - INFO - joeynmt.prediction - Generation took 0.6274[sec]. (No references given)
2022-09-16 09:10:37,094 - INFO - joeynmt.training - Processing Predictions on Batch 15/103
2022-09-16 09:10:41,554 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:10:41,554 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:10:41,890 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:10:42,144 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:10:42,271 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:42,272 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:42,274 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:10:42,798 - INFO - joeynmt.prediction - Generation took 0.5223[sec]. (No references given)
2022-09-16 09:10:42,810 - INFO - joeynmt.training - Processing Predictions on Batch 16/103
2022-09-16 09:10:47,270 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:10:47,271 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:10:47,604 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:10:47,857 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:10:47,983 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:47,984 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:47,985 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:10:48,433 - INFO - joeynmt.prediction - Generation took 0.4456[sec]. (No references given)
2022-09-16 09:10:48,443 - INFO - joeynmt.training - Processing Predictions on Batch 17/103
2022-09-16 09:10:52,891 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:10:52,892 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:10:53,226 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:10:53,479 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:10:53,607 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:53,607 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:53,609 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:10:54,263 - INFO - joeynmt.prediction - Generation took 0.6513[sec]. (No references given)
2022-09-16 09:10:54,273 - INFO - joeynmt.training - Processing Predictions on Batch 18/103
2022-09-16 09:10:58,724 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:10:58,725 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:10:59,060 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:10:59,311 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:10:59,439 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:59,439 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:10:59,441 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:11:00,069 - INFO - joeynmt.prediction - Generation took 0.6259[sec]. (No references given)
2022-09-16 09:11:00,079 - INFO - joeynmt.training - Processing Predictions on Batch 19/103
2022-09-16 09:11:04,523 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:11:04,523 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:11:04,857 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:11:05,110 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:11:05,236 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:05,237 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:05,238 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:11:06,112 - INFO - joeynmt.prediction - Generation took 0.8717[sec]. (No references given)
2022-09-16 09:11:06,122 - INFO - joeynmt.training - Processing Predictions on Batch 20/103
2022-09-16 09:11:10,556 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:11:10,556 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:11:10,891 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:11:11,145 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:11:11,272 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:11,272 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:11,274 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:11:11,764 - INFO - joeynmt.prediction - Generation took 0.4878[sec]. (No references given)
2022-09-16 09:11:11,775 - INFO - joeynmt.training - Processing Predictions on Batch 21/103
2022-09-16 09:11:16,204 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:11:16,204 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:11:16,539 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:11:16,791 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:11:16,919 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:16,919 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:16,924 - INFO - joeynmt.prediction - Predicting 15 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:11:18,400 - INFO - joeynmt.prediction - Generation took 1.4738[sec]. (No references given)
2022-09-16 09:11:18,411 - INFO - joeynmt.training - Processing Predictions on Batch 22/103
2022-09-16 09:11:22,830 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:11:22,831 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:11:23,166 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:11:23,417 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:11:23,544 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:23,544 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:23,546 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:11:23,924 - INFO - joeynmt.prediction - Generation took 0.3760[sec]. (No references given)
2022-09-16 09:11:23,933 - INFO - joeynmt.training - Processing Predictions on Batch 23/103
2022-09-16 09:11:28,351 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:11:28,351 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:11:28,685 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:11:28,938 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:11:29,064 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:29,064 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:29,066 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:11:30,424 - INFO - joeynmt.prediction - Generation took 1.3563[sec]. (No references given)
2022-09-16 09:11:30,434 - INFO - joeynmt.training - Processing Predictions on Batch 24/103
2022-09-16 09:11:34,845 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:11:34,846 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:11:35,180 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:11:35,433 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:11:35,560 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:35,561 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:35,563 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:11:36,324 - INFO - joeynmt.prediction - Generation took 0.7592[sec]. (No references given)
2022-09-16 09:11:36,335 - INFO - joeynmt.training - Processing Predictions on Batch 25/103
2022-09-16 09:11:40,740 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:11:40,741 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:11:41,076 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:11:41,328 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:11:41,457 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:41,457 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:41,458 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:11:42,022 - INFO - joeynmt.prediction - Generation took 0.5619[sec]. (No references given)
2022-09-16 09:11:42,032 - INFO - joeynmt.training - Processing Predictions on Batch 26/103
2022-09-16 09:11:46,454 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:11:46,456 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:11:46,790 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:11:47,042 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:11:47,170 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:47,170 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:47,172 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:11:47,739 - INFO - joeynmt.prediction - Generation took 0.5659[sec]. (No references given)
2022-09-16 09:11:47,750 - INFO - joeynmt.training - Processing Predictions on Batch 27/103
2022-09-16 09:11:52,147 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:11:52,147 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:11:52,482 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:11:52,736 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:11:52,862 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:52,862 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:52,865 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:11:53,706 - INFO - joeynmt.prediction - Generation took 0.8394[sec]. (No references given)
2022-09-16 09:11:53,717 - INFO - joeynmt.training - Processing Predictions on Batch 28/103
2022-09-16 09:11:58,127 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:11:58,127 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:11:58,463 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:11:58,717 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:11:58,845 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:58,845 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:11:58,848 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:12:00,354 - INFO - joeynmt.prediction - Generation took 1.5041[sec]. (No references given)
2022-09-16 09:12:00,365 - INFO - joeynmt.training - Processing Predictions on Batch 29/103
2022-09-16 09:12:04,766 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:12:04,766 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:12:05,101 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:12:05,352 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:12:05,479 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:05,479 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:05,480 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:12:05,891 - INFO - joeynmt.prediction - Generation took 0.4096[sec]. (No references given)
2022-09-16 09:12:05,902 - INFO - joeynmt.training - Processing Predictions on Batch 30/103
2022-09-16 09:12:10,313 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:12:10,313 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:12:10,648 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:12:10,900 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:12:11,027 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:11,028 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:11,029 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:12:11,519 - INFO - joeynmt.prediction - Generation took 0.4880[sec]. (No references given)
2022-09-16 09:12:11,529 - INFO - joeynmt.training - Processing Predictions on Batch 31/103
2022-09-16 09:12:15,934 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:12:15,934 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:12:16,268 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:12:16,521 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:12:16,649 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:16,649 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:16,651 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:12:17,272 - INFO - joeynmt.prediction - Generation took 0.6190[sec]. (No references given)
2022-09-16 09:12:17,282 - INFO - joeynmt.training - Processing Predictions on Batch 32/103
2022-09-16 09:12:21,690 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:12:21,690 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:12:22,024 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:12:22,278 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:12:22,404 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:22,404 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:22,408 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:12:23,621 - INFO - joeynmt.prediction - Generation took 1.2101[sec]. (No references given)
2022-09-16 09:12:23,631 - INFO - joeynmt.training - Processing Predictions on Batch 33/103
2022-09-16 09:12:28,068 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:12:28,069 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:12:28,405 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:12:29,159 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:12:29,286 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:29,286 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:29,289 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:12:29,928 - INFO - joeynmt.prediction - Generation took 0.6366[sec]. (No references given)
2022-09-16 09:12:29,938 - INFO - joeynmt.training - Processing Predictions on Batch 34/103
2022-09-16 09:12:34,354 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:12:34,355 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:12:34,689 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:12:34,942 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:12:35,068 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:35,068 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:35,071 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:12:35,740 - INFO - joeynmt.prediction - Generation took 0.6662[sec]. (No references given)
2022-09-16 09:12:35,750 - INFO - joeynmt.training - Processing Predictions on Batch 35/103
2022-09-16 09:12:40,170 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:12:40,170 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:12:40,506 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:12:40,759 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:12:40,887 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:40,887 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:40,889 - INFO - joeynmt.prediction - Predicting 11 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:12:41,550 - INFO - joeynmt.prediction - Generation took 0.6590[sec]. (No references given)
2022-09-16 09:12:41,561 - INFO - joeynmt.training - Processing Predictions on Batch 36/103
2022-09-16 09:12:45,985 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:12:45,985 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:12:46,319 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:12:46,572 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:12:46,699 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:46,699 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:46,701 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:12:47,684 - INFO - joeynmt.prediction - Generation took 0.9817[sec]. (No references given)
2022-09-16 09:12:47,695 - INFO - joeynmt.training - Processing Predictions on Batch 37/103
2022-09-16 09:12:52,119 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:12:52,119 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:12:52,453 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:12:52,706 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:12:52,832 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:52,833 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:52,834 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:12:53,284 - INFO - joeynmt.prediction - Generation took 0.4490[sec]. (No references given)
2022-09-16 09:12:53,295 - INFO - joeynmt.training - Processing Predictions on Batch 38/103
2022-09-16 09:12:57,719 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:12:57,719 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:12:58,053 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:12:58,306 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:12:58,433 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:58,433 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:12:58,437 - INFO - joeynmt.prediction - Predicting 14 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:12:59,063 - INFO - joeynmt.prediction - Generation took 0.6242[sec]. (No references given)
2022-09-16 09:12:59,074 - INFO - joeynmt.training - Processing Predictions on Batch 39/103
2022-09-16 09:13:03,517 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:13:03,517 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:13:03,852 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:13:04,103 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:13:04,231 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:04,231 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:04,234 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:13:04,917 - INFO - joeynmt.prediction - Generation took 0.6807[sec]. (No references given)
2022-09-16 09:13:04,928 - INFO - joeynmt.training - Processing Predictions on Batch 40/103
2022-09-16 09:13:09,369 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:13:09,369 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:13:09,704 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:13:09,958 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:13:10,084 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:10,084 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:10,087 - INFO - joeynmt.prediction - Predicting 11 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:13:11,655 - INFO - joeynmt.prediction - Generation took 1.5652[sec]. (No references given)
2022-09-16 09:13:11,665 - INFO - joeynmt.training - Processing Predictions on Batch 41/103
2022-09-16 09:13:16,089 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:13:16,089 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:13:16,424 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:13:16,677 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:13:16,807 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:16,807 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:16,809 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:13:17,200 - INFO - joeynmt.prediction - Generation took 0.3887[sec]. (No references given)
2022-09-16 09:13:17,211 - INFO - joeynmt.training - Processing Predictions on Batch 42/103
2022-09-16 09:13:21,636 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:13:21,637 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:13:21,971 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:13:22,224 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:13:22,352 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:22,352 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:22,353 - INFO - joeynmt.prediction - Predicting 3 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:13:22,502 - INFO - joeynmt.prediction - Generation took 0.1476[sec]. (No references given)
2022-09-16 09:13:22,511 - INFO - joeynmt.training - Processing Predictions on Batch 43/103
2022-09-16 09:13:26,966 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:13:26,966 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:13:27,302 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:13:27,556 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:13:27,684 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:27,685 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:27,687 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:13:28,413 - INFO - joeynmt.prediction - Generation took 0.7247[sec]. (No references given)
2022-09-16 09:13:28,424 - INFO - joeynmt.training - Processing Predictions on Batch 44/103
2022-09-16 09:13:32,857 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:13:32,857 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:13:33,194 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:13:33,451 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:13:33,580 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:33,580 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:33,581 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:13:34,220 - INFO - joeynmt.prediction - Generation took 0.6366[sec]. (No references given)
2022-09-16 09:13:34,231 - INFO - joeynmt.training - Processing Predictions on Batch 45/103
2022-09-16 09:13:38,663 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:13:38,663 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:13:38,997 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:13:39,250 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:13:39,376 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:39,376 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:39,379 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:13:39,875 - INFO - joeynmt.prediction - Generation took 0.4935[sec]. (No references given)
2022-09-16 09:13:39,885 - INFO - joeynmt.training - Processing Predictions on Batch 46/103
2022-09-16 09:13:44,326 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:13:44,326 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:13:44,660 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:13:44,913 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:13:45,040 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:45,040 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:45,042 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:13:46,557 - INFO - joeynmt.prediction - Generation took 1.5133[sec]. (No references given)
2022-09-16 09:13:46,568 - INFO - joeynmt.training - Processing Predictions on Batch 47/103
2022-09-16 09:13:50,989 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:13:50,990 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:13:51,326 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:13:51,578 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:13:51,705 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:51,705 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:51,707 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:13:52,173 - INFO - joeynmt.prediction - Generation took 0.4649[sec]. (No references given)
2022-09-16 09:13:52,184 - INFO - joeynmt.training - Processing Predictions on Batch 48/103
2022-09-16 09:13:56,595 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:13:56,595 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:13:56,929 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:13:57,183 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:13:57,308 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:57,308 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:13:57,311 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:13:57,936 - INFO - joeynmt.prediction - Generation took 0.6226[sec]. (No references given)
2022-09-16 09:13:57,946 - INFO - joeynmt.training - Processing Predictions on Batch 49/103
2022-09-16 09:14:02,362 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:14:02,362 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:14:02,696 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:14:02,949 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:14:03,075 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:03,075 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:03,077 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:14:03,552 - INFO - joeynmt.prediction - Generation took 0.4725[sec]. (No references given)
2022-09-16 09:14:03,562 - INFO - joeynmt.training - Processing Predictions on Batch 50/103
2022-09-16 09:14:07,989 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:14:07,989 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:14:08,323 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:14:08,576 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:14:08,703 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:08,703 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:08,706 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:14:09,280 - INFO - joeynmt.prediction - Generation took 0.5714[sec]. (No references given)
2022-09-16 09:14:09,290 - INFO - joeynmt.training - Processing Predictions on Batch 51/103
2022-09-16 09:14:13,697 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:14:13,697 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:14:14,033 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:14:14,285 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:14:14,412 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:14,412 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:14,416 - INFO - joeynmt.prediction - Predicting 11 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:14:15,017 - INFO - joeynmt.prediction - Generation took 0.5990[sec]. (No references given)
2022-09-16 09:14:15,027 - INFO - joeynmt.training - Processing Predictions on Batch 52/103
2022-09-16 09:14:19,447 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:14:19,447 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:14:19,782 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:14:20,036 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:14:20,161 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:20,162 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:20,164 - INFO - joeynmt.prediction - Predicting 11 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:14:21,369 - INFO - joeynmt.prediction - Generation took 1.2024[sec]. (No references given)
2022-09-16 09:14:21,380 - INFO - joeynmt.training - Processing Predictions on Batch 53/103
2022-09-16 09:14:25,781 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:14:25,781 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:14:26,116 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:14:26,370 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:14:26,498 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:26,498 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:26,499 - INFO - joeynmt.prediction - Predicting 2 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:14:26,694 - INFO - joeynmt.prediction - Generation took 0.1937[sec]. (No references given)
2022-09-16 09:14:26,704 - INFO - joeynmt.training - Processing Predictions on Batch 54/103
2022-09-16 09:14:31,130 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:14:31,131 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:14:31,466 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:14:31,718 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:14:31,844 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:31,844 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:31,848 - INFO - joeynmt.prediction - Predicting 12 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:14:33,455 - INFO - joeynmt.prediction - Generation took 1.6042[sec]. (No references given)
2022-09-16 09:14:33,465 - INFO - joeynmt.training - Processing Predictions on Batch 55/103
2022-09-16 09:14:37,875 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:14:37,875 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:14:38,210 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:14:38,463 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:14:38,589 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:38,589 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:38,590 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:14:39,281 - INFO - joeynmt.prediction - Generation took 0.6890[sec]. (No references given)
2022-09-16 09:14:39,291 - INFO - joeynmt.training - Processing Predictions on Batch 56/103
2022-09-16 09:14:43,704 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:14:43,705 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:14:44,039 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:14:44,293 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:14:44,417 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:44,417 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:44,419 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:14:44,964 - INFO - joeynmt.prediction - Generation took 0.5423[sec]. (No references given)
2022-09-16 09:14:44,974 - INFO - joeynmt.training - Processing Predictions on Batch 57/103
2022-09-16 09:14:49,395 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:14:49,395 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:14:49,730 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:14:49,983 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:14:50,109 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:50,109 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:50,111 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:14:51,084 - INFO - joeynmt.prediction - Generation took 0.9707[sec]. (No references given)
2022-09-16 09:14:51,095 - INFO - joeynmt.training - Processing Predictions on Batch 58/103
2022-09-16 09:14:55,517 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:14:55,517 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:14:55,853 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:14:56,104 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:14:56,229 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:56,229 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:14:56,232 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:14:57,711 - INFO - joeynmt.prediction - Generation took 1.4758[sec]. (No references given)
2022-09-16 09:14:57,721 - INFO - joeynmt.training - Processing Predictions on Batch 59/103
2022-09-16 09:15:02,136 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:15:02,136 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:15:02,473 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:15:02,727 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:15:02,852 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:02,852 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:02,852 - INFO - joeynmt.prediction - Predicting 1 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:15:03,296 - INFO - joeynmt.prediction - Generation took 0.4421[sec]. (No references given)
2022-09-16 09:15:03,306 - INFO - joeynmt.training - Processing Predictions on Batch 60/103
2022-09-16 09:15:07,731 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:15:07,731 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:15:08,067 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:15:08,321 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:15:08,445 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:08,446 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:08,449 - INFO - joeynmt.prediction - Predicting 11 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:15:09,190 - INFO - joeynmt.prediction - Generation took 0.7385[sec]. (No references given)
2022-09-16 09:15:09,200 - INFO - joeynmt.training - Processing Predictions on Batch 61/103
2022-09-16 09:15:13,621 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:15:13,621 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:15:13,955 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:15:14,208 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:15:14,332 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:14,333 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:14,336 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:15:14,995 - INFO - joeynmt.prediction - Generation took 0.6570[sec]. (No references given)
2022-09-16 09:15:15,006 - INFO - joeynmt.training - Processing Predictions on Batch 62/103
2022-09-16 09:15:19,434 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:15:19,434 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:15:19,769 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:15:20,020 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:15:20,145 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:20,145 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:20,147 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:15:20,753 - INFO - joeynmt.prediction - Generation took 0.6041[sec]. (No references given)
2022-09-16 09:15:20,763 - INFO - joeynmt.training - Processing Predictions on Batch 63/103
2022-09-16 09:15:25,199 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:15:25,199 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:15:25,533 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:15:26,290 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:15:26,415 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:26,415 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:26,417 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:15:27,124 - INFO - joeynmt.prediction - Generation took 0.7050[sec]. (No references given)
2022-09-16 09:15:27,134 - INFO - joeynmt.training - Processing Predictions on Batch 64/103
2022-09-16 09:15:31,563 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:15:31,563 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:15:31,897 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:15:32,150 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:15:32,276 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:32,276 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:32,277 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:15:32,744 - INFO - joeynmt.prediction - Generation took 0.4649[sec]. (No references given)
2022-09-16 09:15:32,754 - INFO - joeynmt.training - Processing Predictions on Batch 65/103
2022-09-16 09:15:37,190 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:15:37,190 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:15:37,526 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:15:37,776 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:15:37,901 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:37,901 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:37,904 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:15:39,465 - INFO - joeynmt.prediction - Generation took 1.5588[sec]. (No references given)
2022-09-16 09:15:39,476 - INFO - joeynmt.training - Processing Predictions on Batch 66/103
2022-09-16 09:15:43,883 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:15:43,883 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:15:44,218 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:15:44,468 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:15:44,594 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:44,594 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:44,596 - INFO - joeynmt.prediction - Predicting 11 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:15:45,317 - INFO - joeynmt.prediction - Generation took 0.7184[sec]. (No references given)
2022-09-16 09:15:45,326 - INFO - joeynmt.training - Processing Predictions on Batch 67/103
2022-09-16 09:15:49,731 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:15:49,731 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:15:50,067 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:15:50,321 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:15:50,446 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:50,446 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:50,449 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:15:51,967 - INFO - joeynmt.prediction - Generation took 1.5165[sec]. (No references given)
2022-09-16 09:15:51,977 - INFO - joeynmt.training - Processing Predictions on Batch 68/103
2022-09-16 09:15:56,384 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:15:56,385 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:15:56,719 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:15:56,972 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:15:57,096 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:57,097 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:15:57,099 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:15:57,676 - INFO - joeynmt.prediction - Generation took 0.5752[sec]. (No references given)
2022-09-16 09:15:57,686 - INFO - joeynmt.training - Processing Predictions on Batch 69/103
2022-09-16 09:16:02,088 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:16:02,088 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:16:02,424 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:16:02,675 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:16:02,800 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:02,801 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:02,803 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:16:04,378 - INFO - joeynmt.prediction - Generation took 1.5735[sec]. (No references given)
2022-09-16 09:16:04,388 - INFO - joeynmt.training - Processing Predictions on Batch 70/103
2022-09-16 09:16:08,804 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:16:08,805 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:16:09,140 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:16:09,392 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:16:09,515 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:09,515 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:09,519 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:16:10,170 - INFO - joeynmt.prediction - Generation took 0.6491[sec]. (No references given)
2022-09-16 09:16:10,180 - INFO - joeynmt.training - Processing Predictions on Batch 71/103
2022-09-16 09:16:14,575 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:16:14,575 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:16:14,909 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:16:15,167 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:16:15,290 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:15,291 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:15,294 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:16:16,032 - INFO - joeynmt.prediction - Generation took 0.7357[sec]. (No references given)
2022-09-16 09:16:16,042 - INFO - joeynmt.training - Processing Predictions on Batch 72/103
2022-09-16 09:16:20,433 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:16:20,434 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:16:20,769 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:16:21,023 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:16:21,147 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:21,148 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:21,150 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:16:21,805 - INFO - joeynmt.prediction - Generation took 0.6530[sec]. (No references given)
2022-09-16 09:16:21,815 - INFO - joeynmt.training - Processing Predictions on Batch 73/103
2022-09-16 09:16:26,206 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:16:26,207 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:16:26,543 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:16:26,794 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:16:26,918 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:26,918 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:26,920 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:16:27,574 - INFO - joeynmt.prediction - Generation took 0.6515[sec]. (No references given)
2022-09-16 09:16:27,583 - INFO - joeynmt.training - Processing Predictions on Batch 74/103
2022-09-16 09:16:31,978 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:16:31,978 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:16:32,312 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:16:32,565 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:16:32,689 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:32,689 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:32,692 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:16:33,164 - INFO - joeynmt.prediction - Generation took 0.4698[sec]. (No references given)
2022-09-16 09:16:33,173 - INFO - joeynmt.training - Processing Predictions on Batch 75/103
2022-09-16 09:16:37,567 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:16:37,568 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:16:37,902 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:16:38,154 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:16:38,278 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:38,278 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:38,279 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:16:38,937 - INFO - joeynmt.prediction - Generation took 0.6564[sec]. (No references given)
2022-09-16 09:16:38,947 - INFO - joeynmt.training - Processing Predictions on Batch 76/103
2022-09-16 09:16:43,369 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:16:43,369 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:16:43,704 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:16:43,956 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:16:44,080 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:44,080 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:44,082 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:16:44,640 - INFO - joeynmt.prediction - Generation took 0.5560[sec]. (No references given)
2022-09-16 09:16:44,650 - INFO - joeynmt.training - Processing Predictions on Batch 77/103
2022-09-16 09:16:49,054 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:16:49,055 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:16:49,390 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:16:49,641 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:16:49,765 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:49,766 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:49,768 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:16:50,364 - INFO - joeynmt.prediction - Generation took 0.5938[sec]. (No references given)
2022-09-16 09:16:50,374 - INFO - joeynmt.training - Processing Predictions on Batch 78/103
2022-09-16 09:16:54,779 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:16:54,779 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:16:55,114 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:16:55,365 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:16:55,488 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:55,488 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:16:55,490 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:16:55,996 - INFO - joeynmt.prediction - Generation took 0.5044[sec]. (No references given)
2022-09-16 09:16:56,006 - INFO - joeynmt.training - Processing Predictions on Batch 79/103
2022-09-16 09:17:00,408 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:17:00,408 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:17:00,742 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:17:00,997 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:17:01,121 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:01,121 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:01,122 - INFO - joeynmt.prediction - Predicting 2 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:17:01,614 - INFO - joeynmt.prediction - Generation took 0.4904[sec]. (No references given)
2022-09-16 09:17:01,624 - INFO - joeynmt.training - Processing Predictions on Batch 80/103
2022-09-16 09:17:06,033 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:17:06,033 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:17:06,367 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:17:06,622 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:17:06,745 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:06,746 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:06,748 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:17:07,434 - INFO - joeynmt.prediction - Generation took 0.6844[sec]. (No references given)
2022-09-16 09:17:07,444 - INFO - joeynmt.training - Processing Predictions on Batch 81/103
2022-09-16 09:17:11,849 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:17:11,849 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:17:12,185 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:17:12,437 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:17:12,562 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:12,562 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:12,565 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:17:14,170 - INFO - joeynmt.prediction - Generation took 1.6024[sec]. (No references given)
2022-09-16 09:17:14,180 - INFO - joeynmt.training - Processing Predictions on Batch 82/103
2022-09-16 09:17:18,594 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:17:18,595 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:17:18,930 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:17:19,183 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:17:19,306 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:19,306 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:19,308 - INFO - joeynmt.prediction - Predicting 9 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:17:19,864 - INFO - joeynmt.prediction - Generation took 0.5544[sec]. (No references given)
2022-09-16 09:17:19,874 - INFO - joeynmt.training - Processing Predictions on Batch 83/103
2022-09-16 09:17:24,290 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:17:24,290 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:17:24,624 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:17:24,878 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:17:25,001 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:25,001 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:25,003 - INFO - joeynmt.prediction - Predicting 12 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:17:25,392 - INFO - joeynmt.prediction - Generation took 0.3865[sec]. (No references given)
2022-09-16 09:17:25,402 - INFO - joeynmt.training - Processing Predictions on Batch 84/103
2022-09-16 09:17:29,832 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:17:29,832 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:17:30,166 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:17:30,418 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:17:30,543 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:30,544 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:30,545 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:17:31,036 - INFO - joeynmt.prediction - Generation took 0.4892[sec]. (No references given)
2022-09-16 09:17:31,046 - INFO - joeynmt.training - Processing Predictions on Batch 85/103
2022-09-16 09:17:35,490 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:17:35,490 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:17:35,825 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:17:36,076 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:17:36,200 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:36,200 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:36,203 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:17:36,913 - INFO - joeynmt.prediction - Generation took 0.7079[sec]. (No references given)
2022-09-16 09:17:36,923 - INFO - joeynmt.training - Processing Predictions on Batch 86/103
2022-09-16 09:17:41,359 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:17:41,359 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:17:41,695 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:17:41,948 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:17:42,071 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:42,071 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:42,074 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:17:42,660 - INFO - joeynmt.prediction - Generation took 0.5842[sec]. (No references given)
2022-09-16 09:17:42,671 - INFO - joeynmt.training - Processing Predictions on Batch 87/103
2022-09-16 09:17:47,105 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:17:47,105 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:17:47,440 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:17:47,694 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:17:47,817 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:47,817 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:47,819 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:17:49,318 - INFO - joeynmt.prediction - Generation took 1.4962[sec]. (No references given)
2022-09-16 09:17:49,328 - INFO - joeynmt.training - Processing Predictions on Batch 88/103
2022-09-16 09:17:53,751 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:17:53,752 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:17:54,086 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:17:54,340 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:17:54,463 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:54,463 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:17:54,464 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:17:54,887 - INFO - joeynmt.prediction - Generation took 0.4209[sec]. (No references given)
2022-09-16 09:17:54,896 - INFO - joeynmt.training - Processing Predictions on Batch 89/103
2022-09-16 09:17:59,322 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:17:59,322 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:17:59,657 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:17:59,909 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:18:00,033 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:00,033 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:00,034 - INFO - joeynmt.prediction - Predicting 5 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:18:00,410 - INFO - joeynmt.prediction - Generation took 0.3738[sec]. (No references given)
2022-09-16 09:18:00,419 - INFO - joeynmt.training - Processing Predictions on Batch 90/103
2022-09-16 09:18:04,847 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:18:04,847 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:18:05,181 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:18:05,434 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:18:05,557 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:05,557 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:05,559 - INFO - joeynmt.prediction - Predicting 3 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:18:05,911 - INFO - joeynmt.prediction - Generation took 0.3504[sec]. (No references given)
2022-09-16 09:18:05,921 - INFO - joeynmt.training - Processing Predictions on Batch 91/103
2022-09-16 09:18:10,351 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:18:10,352 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:18:10,686 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:18:10,940 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:18:11,064 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:11,064 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:11,066 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:18:12,267 - INFO - joeynmt.prediction - Generation took 1.1988[sec]. (No references given)
2022-09-16 09:18:12,277 - INFO - joeynmt.training - Processing Predictions on Batch 92/103
2022-09-16 09:18:16,693 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:18:16,693 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:18:17,028 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:18:17,282 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:18:17,406 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:17,407 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:17,408 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:18:18,219 - INFO - joeynmt.prediction - Generation took 0.8091[sec]. (No references given)
2022-09-16 09:18:18,229 - INFO - joeynmt.training - Processing Predictions on Batch 93/103
2022-09-16 09:18:22,647 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:18:22,647 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:18:22,982 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:18:23,233 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:18:23,357 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:23,357 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:23,359 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:18:24,249 - INFO - joeynmt.prediction - Generation took 0.8878[sec]. (No references given)
2022-09-16 09:18:24,259 - INFO - joeynmt.training - Processing Predictions on Batch 94/103
2022-09-16 09:18:28,676 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:18:28,676 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:18:29,011 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:18:29,771 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:18:29,894 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:29,895 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:29,898 - INFO - joeynmt.prediction - Predicting 11 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:18:30,665 - INFO - joeynmt.prediction - Generation took 0.7646[sec]. (No references given)
2022-09-16 09:18:30,675 - INFO - joeynmt.training - Processing Predictions on Batch 95/103
2022-09-16 09:18:35,081 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:18:35,083 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:18:35,417 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:18:35,670 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:18:35,794 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:35,794 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:35,796 - INFO - joeynmt.prediction - Predicting 7 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:18:37,312 - INFO - joeynmt.prediction - Generation took 1.5137[sec]. (No references given)
2022-09-16 09:18:37,322 - INFO - joeynmt.training - Processing Predictions on Batch 96/103
2022-09-16 09:18:41,726 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:18:41,727 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:18:42,064 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:18:42,314 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:18:42,439 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:42,439 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:42,440 - INFO - joeynmt.prediction - Predicting 4 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:18:42,932 - INFO - joeynmt.prediction - Generation took 0.4906[sec]. (No references given)
2022-09-16 09:18:42,943 - INFO - joeynmt.training - Processing Predictions on Batch 97/103
2022-09-16 09:18:47,336 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:18:47,336 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:18:47,670 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:18:47,923 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:18:48,047 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:48,047 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:48,049 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:18:49,605 - INFO - joeynmt.prediction - Generation took 1.5541[sec]. (No references given)
2022-09-16 09:18:49,615 - INFO - joeynmt.training - Processing Predictions on Batch 98/103
2022-09-16 09:18:54,003 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:18:54,003 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:18:54,337 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:18:54,591 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:18:54,714 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:54,714 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:18:54,716 - INFO - joeynmt.prediction - Predicting 6 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:18:55,189 - INFO - joeynmt.prediction - Generation took 0.4712[sec]. (No references given)
2022-09-16 09:18:55,198 - INFO - joeynmt.training - Processing Predictions on Batch 99/103
2022-09-16 09:18:59,588 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:18:59,588 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:18:59,922 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:19:00,178 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:19:00,303 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:19:00,303 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:19:00,304 - INFO - joeynmt.prediction - Predicting 2 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:19:00,531 - INFO - joeynmt.prediction - Generation took 0.2256[sec]. (No references given)
2022-09-16 09:19:00,540 - INFO - joeynmt.training - Processing Predictions on Batch 100/103
2022-09-16 09:19:04,931 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:19:04,931 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:19:05,266 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:19:05,518 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:19:05,644 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:19:05,644 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:19:05,647 - INFO - joeynmt.prediction - Predicting 10 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:19:06,392 - INFO - joeynmt.prediction - Generation took 0.7426[sec]. (No references given)
2022-09-16 09:19:06,402 - INFO - joeynmt.training - Processing Predictions on Batch 101/103
2022-09-16 09:19:10,778 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:19:10,779 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:19:11,116 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:19:11,371 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:19:11,494 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:19:11,494 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:19:11,495 - INFO - joeynmt.prediction - Predicting 8 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:19:12,959 - INFO - joeynmt.prediction - Generation took 1.4611[sec]. (No references given)
2022-09-16 09:19:12,969 - INFO - joeynmt.training - Processing Predictions on Batch 102/103
2022-09-16 09:19:17,353 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 09:19:17,353 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 09:19:17,691 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 09:19:17,945 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt.
2022-09-16 09:19:18,070 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=moses, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:19:18,070 - INFO - joeynmt.tokenizers - hi tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 60), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2022-09-16 09:19:18,071 - INFO - joeynmt.prediction - Predicting 2 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=5, min_output_length=1, max_output_length=130, return_prob='hyp', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:19:18,677 - INFO - joeynmt.prediction - Generation took 0.6043[sec]. (No references given)
2022-09-16 09:19:18,825 - INFO - joeynmt.training - Final Query Indices picked: [51024, 180703, 179428, 223853, 275512, 358266, 371930, 155378, 243942, 8816] length: 10000
2022-09-16 09:19:18,825 - INFO - joeynmt.training - Query the samples, one at a time (interactive/file)
2022-09-16 09:19:22,174 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 4096
	effective batch size (w. parallel & accumulation): 4096
2022-09-16 09:19:22,174 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:27:55,021 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 09:27:55,022 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.42, loss:   2.48, ppl:  11.93, acc:   0.52, generation: 502.6165[sec], evaluation: 9.6386[sec]
2022-09-16 09:27:55,465 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/260000.ckpt
2022-09-16 09:27:55,494 - INFO - joeynmt.training - Example #0
2022-09-16 09:27:55,494 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 09:27:55,494 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 09:27:55,494 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 09:27:55,512 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 09:27:55,512 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 09:27:55,512 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 09:27:55,512 - INFO - joeynmt.training - Example #1
2022-09-16 09:27:55,512 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 09:27:55,512 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 09:27:55,512 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'खे@@', 'लें', '</s>']
2022-09-16 09:27:55,527 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 09:27:55,527 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 09:27:55,527 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 09:27:55,527 - INFO - joeynmt.training - Example #2
2022-09-16 09:27:55,527 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 09:27:55,527 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 09:27:55,527 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 09:27:55,542 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 09:27:55,542 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 09:27:55,542 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 09:27:55,542 - INFO - joeynmt.training - Example #3
2022-09-16 09:27:55,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 09:27:55,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 09:27:55,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 09:27:55,557 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 09:27:55,557 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 09:27:55,557 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 09:27:55,619 - INFO - joeynmt.training - EPOCH 1
2022-09-16 09:28:00,195 - INFO - joeynmt.training - Epoch   1, Step:   261100, Batch Loss:     2.569917, Batch Acc: 0.015680, Tokens per Sec:     8938, Lr: 0.000035
2022-09-16 09:28:12,431 - INFO - joeynmt.training - Epoch   1, Step:   261200, Batch Loss:     2.434394, Batch Acc: 0.005191, Tokens per Sec:     9384, Lr: 0.000035
2022-09-16 09:28:24,554 - INFO - joeynmt.training - Epoch   1, Step:   261300, Batch Loss:     2.629673, Batch Acc: 0.004985, Tokens per Sec:     9266, Lr: 0.000035
2022-09-16 09:28:36,658 - INFO - joeynmt.training - Epoch   1, Step:   261400, Batch Loss:     2.823909, Batch Acc: 0.004616, Tokens per Sec:     9308, Lr: 0.000035
2022-09-16 09:28:49,006 - INFO - joeynmt.training - Epoch   1, Step:   261500, Batch Loss:     2.549430, Batch Acc: 0.005156, Tokens per Sec:     9315, Lr: 0.000035
2022-09-16 09:29:01,002 - INFO - joeynmt.training - Epoch   1, Step:   261600, Batch Loss:     2.608217, Batch Acc: 0.004479, Tokens per Sec:     9567, Lr: 0.000035
2022-09-16 09:29:13,068 - INFO - joeynmt.training - Epoch   1, Step:   261700, Batch Loss:     2.428512, Batch Acc: 0.004391, Tokens per Sec:     9551, Lr: 0.000035
2022-09-16 09:29:25,047 - INFO - joeynmt.training - Epoch   1, Step:   261800, Batch Loss:     2.595208, Batch Acc: 0.005065, Tokens per Sec:     9527, Lr: 0.000035
2022-09-16 09:29:37,065 - INFO - joeynmt.training - Epoch   1, Step:   261900, Batch Loss:     2.373309, Batch Acc: 0.005067, Tokens per Sec:     9575, Lr: 0.000035
2022-09-16 09:29:49,025 - INFO - joeynmt.training - Epoch   1, Step:   262000, Batch Loss:     2.633154, Batch Acc: 0.004619, Tokens per Sec:     9558, Lr: 0.000035
2022-09-16 09:29:49,025 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:38:13,475 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 09:38:13,477 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.49, loss:   2.48, ppl:  11.92, acc:   0.52, generation: 494.1788[sec], evaluation: 9.7595[sec]
2022-09-16 09:38:13,920 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/258000.ckpt
2022-09-16 09:38:13,947 - INFO - joeynmt.training - Example #0
2022-09-16 09:38:13,948 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 09:38:13,948 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 09:38:13,948 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 09:38:13,965 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 09:38:13,965 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 09:38:13,965 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 09:38:13,965 - INFO - joeynmt.training - Example #1
2022-09-16 09:38:13,965 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 09:38:13,965 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 09:38:13,965 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 09:38:13,980 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 09:38:13,980 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 09:38:13,980 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 09:38:13,980 - INFO - joeynmt.training - Example #2
2022-09-16 09:38:13,980 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 09:38:13,980 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 09:38:13,981 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'माइ@@', 'म', '</s>']
2022-09-16 09:38:13,995 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 09:38:13,995 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 09:38:13,995 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और माइम
2022-09-16 09:38:13,995 - INFO - joeynmt.training - Example #3
2022-09-16 09:38:13,996 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 09:38:13,996 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 09:38:13,996 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 09:38:14,010 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 09:38:14,011 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 09:38:14,011 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 09:38:25,999 - INFO - joeynmt.training - Epoch   1, Step:   262100, Batch Loss:     2.475501, Batch Acc: 0.005339, Tokens per Sec:     9069, Lr: 0.000035
2022-09-16 09:38:37,898 - INFO - joeynmt.training - Epoch   1, Step:   262200, Batch Loss:     2.750827, Batch Acc: 0.003862, Tokens per Sec:     9532, Lr: 0.000035
2022-09-16 09:38:49,857 - INFO - joeynmt.training - Epoch   1, Step:   262300, Batch Loss:     2.694933, Batch Acc: 0.004803, Tokens per Sec:     9594, Lr: 0.000035
2022-09-16 09:39:01,712 - INFO - joeynmt.training - Epoch   1, Step:   262400, Batch Loss:     2.602595, Batch Acc: 0.004166, Tokens per Sec:     9557, Lr: 0.000035
2022-09-16 09:39:13,621 - INFO - joeynmt.training - Epoch   1, Step:   262500, Batch Loss:     2.543214, Batch Acc: 0.004660, Tokens per Sec:     9623, Lr: 0.000035
2022-09-16 09:39:25,481 - INFO - joeynmt.training - Epoch   1, Step:   262600, Batch Loss:     2.436939, Batch Acc: 0.004499, Tokens per Sec:     9520, Lr: 0.000035
2022-09-16 09:39:37,333 - INFO - joeynmt.training - Epoch   1, Step:   262700, Batch Loss:     2.678772, Batch Acc: 0.004054, Tokens per Sec:     9491, Lr: 0.000035
2022-09-16 09:39:49,176 - INFO - joeynmt.training - Epoch   1, Step:   262800, Batch Loss:     2.571740, Batch Acc: 0.005238, Tokens per Sec:     9640, Lr: 0.000035
2022-09-16 09:40:00,993 - INFO - joeynmt.training - Epoch   1, Step:   262900, Batch Loss:     2.920846, Batch Acc: 0.004052, Tokens per Sec:     9732, Lr: 0.000035
2022-09-16 09:40:13,127 - INFO - joeynmt.training - Epoch   1, Step:   263000, Batch Loss:     2.426185, Batch Acc: 0.005533, Tokens per Sec:     9504, Lr: 0.000035
2022-09-16 09:40:13,127 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:49:03,426 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 09:49:03,428 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.46, loss:   2.48, ppl:  11.95, acc:   0.52, generation: 519.9606[sec], evaluation: 9.8329[sec]
2022-09-16 09:49:03,434 - INFO - joeynmt.training - Example #0
2022-09-16 09:49:03,435 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 09:49:03,435 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 09:49:03,436 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 09:49:03,453 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 09:49:03,453 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 09:49:03,453 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 09:49:03,453 - INFO - joeynmt.training - Example #1
2022-09-16 09:49:03,453 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 09:49:03,454 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 09:49:03,454 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', '</s>']
2022-09-16 09:49:03,469 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 09:49:03,469 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 09:49:03,469 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन
2022-09-16 09:49:03,469 - INFO - joeynmt.training - Example #2
2022-09-16 09:49:03,469 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 09:49:03,469 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 09:49:03,469 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 09:49:03,484 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 09:49:03,484 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 09:49:03,484 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 09:49:03,484 - INFO - joeynmt.training - Example #3
2022-09-16 09:49:03,484 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 09:49:03,485 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 09:49:03,485 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 09:49:03,500 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 09:49:03,500 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 09:49:03,500 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 09:49:15,749 - INFO - joeynmt.training - Epoch   1, Step:   263100, Batch Loss:     2.569830, Batch Acc: 0.005703, Tokens per Sec:     9226, Lr: 0.000035
2022-09-16 09:49:27,936 - INFO - joeynmt.training - Epoch   1, Step:   263200, Batch Loss:     2.591472, Batch Acc: 0.005214, Tokens per Sec:     9349, Lr: 0.000035
2022-09-16 09:49:40,142 - INFO - joeynmt.training - Epoch   1, Step:   263300, Batch Loss:     2.743581, Batch Acc: 0.004602, Tokens per Sec:     9258, Lr: 0.000035
2022-09-16 09:49:52,380 - INFO - joeynmt.training - Epoch   1, Step:   263400, Batch Loss:     2.581812, Batch Acc: 0.003784, Tokens per Sec:     9372, Lr: 0.000035
2022-09-16 09:50:04,450 - INFO - joeynmt.training - Epoch   1, Step:   263500, Batch Loss:     2.393375, Batch Acc: 0.004801, Tokens per Sec:     9526, Lr: 0.000035
2022-09-16 09:50:16,325 - INFO - joeynmt.training - Epoch   1, Step:   263600, Batch Loss:     2.592999, Batch Acc: 0.005222, Tokens per Sec:     9595, Lr: 0.000035
2022-09-16 09:50:28,182 - INFO - joeynmt.training - Epoch   1, Step:   263700, Batch Loss:     2.464962, Batch Acc: 0.005017, Tokens per Sec:     9718, Lr: 0.000035
2022-09-16 09:50:40,045 - INFO - joeynmt.training - Epoch   1, Step:   263800, Batch Loss:     2.637155, Batch Acc: 0.003665, Tokens per Sec:     9707, Lr: 0.000035
2022-09-16 09:50:51,857 - INFO - joeynmt.training - Epoch   1, Step:   263900, Batch Loss:     2.666602, Batch Acc: 0.004901, Tokens per Sec:     9588, Lr: 0.000035
2022-09-16 09:51:03,685 - INFO - joeynmt.training - Epoch   1, Step:   264000, Batch Loss:     2.483078, Batch Acc: 0.004866, Tokens per Sec:     9575, Lr: 0.000035
2022-09-16 09:51:03,685 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 09:59:39,760 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 09:59:39,761 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.57, loss:   2.48, ppl:  11.92, acc:   0.52, generation: 506.2762[sec], evaluation: 9.2830[sec]
2022-09-16 09:59:40,580 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261064.ckpt
2022-09-16 09:59:40,608 - INFO - joeynmt.training - Example #0
2022-09-16 09:59:40,608 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 09:59:40,608 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 09:59:40,608 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'शामिल', 'किया', 'गया', 'है', '</s>']
2022-09-16 09:59:40,626 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 09:59:40,626 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 09:59:40,626 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को शामिल किया गया है
2022-09-16 09:59:40,626 - INFO - joeynmt.training - Example #1
2022-09-16 09:59:40,626 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 09:59:40,626 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 09:59:40,626 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'खे@@', 'लें', '</s>']
2022-09-16 09:59:40,641 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 09:59:40,641 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 09:59:40,641 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 09:59:40,641 - INFO - joeynmt.training - Example #2
2022-09-16 09:59:40,641 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 09:59:40,641 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 09:59:40,641 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 09:59:40,656 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 09:59:40,656 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 09:59:40,656 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 09:59:40,656 - INFO - joeynmt.training - Example #3
2022-09-16 09:59:40,657 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 09:59:40,657 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 09:59:40,657 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 09:59:40,671 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 09:59:40,671 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 09:59:40,672 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 09:59:52,574 - INFO - joeynmt.training - Epoch   1, Step:   264100, Batch Loss:     2.656424, Batch Acc: 0.005065, Tokens per Sec:     8863, Lr: 0.000035
2022-09-16 10:00:04,415 - INFO - joeynmt.training - Epoch   1, Step:   264200, Batch Loss:     2.471980, Batch Acc: 0.004663, Tokens per Sec:     9618, Lr: 0.000035
2022-09-16 10:00:16,284 - INFO - joeynmt.training - Epoch   1, Step:   264300, Batch Loss:     2.478002, Batch Acc: 0.005176, Tokens per Sec:     9767, Lr: 0.000035
2022-09-16 10:00:28,106 - INFO - joeynmt.training - Epoch   1, Step:   264400, Batch Loss:     2.490004, Batch Acc: 0.005667, Tokens per Sec:     9792, Lr: 0.000035
2022-09-16 10:00:39,944 - INFO - joeynmt.training - Epoch   1, Step:   264500, Batch Loss:     2.669018, Batch Acc: 0.004698, Tokens per Sec:     9693, Lr: 0.000035
2022-09-16 10:00:51,773 - INFO - joeynmt.training - Epoch   1, Step:   264600, Batch Loss:     2.555085, Batch Acc: 0.003851, Tokens per Sec:     9637, Lr: 0.000035
2022-09-16 10:01:03,596 - INFO - joeynmt.training - Epoch   1, Step:   264700, Batch Loss:     2.513205, Batch Acc: 0.004775, Tokens per Sec:     9636, Lr: 0.000035
2022-09-16 10:01:15,412 - INFO - joeynmt.training - Epoch   1, Step:   264800, Batch Loss:     2.516622, Batch Acc: 0.005021, Tokens per Sec:     9777, Lr: 0.000035
2022-09-16 10:01:27,257 - INFO - joeynmt.training - Epoch   1, Step:   264900, Batch Loss:     2.408744, Batch Acc: 0.005114, Tokens per Sec:     9856, Lr: 0.000035
2022-09-16 10:01:39,113 - INFO - joeynmt.training - Epoch   1, Step:   265000, Batch Loss:     2.445458, Batch Acc: 0.004315, Tokens per Sec:     9656, Lr: 0.000035
2022-09-16 10:01:39,114 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 10:09:56,392 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 10:09:56,393 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.53, loss:   2.48, ppl:  11.92, acc:   0.52, generation: 487.5585[sec], evaluation: 9.2167[sec]
2022-09-16 10:09:56,837 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/262000.ckpt
2022-09-16 10:09:56,865 - INFO - joeynmt.training - Example #0
2022-09-16 10:09:56,865 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 10:09:56,865 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 10:09:56,865 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 10:09:56,882 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 10:09:56,882 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 10:09:56,882 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 10:09:56,882 - INFO - joeynmt.training - Example #1
2022-09-16 10:09:56,882 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 10:09:56,883 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 10:09:56,883 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'खे@@', 'लें', '</s>']
2022-09-16 10:09:56,898 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 10:09:56,898 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 10:09:56,898 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 10:09:56,898 - INFO - joeynmt.training - Example #2
2022-09-16 10:09:56,898 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 10:09:56,898 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 10:09:56,898 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 10:09:56,913 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 10:09:56,913 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 10:09:56,913 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 10:09:56,913 - INFO - joeynmt.training - Example #3
2022-09-16 10:09:56,913 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 10:09:56,913 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 10:09:56,913 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 10:09:56,928 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 10:09:56,928 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 10:09:56,928 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 10:10:08,883 - INFO - joeynmt.training - Epoch   1, Step:   265100, Batch Loss:     2.558388, Batch Acc: 0.004454, Tokens per Sec:     9244, Lr: 0.000035
2022-09-16 10:10:20,714 - INFO - joeynmt.training - Epoch   1, Step:   265200, Batch Loss:     2.505082, Batch Acc: 0.004750, Tokens per Sec:     9573, Lr: 0.000035
2022-09-16 10:10:32,570 - INFO - joeynmt.training - Epoch   1, Step:   265300, Batch Loss:     2.538248, Batch Acc: 0.004655, Tokens per Sec:     9657, Lr: 0.000035
2022-09-16 10:10:44,384 - INFO - joeynmt.training - Epoch   1, Step:   265400, Batch Loss:     2.790062, Batch Acc: 0.004052, Tokens per Sec:     9442, Lr: 0.000035
2022-09-16 10:10:56,220 - INFO - joeynmt.training - Epoch   1, Step:   265500, Batch Loss:     2.426512, Batch Acc: 0.004991, Tokens per Sec:     9834, Lr: 0.000035
2022-09-16 10:11:08,063 - INFO - joeynmt.training - Epoch   1, Step:   265600, Batch Loss:     2.390386, Batch Acc: 0.004665, Tokens per Sec:     9666, Lr: 0.000035
2022-09-16 10:11:19,913 - INFO - joeynmt.training - Epoch   1, Step:   265700, Batch Loss:     2.646073, Batch Acc: 0.004898, Tokens per Sec:     9769, Lr: 0.000035
2022-09-16 10:11:32,193 - INFO - joeynmt.training - Epoch   1, Step:   265800, Batch Loss:     2.565579, Batch Acc: 0.003576, Tokens per Sec:     9497, Lr: 0.000035
2022-09-16 10:11:44,039 - INFO - joeynmt.training - Epoch   1, Step:   265900, Batch Loss:     2.473513, Batch Acc: 0.004703, Tokens per Sec:     9658, Lr: 0.000035
2022-09-16 10:11:55,871 - INFO - joeynmt.training - Epoch   1, Step:   266000, Batch Loss:     2.563687, Batch Acc: 0.004032, Tokens per Sec:     9727, Lr: 0.000035
2022-09-16 10:11:55,871 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 10:20:25,562 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 10:20:25,564 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.60, loss:   2.47, ppl:  11.87, acc:   0.52, generation: 499.8595[sec], evaluation: 9.3228[sec]
2022-09-16 10:20:25,570 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 10:20:26,007 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/265000.ckpt
2022-09-16 10:20:26,035 - INFO - joeynmt.training - Example #0
2022-09-16 10:20:26,035 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 10:20:26,035 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 10:20:26,035 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 10:20:26,052 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 10:20:26,052 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 10:20:26,052 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 10:20:26,052 - INFO - joeynmt.training - Example #1
2022-09-16 10:20:26,052 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 10:20:26,052 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 10:20:26,052 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'खे@@', 'लें', '</s>']
2022-09-16 10:20:26,067 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 10:20:26,067 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 10:20:26,068 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 10:20:26,068 - INFO - joeynmt.training - Example #2
2022-09-16 10:20:26,068 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 10:20:26,068 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 10:20:26,068 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 10:20:26,083 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 10:20:26,083 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 10:20:26,083 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 10:20:26,083 - INFO - joeynmt.training - Example #3
2022-09-16 10:20:26,083 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 10:20:26,083 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 10:20:26,083 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 10:20:26,098 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 10:20:26,098 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 10:20:26,098 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 10:20:37,978 - INFO - joeynmt.training - Epoch   1, Step:   266100, Batch Loss:     2.453869, Batch Acc: 0.004530, Tokens per Sec:     9161, Lr: 0.000035
2022-09-16 10:20:49,808 - INFO - joeynmt.training - Epoch   1, Step:   266200, Batch Loss:     2.612548, Batch Acc: 0.004383, Tokens per Sec:     9566, Lr: 0.000035
2022-09-16 10:21:01,633 - INFO - joeynmt.training - Epoch   1, Step:   266300, Batch Loss:     2.583242, Batch Acc: 0.004263, Tokens per Sec:     9403, Lr: 0.000035
2022-09-16 10:21:13,462 - INFO - joeynmt.training - Epoch   1, Step:   266400, Batch Loss:     2.577844, Batch Acc: 0.003534, Tokens per Sec:     9616, Lr: 0.000035
2022-09-16 10:21:25,370 - INFO - joeynmt.training - Epoch   1, Step:   266500, Batch Loss:     2.532208, Batch Acc: 0.005155, Tokens per Sec:     9840, Lr: 0.000035
2022-09-16 10:21:37,219 - INFO - joeynmt.training - Epoch   1, Step:   266600, Batch Loss:     2.665034, Batch Acc: 0.004552, Tokens per Sec:     9640, Lr: 0.000035
2022-09-16 10:21:49,091 - INFO - joeynmt.training - Epoch   1, Step:   266700, Batch Loss:     2.468610, Batch Acc: 0.004928, Tokens per Sec:     9555, Lr: 0.000035
2022-09-16 10:22:00,919 - INFO - joeynmt.training - Epoch   1, Step:   266800, Batch Loss:     2.582127, Batch Acc: 0.003943, Tokens per Sec:     9627, Lr: 0.000035
2022-09-16 10:22:12,780 - INFO - joeynmt.training - Epoch   1, Step:   266900, Batch Loss:     2.654174, Batch Acc: 0.003813, Tokens per Sec:     9751, Lr: 0.000035
2022-09-16 10:22:24,620 - INFO - joeynmt.training - Epoch   1, Step:   267000, Batch Loss:     2.862012, Batch Acc: 0.004474, Tokens per Sec:     9514, Lr: 0.000035
2022-09-16 10:22:24,621 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 10:30:48,979 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 10:30:48,980 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.57, loss:   2.48, ppl:  11.90, acc:   0.52, generation: 493.9045[sec], evaluation: 9.9360[sec]
2022-09-16 10:30:49,421 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/264000.ckpt
2022-09-16 10:30:49,449 - INFO - joeynmt.training - Example #0
2022-09-16 10:30:49,449 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 10:30:49,449 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 10:30:49,449 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 10:30:49,467 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 10:30:49,467 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 10:30:49,467 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 10:30:49,467 - INFO - joeynmt.training - Example #1
2022-09-16 10:30:49,467 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 10:30:49,467 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 10:30:49,467 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'खे@@', 'लें', '</s>']
2022-09-16 10:30:49,482 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 10:30:49,482 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 10:30:49,482 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 10:30:49,482 - INFO - joeynmt.training - Example #2
2022-09-16 10:30:49,482 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 10:30:49,482 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 10:30:49,482 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'गति', 'और', 'इ@@', 'म', '</s>']
2022-09-16 10:30:49,497 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 10:30:49,497 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 10:30:49,497 - INFO - joeynmt.training - 	Hypothesis: अभिनय , गति और इम
2022-09-16 10:30:49,498 - INFO - joeynmt.training - Example #3
2022-09-16 10:30:49,498 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 10:30:49,498 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 10:30:49,498 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 10:30:49,513 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 10:30:49,513 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 10:30:49,513 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 10:31:01,444 - INFO - joeynmt.training - Epoch   1, Step:   267100, Batch Loss:     2.662068, Batch Acc: 0.004029, Tokens per Sec:     9225, Lr: 0.000035
2022-09-16 10:31:13,307 - INFO - joeynmt.training - Epoch   1, Step:   267200, Batch Loss:     2.656693, Batch Acc: 0.005102, Tokens per Sec:     9633, Lr: 0.000035
2022-09-16 10:31:25,152 - INFO - joeynmt.training - Epoch   1, Step:   267300, Batch Loss:     2.485935, Batch Acc: 0.004412, Tokens per Sec:     9624, Lr: 0.000035
2022-09-16 10:31:36,992 - INFO - joeynmt.training - Epoch   1, Step:   267400, Batch Loss:     2.565913, Batch Acc: 0.005114, Tokens per Sec:     9579, Lr: 0.000035
2022-09-16 10:31:48,850 - INFO - joeynmt.training - Epoch   1, Step:   267500, Batch Loss:     2.483279, Batch Acc: 0.004965, Tokens per Sec:     9547, Lr: 0.000035
2022-09-16 10:32:00,683 - INFO - joeynmt.training - Epoch   1, Step:   267600, Batch Loss:     2.395773, Batch Acc: 0.004034, Tokens per Sec:     9804, Lr: 0.000035
2022-09-16 10:32:12,573 - INFO - joeynmt.training - Epoch   1, Step:   267700, Batch Loss:     2.546032, Batch Acc: 0.003756, Tokens per Sec:     9876, Lr: 0.000035
2022-09-16 10:32:24,419 - INFO - joeynmt.training - Epoch   1, Step:   267800, Batch Loss:     2.593274, Batch Acc: 0.004756, Tokens per Sec:     9657, Lr: 0.000035
2022-09-16 10:32:36,243 - INFO - joeynmt.training - Epoch   1, Step:   267900, Batch Loss:     2.572265, Batch Acc: 0.005620, Tokens per Sec:     9677, Lr: 0.000035
2022-09-16 10:32:48,095 - INFO - joeynmt.training - Epoch   1, Step:   268000, Batch Loss:     2.640775, Batch Acc: 0.004013, Tokens per Sec:     9504, Lr: 0.000035
2022-09-16 10:32:48,096 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 10:41:22,171 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 10:41:22,172 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.65, loss:   2.47, ppl:  11.84, acc:   0.52, generation: 504.1289[sec], evaluation: 9.4315[sec]
2022-09-16 10:41:22,178 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 10:41:22,617 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/261000.ckpt
2022-09-16 10:41:22,646 - INFO - joeynmt.training - Example #0
2022-09-16 10:41:22,646 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 10:41:22,646 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 10:41:22,646 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-16 10:41:22,663 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 10:41:22,663 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 10:41:22,663 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 10:41:22,664 - INFO - joeynmt.training - Example #1
2022-09-16 10:41:22,664 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 10:41:22,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 10:41:22,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'खे@@', 'लें', '</s>']
2022-09-16 10:41:22,679 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 10:41:22,679 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 10:41:22,679 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 10:41:22,679 - INFO - joeynmt.training - Example #2
2022-09-16 10:41:22,679 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 10:41:22,679 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 10:41:22,679 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 10:41:22,694 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 10:41:22,694 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 10:41:22,694 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 10:41:22,694 - INFO - joeynmt.training - Example #3
2022-09-16 10:41:22,694 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 10:41:22,694 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 10:41:22,694 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 10:41:22,709 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 10:41:22,709 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 10:41:22,709 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 10:41:34,604 - INFO - joeynmt.training - Epoch   1, Step:   268100, Batch Loss:     2.489332, Batch Acc: 0.004352, Tokens per Sec:     9099, Lr: 0.000035
2022-09-16 10:41:46,468 - INFO - joeynmt.training - Epoch   1, Step:   268200, Batch Loss:     2.416146, Batch Acc: 0.005601, Tokens per Sec:     9587, Lr: 0.000035
2022-09-16 10:41:58,329 - INFO - joeynmt.training - Epoch   1, Step:   268300, Batch Loss:     2.632030, Batch Acc: 0.004350, Tokens per Sec:     9672, Lr: 0.000035
2022-09-16 10:42:10,212 - INFO - joeynmt.training - Epoch   1, Step:   268400, Batch Loss:     2.684762, Batch Acc: 0.003813, Tokens per Sec:     9779, Lr: 0.000035
2022-09-16 10:42:22,081 - INFO - joeynmt.training - Epoch   1, Step:   268500, Batch Loss:     2.420079, Batch Acc: 0.005204, Tokens per Sec:     9780, Lr: 0.000035
2022-09-16 10:42:33,953 - INFO - joeynmt.training - Epoch   1, Step:   268600, Batch Loss:     2.613468, Batch Acc: 0.004851, Tokens per Sec:     9671, Lr: 0.000035
2022-09-16 10:42:45,826 - INFO - joeynmt.training - Epoch   1, Step:   268700, Batch Loss:     2.771714, Batch Acc: 0.004262, Tokens per Sec:     9723, Lr: 0.000035
2022-09-16 10:42:57,660 - INFO - joeynmt.training - Epoch   1, Step:   268800, Batch Loss:     2.525426, Batch Acc: 0.004238, Tokens per Sec:     9552, Lr: 0.000035
2022-09-16 10:43:09,502 - INFO - joeynmt.training - Epoch   1, Step:   268900, Batch Loss:     2.345333, Batch Acc: 0.003767, Tokens per Sec:     9595, Lr: 0.000035
2022-09-16 10:43:21,377 - INFO - joeynmt.training - Epoch   1, Step:   269000, Batch Loss:     2.649379, Batch Acc: 0.005116, Tokens per Sec:     9596, Lr: 0.000035
2022-09-16 10:43:21,378 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 10:51:46,787 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 10:51:46,788 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.59, loss:   2.47, ppl:  11.82, acc:   0.52, generation: 494.9123[sec], evaluation: 9.9857[sec]
2022-09-16 10:51:46,794 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 10:51:47,242 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/267000.ckpt
2022-09-16 10:51:47,270 - INFO - joeynmt.training - Example #0
2022-09-16 10:51:47,270 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 10:51:47,270 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 10:51:47,270 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 10:51:47,288 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 10:51:47,288 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 10:51:47,288 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 10:51:47,288 - INFO - joeynmt.training - Example #1
2022-09-16 10:51:47,288 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 10:51:47,288 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 10:51:47,288 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'खे@@', 'लें', '</s>']
2022-09-16 10:51:47,303 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 10:51:47,303 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 10:51:47,303 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 10:51:47,303 - INFO - joeynmt.training - Example #2
2022-09-16 10:51:47,303 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 10:51:47,303 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 10:51:47,303 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 10:51:47,318 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 10:51:47,318 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 10:51:47,318 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 10:51:47,318 - INFO - joeynmt.training - Example #3
2022-09-16 10:51:47,319 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 10:51:47,319 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 10:51:47,319 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 10:51:47,333 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 10:51:47,334 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 10:51:47,334 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 10:51:59,236 - INFO - joeynmt.training - Epoch   1, Step:   269100, Batch Loss:     2.300306, Batch Acc: 0.004261, Tokens per Sec:     9072, Lr: 0.000035
2022-09-16 10:52:11,060 - INFO - joeynmt.training - Epoch   1, Step:   269200, Batch Loss:     2.619353, Batch Acc: 0.003905, Tokens per Sec:     9530, Lr: 0.000035
2022-09-16 10:52:22,865 - INFO - joeynmt.training - Epoch   1, Step:   269300, Batch Loss:     2.486250, Batch Acc: 0.003898, Tokens per Sec:     9626, Lr: 0.000035
2022-09-16 10:52:34,625 - INFO - joeynmt.training - Epoch   1, Step:   269400, Batch Loss:     2.353864, Batch Acc: 0.004993, Tokens per Sec:     9828, Lr: 0.000035
2022-09-16 10:52:46,400 - INFO - joeynmt.training - Epoch   1, Step:   269500, Batch Loss:     2.768989, Batch Acc: 0.004003, Tokens per Sec:     9610, Lr: 0.000035
2022-09-16 10:52:58,169 - INFO - joeynmt.training - Epoch   1, Step:   269600, Batch Loss:     2.457184, Batch Acc: 0.004968, Tokens per Sec:     9594, Lr: 0.000035
2022-09-16 10:53:09,947 - INFO - joeynmt.training - Epoch   1, Step:   269700, Batch Loss:     2.549578, Batch Acc: 0.004243, Tokens per Sec:     9727, Lr: 0.000035
2022-09-16 10:53:21,724 - INFO - joeynmt.training - Epoch   1, Step:   269800, Batch Loss:     2.454944, Batch Acc: 0.004837, Tokens per Sec:     9550, Lr: 0.000035
2022-09-16 10:53:33,486 - INFO - joeynmt.training - Epoch   1, Step:   269900, Batch Loss:     2.514468, Batch Acc: 0.005230, Tokens per Sec:     9608, Lr: 0.000035
2022-09-16 10:53:45,223 - INFO - joeynmt.training - Epoch   1, Step:   270000, Batch Loss:     2.441563, Batch Acc: 0.005029, Tokens per Sec:     9725, Lr: 0.000035
2022-09-16 10:53:45,223 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:02:28,806 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 11:02:28,808 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.76, loss:   2.47, ppl:  11.79, acc:   0.52, generation: 513.5076[sec], evaluation: 9.5565[sec]
2022-09-16 11:02:28,814 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 11:02:29,258 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/266000.ckpt
2022-09-16 11:02:29,286 - INFO - joeynmt.training - Example #0
2022-09-16 11:02:29,286 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 11:02:29,286 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 11:02:29,286 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 11:02:29,304 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 11:02:29,304 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 11:02:29,304 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 11:02:29,304 - INFO - joeynmt.training - Example #1
2022-09-16 11:02:29,304 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 11:02:29,304 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 11:02:29,304 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'खे@@', 'लें', '</s>']
2022-09-16 11:02:29,319 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 11:02:29,319 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 11:02:29,319 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 11:02:29,319 - INFO - joeynmt.training - Example #2
2022-09-16 11:02:29,319 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 11:02:29,319 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 11:02:29,319 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'इ@@', 'म', '</s>']
2022-09-16 11:02:29,334 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 11:02:29,334 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 11:02:29,335 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और इम
2022-09-16 11:02:29,335 - INFO - joeynmt.training - Example #3
2022-09-16 11:02:29,335 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 11:02:29,335 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 11:02:29,335 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 11:02:29,350 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 11:02:29,350 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 11:02:29,350 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 11:02:41,495 - INFO - joeynmt.training - Epoch   1, Step:   270100, Batch Loss:     2.450143, Batch Acc: 0.003884, Tokens per Sec:     9198, Lr: 0.000035
2022-09-16 11:02:53,558 - INFO - joeynmt.training - Epoch   1, Step:   270200, Batch Loss:     2.552031, Batch Acc: 0.004528, Tokens per Sec:     9264, Lr: 0.000035
2022-09-16 11:03:05,659 - INFO - joeynmt.training - Epoch   1, Step:   270300, Batch Loss:     2.652199, Batch Acc: 0.004426, Tokens per Sec:     9466, Lr: 0.000035
2022-09-16 11:03:17,743 - INFO - joeynmt.training - Epoch   1, Step:   270400, Batch Loss:     2.697551, Batch Acc: 0.004466, Tokens per Sec:     9507, Lr: 0.000035
2022-09-16 11:03:29,844 - INFO - joeynmt.training - Epoch   1, Step:   270500, Batch Loss:     2.838217, Batch Acc: 0.004508, Tokens per Sec:     9588, Lr: 0.000035
2022-09-16 11:03:41,907 - INFO - joeynmt.training - Epoch   1, Step:   270600, Batch Loss:     2.702452, Batch Acc: 0.004471, Tokens per Sec:     9587, Lr: 0.000035
2022-09-16 11:03:54,049 - INFO - joeynmt.training - Epoch   1, Step:   270700, Batch Loss:     2.612350, Batch Acc: 0.005440, Tokens per Sec:     9432, Lr: 0.000035
2022-09-16 11:04:06,187 - INFO - joeynmt.training - Epoch   1, Step:   270800, Batch Loss:     2.650368, Batch Acc: 0.005368, Tokens per Sec:     9455, Lr: 0.000035
2022-09-16 11:04:18,291 - INFO - joeynmt.training - Epoch   1, Step:   270900, Batch Loss:     2.691574, Batch Acc: 0.004557, Tokens per Sec:     9447, Lr: 0.000035
2022-09-16 11:04:30,366 - INFO - joeynmt.training - Epoch   1, Step:   271000, Batch Loss:     2.482907, Batch Acc: 0.004955, Tokens per Sec:     9561, Lr: 0.000035
2022-09-16 11:04:30,366 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:13:02,868 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 11:13:02,870 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.62, loss:   2.46, ppl:  11.72, acc:   0.52, generation: 502.5791[sec], evaluation: 9.3999[sec]
2022-09-16 11:13:02,876 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 11:13:03,309 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/268000.ckpt
2022-09-16 11:13:03,337 - INFO - joeynmt.training - Example #0
2022-09-16 11:13:03,337 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 11:13:03,337 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 11:13:03,337 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 11:13:03,354 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 11:13:03,354 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 11:13:03,354 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 11:13:03,354 - INFO - joeynmt.training - Example #1
2022-09-16 11:13:03,354 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 11:13:03,355 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 11:13:03,355 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'खे@@', 'लें', '</s>']
2022-09-16 11:13:03,370 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 11:13:03,370 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 11:13:03,370 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 11:13:03,370 - INFO - joeynmt.training - Example #2
2022-09-16 11:13:03,370 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 11:13:03,370 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 11:13:03,370 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'माइ@@', 'म', '</s>']
2022-09-16 11:13:03,385 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 11:13:03,385 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 11:13:03,385 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और माइम
2022-09-16 11:13:03,385 - INFO - joeynmt.training - Example #3
2022-09-16 11:13:03,385 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 11:13:03,385 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 11:13:03,385 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 11:13:03,400 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 11:13:03,400 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 11:13:03,400 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 11:13:15,244 - INFO - joeynmt.training - Epoch   1, Step:   271100, Batch Loss:     2.774198, Batch Acc: 0.004204, Tokens per Sec:     9155, Lr: 0.000035
2022-09-16 11:13:27,035 - INFO - joeynmt.training - Epoch   1, Step:   271200, Batch Loss:     2.593941, Batch Acc: 0.004948, Tokens per Sec:     9737, Lr: 0.000035
2022-09-16 11:13:38,836 - INFO - joeynmt.training - Epoch   1, Step:   271300, Batch Loss:     2.563487, Batch Acc: 0.005226, Tokens per Sec:     9648, Lr: 0.000035
2022-09-16 11:13:50,585 - INFO - joeynmt.training - Epoch   1, Step:   271400, Batch Loss:     2.917053, Batch Acc: 0.004833, Tokens per Sec:     9651, Lr: 0.000035
2022-09-16 11:14:02,393 - INFO - joeynmt.training - Epoch   1, Step:   271500, Batch Loss:     2.341384, Batch Acc: 0.004417, Tokens per Sec:     9740, Lr: 0.000035
2022-09-16 11:14:14,217 - INFO - joeynmt.training - Epoch   1, Step:   271600, Batch Loss:     2.598774, Batch Acc: 0.003761, Tokens per Sec:     9806, Lr: 0.000035
2022-09-16 11:14:25,974 - INFO - joeynmt.training - Epoch   1, Step:   271700, Batch Loss:     2.508340, Batch Acc: 0.004045, Tokens per Sec:     9504, Lr: 0.000035
2022-09-16 11:14:37,774 - INFO - joeynmt.training - Epoch   1, Step:   271800, Batch Loss:     2.600524, Batch Acc: 0.003998, Tokens per Sec:     9815, Lr: 0.000035
2022-09-16 11:14:49,557 - INFO - joeynmt.training - Epoch   1, Step:   271900, Batch Loss:     2.353129, Batch Acc: 0.005940, Tokens per Sec:     9644, Lr: 0.000035
2022-09-16 11:15:01,357 - INFO - joeynmt.training - Epoch   1, Step:   272000, Batch Loss:     2.744347, Batch Acc: 0.004381, Tokens per Sec:     9731, Lr: 0.000035
2022-09-16 11:15:01,358 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:23:25,227 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 11:23:25,228 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.72, loss:   2.46, ppl:  11.72, acc:   0.52, generation: 493.2205[sec], evaluation: 10.1352[sec]
2022-09-16 11:23:25,234 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 11:23:25,681 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/269000.ckpt
2022-09-16 11:23:25,709 - INFO - joeynmt.training - Example #0
2022-09-16 11:23:25,710 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 11:23:25,710 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 11:23:25,710 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 11:23:25,727 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 11:23:25,728 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 11:23:25,728 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 11:23:25,728 - INFO - joeynmt.training - Example #1
2022-09-16 11:23:25,728 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 11:23:25,728 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 11:23:25,728 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'खे@@', 'लें', '</s>']
2022-09-16 11:23:25,743 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 11:23:25,743 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 11:23:25,744 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 11:23:25,744 - INFO - joeynmt.training - Example #2
2022-09-16 11:23:25,744 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 11:23:25,744 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 11:23:25,744 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'गति', 'और', 'माइ@@', 'म', '</s>']
2022-09-16 11:23:25,759 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 11:23:25,759 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 11:23:25,759 - INFO - joeynmt.training - 	Hypothesis: अभिनय , गति और माइम
2022-09-16 11:23:25,759 - INFO - joeynmt.training - Example #3
2022-09-16 11:23:25,759 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 11:23:25,760 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 11:23:25,760 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 11:23:25,775 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 11:23:25,775 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 11:23:25,775 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 11:23:37,906 - INFO - joeynmt.training - Epoch   1, Step:   272100, Batch Loss:     2.624753, Batch Acc: 0.004037, Tokens per Sec:     8952, Lr: 0.000035
2022-09-16 11:23:49,996 - INFO - joeynmt.training - Epoch   1, Step:   272200, Batch Loss:     2.866745, Batch Acc: 0.003517, Tokens per Sec:     9524, Lr: 0.000035
2022-09-16 11:24:02,102 - INFO - joeynmt.training - Epoch   1, Step:   272300, Batch Loss:     2.564026, Batch Acc: 0.004890, Tokens per Sec:     9596, Lr: 0.000035
2022-09-16 11:24:14,170 - INFO - joeynmt.training - Epoch   1, Step:   272400, Batch Loss:     2.336769, Batch Acc: 0.005451, Tokens per Sec:     9470, Lr: 0.000035
2022-09-16 11:24:26,267 - INFO - joeynmt.training - Epoch   1, Step:   272500, Batch Loss:     2.514210, Batch Acc: 0.004121, Tokens per Sec:     9510, Lr: 0.000035
2022-09-16 11:24:38,331 - INFO - joeynmt.training - Epoch   1, Step:   272600, Batch Loss:     2.599821, Batch Acc: 0.005003, Tokens per Sec:     9361, Lr: 0.000035
2022-09-16 11:24:50,385 - INFO - joeynmt.training - Epoch   1, Step:   272700, Batch Loss:     2.837869, Batch Acc: 0.003594, Tokens per Sec:     9394, Lr: 0.000035
2022-09-16 11:25:02,510 - INFO - joeynmt.training - Epoch   1, Step:   272800, Batch Loss:     2.444754, Batch Acc: 0.004491, Tokens per Sec:     9440, Lr: 0.000035
2022-09-16 11:25:14,648 - INFO - joeynmt.training - Epoch   1, Step:   272900, Batch Loss:     2.854216, Batch Acc: 0.004120, Tokens per Sec:     9278, Lr: 0.000035
2022-09-16 11:25:26,738 - INFO - joeynmt.training - Epoch   1, Step:   273000, Batch Loss:     2.661641, Batch Acc: 0.004911, Tokens per Sec:     9517, Lr: 0.000035
2022-09-16 11:25:26,738 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:34:03,835 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 11:34:03,836 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.75, loss:   2.46, ppl:  11.71, acc:   0.52, generation: 506.9543[sec], evaluation: 9.6273[sec]
2022-09-16 11:34:03,843 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2022-09-16 11:34:04,288 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/270000.ckpt
2022-09-16 11:34:04,316 - INFO - joeynmt.training - Example #0
2022-09-16 11:34:04,316 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 11:34:04,316 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 11:34:04,316 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 11:34:04,334 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 11:34:04,334 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 11:34:04,334 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 11:34:04,334 - INFO - joeynmt.training - Example #1
2022-09-16 11:34:04,334 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 11:34:04,334 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 11:34:04,334 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'खे@@', 'लें', '</s>']
2022-09-16 11:34:04,350 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 11:34:04,350 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 11:34:04,350 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 11:34:04,350 - INFO - joeynmt.training - Example #2
2022-09-16 11:34:04,350 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 11:34:04,350 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 11:34:04,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'गति', 'और', 'माइ@@', 'म', '</s>']
2022-09-16 11:34:04,366 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 11:34:04,366 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 11:34:04,366 - INFO - joeynmt.training - 	Hypothesis: अभिनय , गति और माइम
2022-09-16 11:34:04,366 - INFO - joeynmt.training - Example #3
2022-09-16 11:34:04,366 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 11:34:04,366 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 11:34:04,366 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 11:34:04,382 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 11:34:04,382 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 11:34:04,382 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 11:34:16,556 - INFO - joeynmt.training - Epoch   1, Step:   273100, Batch Loss:     2.823523, Batch Acc: 0.004591, Tokens per Sec:     9047, Lr: 0.000035
2022-09-16 11:34:28,635 - INFO - joeynmt.training - Epoch   1, Step:   273200, Batch Loss:     2.670875, Batch Acc: 0.003896, Tokens per Sec:     9498, Lr: 0.000035
2022-09-16 11:34:40,733 - INFO - joeynmt.training - Epoch   1, Step:   273300, Batch Loss:     2.775642, Batch Acc: 0.004045, Tokens per Sec:     9606, Lr: 0.000035
2022-09-16 11:34:52,824 - INFO - joeynmt.training - Epoch   1, Step:   273400, Batch Loss:     2.626444, Batch Acc: 0.004120, Tokens per Sec:     9474, Lr: 0.000035
2022-09-16 11:35:04,885 - INFO - joeynmt.training - Epoch   1, Step:   273500, Batch Loss:     2.649558, Batch Acc: 0.004709, Tokens per Sec:     9333, Lr: 0.000035
2022-09-16 11:35:16,936 - INFO - joeynmt.training - Epoch   1, Step:   273600, Batch Loss:     2.300899, Batch Acc: 0.005425, Tokens per Sec:     9483, Lr: 0.000035
2022-09-16 11:35:29,013 - INFO - joeynmt.training - Epoch   1, Step:   273700, Batch Loss:     2.636569, Batch Acc: 0.003870, Tokens per Sec:     9436, Lr: 0.000035
2022-09-16 11:35:41,106 - INFO - joeynmt.training - Epoch   1, Step:   273800, Batch Loss:     2.533844, Batch Acc: 0.004225, Tokens per Sec:     9493, Lr: 0.000035
2022-09-16 11:35:53,234 - INFO - joeynmt.training - Epoch   1, Step:   273900, Batch Loss:     2.419372, Batch Acc: 0.004220, Tokens per Sec:     9379, Lr: 0.000035
2022-09-16 11:36:05,316 - INFO - joeynmt.training - Epoch   1, Step:   274000, Batch Loss:     2.755618, Batch Acc: 0.004380, Tokens per Sec:     9411, Lr: 0.000035
2022-09-16 11:36:05,317 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:44:45,448 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 11:44:45,449 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.66, loss:   2.46, ppl:  11.71, acc:   0.52, generation: 510.0698[sec], evaluation: 9.5447[sec]
2022-09-16 11:44:45,904 - INFO - joeynmt.helpers - delete /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/271000.ckpt
2022-09-16 11:44:45,932 - INFO - joeynmt.training - Example #0
2022-09-16 11:44:45,933 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 11:44:45,933 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 11:44:45,933 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषयों', 'को', 'कवर', 'किया', 'गया', 'है', '</s>']
2022-09-16 11:44:45,950 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 11:44:45,950 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 11:44:45,950 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषयों को कवर किया गया है
2022-09-16 11:44:45,951 - INFO - joeynmt.training - Example #1
2022-09-16 11:44:45,951 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 11:44:45,951 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 11:44:45,951 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'खे@@', 'लें', '</s>']
2022-09-16 11:44:45,966 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 11:44:45,966 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 11:44:45,966 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 11:44:45,966 - INFO - joeynmt.training - Example #2
2022-09-16 11:44:45,966 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 11:44:45,966 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 11:44:45,966 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'गति', 'और', 'माइ@@', 'म', '</s>']
2022-09-16 11:44:45,981 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 11:44:45,981 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 11:44:45,981 - INFO - joeynmt.training - 	Hypothesis: अभिनय , गति और माइम
2022-09-16 11:44:45,981 - INFO - joeynmt.training - Example #3
2022-09-16 11:44:45,981 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 11:44:45,982 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 11:44:45,982 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 11:44:45,997 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 11:44:45,997 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 11:44:45,997 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 11:44:58,145 - INFO - joeynmt.training - Epoch   1, Step:   274100, Batch Loss:     2.862774, Batch Acc: 0.004165, Tokens per Sec:     8987, Lr: 0.000035
2022-09-16 11:45:10,233 - INFO - joeynmt.training - Epoch   1, Step:   274200, Batch Loss:     2.437513, Batch Acc: 0.004984, Tokens per Sec:     9312, Lr: 0.000035
2022-09-16 11:45:22,321 - INFO - joeynmt.training - Epoch   1, Step:   274300, Batch Loss:     2.547149, Batch Acc: 0.004374, Tokens per Sec:     9513, Lr: 0.000035
2022-09-16 11:45:34,370 - INFO - joeynmt.training - Epoch   1, Step:   274400, Batch Loss:     2.750576, Batch Acc: 0.004342, Tokens per Sec:     9347, Lr: 0.000035
2022-09-16 11:45:46,189 - INFO - joeynmt.training - Epoch   1, Step:   274500, Batch Loss:     2.532323, Batch Acc: 0.004245, Tokens per Sec:     9707, Lr: 0.000035
2022-09-16 11:45:57,982 - INFO - joeynmt.training - Epoch   1, Step:   274600, Batch Loss:     2.922097, Batch Acc: 0.003343, Tokens per Sec:     9411, Lr: 0.000035
2022-09-16 11:46:09,764 - INFO - joeynmt.training - Epoch   1, Step:   274700, Batch Loss:     2.589400, Batch Acc: 0.005124, Tokens per Sec:     9823, Lr: 0.000035
2022-09-16 11:46:21,528 - INFO - joeynmt.training - Epoch   1, Step:   274800, Batch Loss:     2.597955, Batch Acc: 0.004710, Tokens per Sec:     9836, Lr: 0.000035
2022-09-16 11:46:33,262 - INFO - joeynmt.training - Epoch   1, Step:   274900, Batch Loss:     2.517210, Batch Acc: 0.004436, Tokens per Sec:     9624, Lr: 0.000035
2022-09-16 11:46:45,064 - INFO - joeynmt.training - Epoch   1, Step:   275000, Batch Loss:     2.734736, Batch Acc: 0.004428, Tokens per Sec:     9549, Lr: 0.000035
2022-09-16 11:46:45,064 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Greedy decoding with min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 11:55:11,913 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 11:55:11,914 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  22.75, loss:   2.46, ppl:  11.73, acc:   0.52, generation: 496.9541[sec], evaluation: 9.3696[sec]
2022-09-16 11:55:11,922 - INFO - joeynmt.training - Example #0
2022-09-16 11:55:11,922 - DEBUG - joeynmt.training - 	Tokenized source:     ['on', 'the', 'technical', 'side', ',', 'the', 'subjects', 'covered', 'are']
2022-09-16 11:55:11,922 - DEBUG - joeynmt.training - 	Tokenized reference:  ['निम्न', 'विषय', 'तकनीकी', 'क्षेत्र', 'में', 'उपलब्ध', 'हैं']
2022-09-16 11:55:11,922 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['तकनीकी', 'पक्ष', 'में', ',', 'विषय', 'शामिल', 'हैं', '</s>']
2022-09-16 11:55:11,939 - INFO - joeynmt.training - 	Source:     on the technical side , the subjects covered are
2022-09-16 11:55:11,939 - INFO - joeynmt.training - 	Reference:  निम्न विषय तकनीकी क्षेत्र में उपलब्ध हैं
2022-09-16 11:55:11,940 - INFO - joeynmt.training - 	Hypothesis: तकनीकी पक्ष में , विषय शामिल हैं
2022-09-16 11:55:11,940 - INFO - joeynmt.training - Example #1
2022-09-16 11:55:11,940 - DEBUG - joeynmt.training - 	Tokenized source:     ['play', 'direction', 'production']
2022-09-16 11:55:11,940 - DEBUG - joeynmt.training - 	Tokenized reference:  ['नाट्य', 'निर्देशन', 'प्रस्तुति']
2022-09-16 11:55:11,940 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['दिशा', 'उत्पादन', 'खे@@', 'लें', '</s>']
2022-09-16 11:55:11,955 - INFO - joeynmt.training - 	Source:     play direction production
2022-09-16 11:55:11,955 - INFO - joeynmt.training - 	Reference:  नाट्य निर्देशन प्रस्तुति
2022-09-16 11:55:11,955 - INFO - joeynmt.training - 	Hypothesis: दिशा उत्पादन खेलें
2022-09-16 11:55:11,955 - INFO - joeynmt.training - Example #2
2022-09-16 11:55:11,955 - DEBUG - joeynmt.training - 	Tokenized source:     ['acting', ',', 'movement', 'amp', 'mime']
2022-09-16 11:55:11,955 - DEBUG - joeynmt.training - 	Tokenized reference:  ['अभिनय', ',', 'गति', 'संचालन', 'और', 'मू@@', 'का@@', 'भि@@', 'न@@', 'य']
2022-09-16 11:55:11,955 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['अभिनय', ',', 'आंदोलन', 'और', 'माइ@@', 'म', '</s>']
2022-09-16 11:55:11,970 - INFO - joeynmt.training - 	Source:     acting , movement amp mime
2022-09-16 11:55:11,970 - INFO - joeynmt.training - 	Reference:  अभिनय , गति संचालन और मूकाभिनय
2022-09-16 11:55:11,970 - INFO - joeynmt.training - 	Hypothesis: अभिनय , आंदोलन और माइम
2022-09-16 11:55:11,970 - INFO - joeynmt.training - Example #3
2022-09-16 11:55:11,970 - DEBUG - joeynmt.training - 	Tokenized source:     ['speech', 'and', 'voice', 'training']
2022-09-16 11:55:11,971 - DEBUG - joeynmt.training - 	Tokenized reference:  ['भाषण', 'शैली', 'और', 'वा@@', 'क्', 'प्रशिक्षण']
2022-09-16 11:55:11,971 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['भाषण', 'और', 'आवाज', 'प्रशिक्षण', '</s>']
2022-09-16 11:55:11,985 - INFO - joeynmt.training - 	Source:     speech and voice training
2022-09-16 11:55:11,985 - INFO - joeynmt.training - 	Reference:  भाषण शैली और वाक् प्रशिक्षण
2022-09-16 11:55:11,986 - INFO - joeynmt.training - 	Hypothesis: भाषण और आवाज प्रशिक्षण
2022-09-16 11:55:23,887 - INFO - joeynmt.training - Epoch   1, Step:   275100, Batch Loss:     2.575066, Batch Acc: 0.005007, Tokens per Sec:     9680, Lr: 0.000035
2022-09-16 11:55:35,733 - INFO - joeynmt.training - Epoch   1, Step:   275200, Batch Loss:     2.583625, Batch Acc: 0.004339, Tokens per Sec:     9728, Lr: 0.000035
2022-09-16 11:55:47,570 - INFO - joeynmt.training - Epoch   1, Step:   275300, Batch Loss:     2.597494, Batch Acc: 0.004560, Tokens per Sec:     9652, Lr: 0.000035
2022-09-16 11:55:59,338 - INFO - joeynmt.training - Epoch   1, Step:   275400, Batch Loss:     2.355397, Batch Acc: 0.004010, Tokens per Sec:     9706, Lr: 0.000035
2022-09-16 11:56:11,117 - INFO - joeynmt.training - Epoch   1, Step:   275500, Batch Loss:     2.507886, Batch Acc: 0.004060, Tokens per Sec:     9787, Lr: 0.000035
2022-09-16 11:56:22,930 - INFO - joeynmt.training - Epoch   1, Step:   275600, Batch Loss:     2.358391, Batch Acc: 0.004612, Tokens per Sec:     9599, Lr: 0.000035
2022-09-16 11:56:34,714 - INFO - joeynmt.training - Epoch   1, Step:   275700, Batch Loss:     2.628458, Batch Acc: 0.004186, Tokens per Sec:     9568, Lr: 0.000035
2022-09-16 11:56:46,687 - INFO - joeynmt.training - Epoch   1, Step:   275800, Batch Loss:     2.576312, Batch Acc: 0.005027, Tokens per Sec:     9537, Lr: 0.000035
2022-09-16 11:56:48,196 - INFO - joeynmt.training - Epoch   1: total training loss 38205.03
2022-09-16 11:56:48,196 - INFO - joeynmt.training - Training ended after   1 epochs.
2022-09-16 11:56:48,197 - INFO - joeynmt.training - Best validation result (greedy) at step   273000:  11.71 ppl.
2022-09-16 11:56:48,197 - INFO - joeynmt.training - Loading from ckpt file: /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/273000.ckpt
2022-09-16 11:56:48,215 - INFO - joeynmt.model - Building an encoder-decoder model...
2022-09-16 11:56:48,215 - INFO - joeynmt.model - Pytorch version 1.12.1+cu102 torch.version.cuda 10.2 Cuda available True Cuda devices 1
2022-09-16 11:56:48,549 - INFO - joeynmt.model - Enc-dec model built.
2022-09-16 11:56:48,554 - INFO - joeynmt.model - Total params: 19302144
2022-09-16 11:56:48,821 - INFO - joeynmt.helpers - Load model from /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/273000.ckpt.
2022-09-16 11:56:48,887 - INFO - joeynmt.prediction - Decoding on dev set...
2022-09-16 11:56:48,887 - INFO - joeynmt.prediction - Predicting 40856 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:22:50,694 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 12:22:50,696 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  24.11, generation: 1551.1021[sec], evaluation: 10.1985[sec]
2022-09-16 12:22:50,757 - INFO - joeynmt.prediction - Translations saved to: /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/00273000.hyps.dev.
2022-09-16 12:22:50,757 - INFO - joeynmt.prediction - Decoding on test set...
2022-09-16 12:22:50,757 - INFO - joeynmt.prediction - Predicting 40858 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=130, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2022-09-16 12:47:40,361 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0
2022-09-16 12:47:40,363 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  23.36, generation: 1478.8113[sec], evaluation: 10.2912[sec]
2022-09-16 12:47:40,434 - INFO - joeynmt.prediction - Translations saved to: /home/ubuntu/joeynmt_kriti/models/v1_enhi_100_transformer/enhi_transformer_t1_least_confidence_resume/00273000.hyps.test.
2022-09-16 12:47:40,447 - INFO - joeynmt.training - ACTIVE LEARNING MODEL END - LEAST CONFIDENCE 2
